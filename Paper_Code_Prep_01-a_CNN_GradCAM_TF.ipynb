{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4258f10b-7657-4690-a3e4-9f60e26eb26b",
   "metadata": {},
   "source": [
    "# Basic Model Structures for RMFB Paper<br/>Part I: CNN Baseline with Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103e480-bcf9-4b03-b923-37395f95865b",
   "metadata": {},
   "source": [
    "## Rebuilding All Models with Latest Versions as well as<br/>Doing Learned Visualization of Architecture Structure<sup>\\[NB1\\]\\[NB2\\]\\[NB3\\]</sup><br/>and Especially Grad-CAM<sup>\\[NB4\\]</sup> Visual Classification Explanations  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4900f2-a2e8-4292-a3b1-584f6e9ab275",
   "metadata": {},
   "source": [
    "David BLACK, GitHub @bballdave025 ,\n",
    "\n",
    "Working Title for Technical Paper: &quot;AI Computer Vision Methods for\n",
    "Finding Information-bearing Writing Surfaces (Especially Codex Fragments) Reused in Other Codex Bindings&quot;\n",
    "\n",
    "Working Title for <em>Fragmentology</em><sup>[NB5]</sup>\n",
    "(Manuscript Studies) Paper: None Yet\n",
    "\n",
    "(It would be nice to have some statistics before making a title.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8509c5-9a01-4ee0-9201-2587cb68bb9c",
   "metadata": {},
   "source": [
    "## Part 1-a: CNN Baseline, Visualizations, GradCAM, all with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6530a-4c8b-46ed-976b-41f8dbbca43f",
   "metadata": {},
   "source": [
    "#### Why these Notebooks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cb2cf-df6c-4aa6-8fea-b1d69c7983ff",
   "metadata": {},
   "source": [
    "There are several reasons for writing these notebooks and doing these various visualizations. The most acceptable (I almost put that in quotation marks; I mean the most-what-employers-or-theoretical-AI/ML-people-would-like-to-hear) answers are: <br/>&nbsp;&nbsp;&nbsp;1) I want to give <strong>greater explainability of the CV model  decisions</strong>,  especially as it relates to the audience of the paper and their acceptance of a non-manual, non-human method, possibly seen as an intruder  into the Manuscript Studies community. I think that if they see the model  making decisions in the same way that <em>they</em> would make decisions, they will be more likely to see it as an aide doing such things as allowing grad students to analyze fragments rather than finding them. For clarity, I mention that the audience consists of the readers of the Binding-Fragment paper to be submitted to <em>Fragmentology</em>; <br/>&nbsp;&nbsp;&nbsp;2) I want to make better decisions as to which model will most-likely make the <strong>goal of ~95% precision and ~70% recall possible on a wide variety of writing-material and binding images</strong>. The ultimate goal for (this stage of) the study is to run the algorithm over as many of FamilySearch's earliest-to-18th Century images (let's make it readable: images from the earliest records they have up to the 1700s) as possible. Through the analysis and visualization of how well the chosen model or models performs, we can have higher confidence that an eventual <strong>one run</strong> over what I'll estimate as 0.5 to 1 billion images with a thoroughly tested and understood/explainable model will <strong>suffice for at least a decade</strong>. <br/>&nbsp;&nbsp;&nbsp;3) (This is a long one.) Especially with the Grad-CAM visualizations of the salient features for model decisions, I want to have something to <strong>show to the Manuscript Studies/Fragmentology community that will get them excited</strong> about <em>helping us</em> to <em>create better training sets</em> and to <em>help create new training sets for interesting phenomena/occurences</em> (for lack of a better word describing what each of the classifications describes). I think that the best example of this is to create a model (or add to the existing model) the search for images that not only have iron-gall-ink damage to the point of going through the page (something I have tried to do without expertise), but which are at different stages<sup>\\[NB5\\]</sup> of the iron-gall-ink (and other corrosive dye) damage process. I'm also going to face the fact that, even as concerns the main object of studyâ€”fragments found in bindingsâ€”I'm perhaps passingly knowledgeable but by no means an expert. Seeing heat maps describing what the model thinks is most important in finding a specific images I've classified should make this a more interesting invitation, as it allows quick view of the types of phenomena such models can find in and on writing materials, bindings, etc.\n",
    "\n",
    "For my estimate of the sum total of such documents, I make reference to the <em>Company Facts</em> page on FamilySearch [as I see it now (archived with numbers updated 2025-04-07)](https://web.archive.org/web/20250423044740/https://www.familysearch.org/en/newsroom/company-facts), which reports \"5.65 Billion \\[Digital Images Published\\]\" . For a second opinion, I've looked at the running counter of images digitized at https://www.familysearch.org/en/records/images/, which is at \\[let me go take a screenshot\\]\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/FamilySearch_5667347462_5point667-B_Images_2025-04-22T113500-0600.png\"\n",
    "       alt=\"Screenshot of family search dot org forward slash E N forward slash records forward slash images. Tab name is. QUOTE. Explore Historical Images. END QUOTE. Visible text is. QUOTE. Access billions of documents newline FamilySearch has been collecting historical documents since eighteen ninety four and currently has. Left square bracket. Transcriber's note colon. Number in short scale, i.e. without milliard, billiard, et cetera, so ten to the ninth is read billion, ten to the twelfth is read trillion, et cetera. Right bracket. five billion six hundred sixty-seven million, three hundred forty-seven thousand four hundred sixty-one images from all over the world. That makes FamilySearch one of the world's largest collections of historical documents exclamation point. END QUOTE. Bottom toolbar shows eleven colon thirty-five A M. and. four slash twenty-two forward slash two thousand twenty-five. Left Bracket. Date format is as in the stupid U S A format comma month slash day slash year. Right Bracket. An errow points from the annotated text. QUOTE. M D T. Left parenthesis. G M T minus six Right parenthesis. END QUOTE. to the time and date.\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "5,667,347,461 (five billion six hundred sixty-seven million, three hundred forty-seven thousand four hundred sixty-one) images. The type of binding we're seeking is most-commonly found in Western and Central Europe going into Russia and Persia, with some around the Mediterraneanâ€”including Muslim and Jewish recordsâ€”the Near East, and the Americasâ€”mostly (Catholic) South America. Since I think that records from the current United States of America probably take up at least a third of all digital images published, and Western/Central Europe <em>at least</em> one quarter of the remaining digital images, my guess is that a search in those geographical regions from the earliest date for which FamilySearch has records to 1799 will yield approximately<br/>$ \\frac{2}{3} \\cdot \\frac{1}{4} \\cdot 5.66 \\times 10^{9} $ images or $ 9.45 \\times 10^{8} \\left(\\substack{+9e8 \\\\ -5e9} \\: \\mathrm{systematic}\\right) \\left(\\pm 0.25e9 \\;  \\mathrm{who} \\! \\cdot \\! \\mathrm{knows}\\right) $,<br/>$ \\frac{1}{6}\\;\\mathrm{ish} $ of the records, 500 million to 1.5 billion (again, very $ \\mathrm{ish} $).<br/>(Note that my methodology will let in records from the 1800s and even a few from the 1900s, but not on purpose.)\n",
    "\n",
    "The current study will include around hundreds of thousands of images, with an approximate guess of half being from FamilySearch, for the intial study and publication in <em>Fragmentology</em>. This study entails at least thousands of images, up to tens of thousands, for the training, validation, and test sets. (That is, tens of thousands of images split into the three sets.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2405510-aa50-47ab-8544-52b1a4b07da9",
   "metadata": {},
   "source": [
    "### Some Visualization Ruminations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6507fe4-c01d-4148-bbbb-558d767b4e16",
   "metadata": {},
   "source": [
    "#### New"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759ed47-896d-45eb-8bfc-962d176e82f7",
   "metadata": {},
   "source": [
    "I'm going to try using the network architecture visualization tools:\n",
    "\n",
    "#### [Netron (non-GitHub site)](https://netron.app/)\n",
    "\n",
    "https://github.com/lutzroeder/netron?tab=readme-ov-file\n",
    "\n",
    "<strike>https://github.com/lutzroeder/netron/releases/tag/v8.3.9 (has downloads)</strike>\n",
    "\n",
    "Luckily, we don't have to worry about OS package manager downloads here on SageMaker, \n",
    "because we can do the following, according to the README\n",
    "\n",
    "> `pip install netron`, then run `netron [FILE]` or `netron.start('[FILE]')`\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<strong>AS WELL AS</strong> the architecture and convolution result visualization tool, \n",
    "\n",
    "<hr/>\n",
    "\n",
    "#### [Quiver](https://github.com/keplr-io/quiver)\n",
    "\n",
    "https://github.com/keplr-io/quiver\n",
    "\n",
    "```\n",
    "misc{bianquiver,\n",
    "  title={Quiver},\n",
    "  author={Bian, Jake},\n",
    "  year={2016},\n",
    "  publisher={GitHub},\n",
    "  howpublished={\\url{https://github.com/keplr-io/quiver}},\n",
    "}\n",
    "```\n",
    "\n",
    "Luckily, it's easily installed with\n",
    "\n",
    "`pip install quiver_engine`\n",
    "\n",
    "and used as\n",
    "\n",
    "```\n",
    "# really quickly\n",
    "\n",
    "# Build your model in Keras\n",
    "model = Model(...)\n",
    "\n",
    "quiver_engine.server.launch(model, classes=['cat','dog'], input_folder='./imgs')\n",
    "```\n",
    "\n",
    "More deeply:\n",
    "\n",
    "> <strong>Usage</strong>\n",
    ">\n",
    "> Take your keras `model`, launching Quiver is a one-liner.\n",
    "> ```\n",
    "> from quiver_engine import server\n",
    "> server.launch(model)\n",
    "> ```\n",
    ">\n",
    "> This will launch the visualization at `localhost:5000`\n",
    ">\n",
    "> <strong>Options</strong>\n",
    ">\n",
    "> ```\n",
    ">    server.launch(\n",
    ">        model, # a Keras Model\n",
    ">\n",
    ">        classes, # list of output classes from the model to present (if not specified 1000 ImageNet classes will be used)\n",
    ">\n",
    ">        top, # number of top predictions to show in the gui (default 5)\n",
    ">\n",
    ">        # where to store temporary files generatedby quiver (e.g. image files of layers)\n",
    ">        temp_folder='./tmp',\n",
    ">\n",
    ">        # a folder where input images are stored\n",
    ">        input_folder='./',\n",
    ">\n",
    ">        # the localhost port the dashboard is to be served on\n",
    ">        port=5000,\n",
    ">        # custom data mean\n",
    ">        mean=[123.568, 124.89, 111.56],\n",
    ">        # custom data standard deviation\n",
    ">        std=[52.85, 48.65, 51.56]\n",
    ">    )\n",
    "> ```\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Oh, looks like Quiver is just for Keras. Well, with PyTorch, I can look at\n",
    "\n",
    "#### [TorchLens](https://github.com/johnmarktaylor91/torchlens)\n",
    "\n",
    "https://github.com/johnmarktaylor91/torchlens\n",
    "\n",
    "[On PyPI](https://pypi.org/project/torchlens/) (so from `pip`)\n",
    "\n",
    "[TorchLens (Nature article HTML)](https://www.nature.com/articles/s41598-023-40807-0)\n",
    "\n",
    "[TorchLens (Nature article pdf download](https://www.nature.com/articles/s41598-023-40807-0.pdf)\n",
    "\n",
    "[NIH National Library of Medicine](https://pubmed.ncbi.nlm.nih.gov/36993311/)\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Note that a really nice option &ndash; deepvis &ndash; is mentioned by the Quiver author\n",
    "as an inspiration, but is only available with Caffe.\n",
    "\n",
    "https://github.com/yosinski/deep-visualization-toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77fc88-bdcc-4664-9555-16daee47ca17",
   "metadata": {},
   "source": [
    "#### Attempted before - and things worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbac821-9e93-4042-87a3-57adfa50dea8",
   "metadata": {},
   "source": [
    "`model.summary()` and I think there was some kind of `print(model)`\n",
    "\n",
    "Then\n",
    "\n",
    "```\n",
    "model_arch_str = str(model)\n",
    "\n",
    "with open(\"this_model_architecture.txt\", 'w', encoding='utf-8') as fh:\n",
    "    fh.write(model_arch_str)\n",
    "##endof:  with open ... fh\n",
    "```\n",
    "\n",
    "Also\n",
    "\n",
    "`visualkeras`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f88c52-dc07-45bd-96fa-8f1fa1cf9b91",
   "metadata": {},
   "source": [
    "### Most Important Visualization, GradCAM, for Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f514c9-470c-42a4-b219-d9eff424cda7",
   "metadata": {},
   "source": [
    "[Paper on arXiv](https://arxiv.org/abs/1610.02391) - Landing Page\n",
    "\n",
    "[Download PDF from arXiv](https://arxiv.org/pdf/1610.02391)\n",
    "\n",
    "<hr/>\n",
    "\n",
    "#### For TensorFlow\n",
    "\n",
    "##### By Keras\n",
    "\n",
    "This first one might be for `tensorflow<2`\n",
    "\n",
    "https://keras.io/examples/vision/grad_cam/\n",
    "\n",
    "##### By @ismailuddin\n",
    "\n",
    "The following is specifically for TensorFlow2. And it's WONDERFUL! Great, accessible math.\n",
    "\n",
    "https://github.com/ismailuddin/gradcam-tensorflow-2\n",
    "\n",
    "He also references @jacobgil\n",
    "\n",
    "```\n",
    "$ type atree\n",
    "atree is aliased to `tree --charset=ascii'\n",
    "\n",
    "$ atree gradcam-tensorflow-2\n",
    "gradcam-tensorflow-2\n",
    "|-- README.md\n",
    "|-- data\n",
    "|   |-- cat.jpg\n",
    "|   |-- cat_and_dog.jpg\n",
    "|   `-- example.jpg\n",
    "|-- notebooks\n",
    "|   `-- GradCam.ipynb\n",
    "`-- requirements.txt\n",
    "\n",
    "2 directories, 6 files\n",
    "\n",
    "$\n",
    "```\n",
    "\n",
    "\n",
    "##### By PyImageSearch\n",
    "\n",
    "And, of course, here's my good, marketing-happy buddy, Adrian Rosebrock\n",
    "\n",
    "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n",
    "\n",
    "<hr/>\n",
    "\n",
    "#### For the PyTorch models\n",
    "\n",
    "##### By @jacobgil\n",
    "\n",
    "https://github.com/jacobgil/pytorch-grad-cam\n",
    "\n",
    "```\n",
    "@misc{jacobgilpytorchcam,\n",
    "  title={PyTorch library for CAM methods},\n",
    "  author={Jacob Gildenblat and contributors},\n",
    "  year={2021},\n",
    "  publisher={GitHub},\n",
    "  howpublished={\\url{https://github.com/jacobgil/pytorch-grad-cam}},\n",
    "}\n",
    "```\n",
    "\n",
    "This @jacobgil stuff has lots of good references\n",
    "\n",
    "> https://arxiv.org/abs/1610.02391<br/>\n",
    "> `Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra`\n",
    ">\n",
    "> https://arxiv.org/abs/2011.08891<br/>\n",
    "> `Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks Rachel L. Draelos, Lawrence Carin`\n",
    ">\n",
    "> https://arxiv.org/abs/1710.11063<br/>\n",
    "> `Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian`\n",
    ">\n",
    "> https://arxiv.org/abs/1910.01279<br/>\n",
    "> `Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu`\n",
    ">\n",
    "> https://ieeexplore.ieee.org/abstract/document/9093360/<br/>\n",
    "> `Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization. Saurabh Desai and Harish G Ramaswamy. In WACV, pages 972â€“980, 2020`\n",
    ">\n",
    "> https://arxiv.org/abs/2008.02312<br/>\n",
    "> `Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li`\n",
    ">\n",
    "> https://arxiv.org/abs/2008.00299<br/>\n",
    "> `Eigen-CAM: Class Activation Map using Principal Components Mohammed Bany Muhammad, Mohammed Yeasin`\n",
    ">\n",
    "> http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf<br/>\n",
    "> `LayerCAM: Exploring Hierarchical Class Activation Maps for Localization Peng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei`\n",
    ">\n",
    "> https://arxiv.org/abs/1905.00780<br/>\n",
    "> `Full-Gradient Representation for Neural Network Visualization Suraj Srinivas, Francois Fleuret`\n",
    ">\n",
    "> https://arxiv.org/abs/1806.10206<br/>\n",
    "> `Deep Feature Factorization For Concept Discovery Edo Collins, Radhakrishna Achanta, Sabine SÃ¼sstrunk`\n",
    ">\n",
    "> https://arxiv.org/abs/2410.00267<br/>\n",
    "> `KPCA-CAM: Visual Explainability of Deep Computer Vision Models using Kernel PCA Sachin Karmani, Thanushon Sivakaran, Gaurav Prasad, Mehmet Ali, Wenbo Yang, Sheyang Tang`\n",
    ">\n",
    "> https://hal.science/hal-02963298/document<br/>\n",
    "> `Features Understanding in 3D CNNs for Actions Recognition in Video Kazi Ahmed Asif Fuad, Pierre-Etienne Martin, Romain Giot, Romain Bourqui, Jenny Benois-Pineau, Akka Zemmar`\n",
    ">\n",
    "> https://arxiv.org/abs/2501.06261<br/>\n",
    "> `CAMs as Shapley Value-based Explainers Huaiguang Cai`\n",
    ">\n",
    "> https://arxiv.org/pdf/2501.11309<br/>\n",
    "> `Finer-CAM : Spotting the Difference Reveals Finer Details for Visual Explanation`<br/>\n",
    "> `Ziheng Zhang*, Jianyang Gu*, Arpita Chowdhury, Zheda Mai, David Carlyn,Tanya Berger-Wolf, Yu Su, Wei-Lun Chao`\n",
    "\n",
    "and tutorial links\n",
    "\n",
    "> Here you can find detailed examples of how to use this for various custom use cases like object detection:\n",
    ">\n",
    "> These point to the new documentation jupter-book for fast rendering. The jupyter notebooks themselves can be found under the tutorials folder in the git repository.\n",
    ">\n",
    "> [Notebook tutorial: XAI Recipes for the HuggingFace ðŸ¤— Image Classification Models](https://jacobgil.github.io/pytorch-gradcam-book/HuggingFace.html)\n",
    ">\n",
    "> [Notebook tutorial: Deep Feature Factorizations for better model explainability](https://jacobgil.github.io/pytorch-gradcam-book/Deep%20Feature%20Factorizations.html)\n",
    ">\n",
    "> [Notebook tutorial: Class Activation Maps for Object Detection with Faster-RCNN](https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.html)\n",
    ">\n",
    "> [Notebook tutorial: Class Activation Maps for YOLO5](https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html)\n",
    ">\n",
    "> [Notebook tutorial: Class Activation Maps for Semantic Segmentation](https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html)\n",
    ">\n",
    "> [Notebook tutorial: Adapting pixel attribution methods for embedding outputs from models](https://jacobgil.github.io/pytorch-gradcam-book/Pixel%20Attribution%20for%20embeddings.html)\n",
    ">\n",
    "> [Notebook tutorial: May the best explanation win. CAM Metrics and Tuning](https://jacobgil.github.io/pytorch-gradcam-book/CAM%20Metrics%20And%20Tuning%20Tutorial.html)\n",
    ">\n",
    "> [How it works with Vision/SwinT transformers](https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/vision_transformers.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f50dda-a363-44e5-b96f-bc356cfd314e",
   "metadata": {},
   "source": [
    "#### File structure for repo cloned from @jacobgil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e570d1b-e90d-451a-87e7-1cf6c5139305",
   "metadata": {},
   "source": [
    "```\n",
    "$ type atree\n",
    "atree is aliased to `tree --charset=ascii'\n",
    "\n",
    "$ atree pytorch-grad-cam/\n",
    "pytorch-grad-cam/\n",
    "|-- LICENSE\n",
    "|-- MANIFEST.in\n",
    "|-- README.md\n",
    "|-- cam.py\n",
    "|-- examples\n",
    "|   |-- augsmooth.jpg\n",
    "|   |-- both.png\n",
    "|   |-- both_detection.png\n",
    "|   |-- cam_gb_dog.jpg\n",
    "|   |-- cars_segmentation.png\n",
    "|   |-- cat.jpg\n",
    "|   |-- clip_cat.jpg\n",
    "|   |-- clip_dog.jpg\n",
    "|   |-- dff1.png\n",
    "|   |-- dff2.png\n",
    "|   |-- dog.jpg\n",
    "|   |-- dog_cat.jfif\n",
    "|   |-- dogs.png\n",
    "|   |-- dogs_gradcam++_resnet50.jpg\n",
    "|   |-- dogs_gradcam++_vgg16.jpg\n",
    "|   |-- dogs_gradcam_resnet50.jpg\n",
    "|   |-- dogs_gradcam_vgg16.jpg\n",
    "|   |-- dogs_scorecam_resnet50.jpg\n",
    "|   |-- dogs_scorecam_vgg16.jpg\n",
    "|   |-- eigenaug.jpg\n",
    "|   |-- eigensmooth.jpg\n",
    "|   |-- embeddings.png\n",
    "|   |-- horses.jpg\n",
    "|   |-- metrics.png\n",
    "|   |-- multiorgan_segmentation.gif\n",
    "|   |-- nosmooth.jpg\n",
    "|   |-- resnet50_cat_ablationcam_cam.jpg\n",
    "|   |-- resnet50_cat_gradcam_cam.jpg\n",
    "|   |-- resnet50_cat_scorecam_cam.jpg\n",
    "|   |-- resnet50_dog_ablationcam_cam.jpg\n",
    "|   |-- resnet50_dog_gradcam_cam.jpg\n",
    "|   |-- resnet50_dog_scorecam_cam.jpg\n",
    "|   |-- resnet_horses_ablationcam_cam.jpg\n",
    "|   |-- resnet_horses_gradcam++_cam.jpg\n",
    "|   |-- resnet_horses_gradcam_cam.jpg\n",
    "|   |-- resnet_horses_horses_eigencam_cam.jpg\n",
    "|   |-- resnet_horses_scorecam_cam.jpg\n",
    "|   |-- road.png\n",
    "|   |-- swinT_cat_ablationcam_cam.jpg\n",
    "|   |-- swinT_cat_gradcam_cam.jpg\n",
    "|   |-- swinT_cat_scorecam_cam.jpg\n",
    "|   |-- swinT_dog_ablationcam_cam.jpg\n",
    "|   |-- swinT_dog_gradcam_cam.jpg\n",
    "|   |-- swinT_dog_scorecam_cam.jpg\n",
    "|   |-- vgg_horses_ablationcam_cam.jpg\n",
    "|   |-- vgg_horses_eigencam_cam.jpg\n",
    "|   |-- vgg_horses_gradcam++_cam.jpg\n",
    "|   |-- vgg_horses_gradcam_cam.jpg\n",
    "|   |-- vgg_horses_scorecam_cam.jpg\n",
    "|   |-- vit_cat_ablationcam_cam.jpg\n",
    "|   |-- vit_cat_gradcam_cam.jpg\n",
    "|   |-- vit_cat_scorecam_cam.jpg\n",
    "|   |-- vit_dog_ablationcam_cam.jpg\n",
    "|   |-- vit_dog_gradcam_cam.jpg\n",
    "|   |-- vit_dog_scorecam_cam.jpg\n",
    "|   `-- yolo_eigencam.png\n",
    "|-- pyproject.toml\n",
    "|-- pytorch_grad_cam\n",
    "|   |-- __init__.py\n",
    "|   |-- ablation_cam.py\n",
    "|   |-- ablation_cam_multilayer.py\n",
    "|   |-- ablation_layer.py\n",
    "|   |-- activations_and_gradients.py\n",
    "|   |-- base_cam.py\n",
    "|   |-- eigen_cam.py\n",
    "|   |-- eigen_grad_cam.py\n",
    "|   |-- feature_factorization\n",
    "|   |   |-- __init__.py\n",
    "|   |   `-- deep_feature_factorization.py\n",
    "|   |-- fem.py\n",
    "|   |-- finer_cam.py\n",
    "|   |-- fullgrad_cam.py\n",
    "|   |-- grad_cam.py\n",
    "|   |-- grad_cam_elementwise.py\n",
    "|   |-- grad_cam_plusplus.py\n",
    "|   |-- guided_backprop.py\n",
    "|   |-- hirescam.py\n",
    "|   |-- kpca_cam.py\n",
    "|   |-- layer_cam.py\n",
    "|   |-- metrics\n",
    "|   |   |-- __init__.py\n",
    "|   |   |-- cam_mult_image.py\n",
    "|   |   |-- perturbation_confidence.py\n",
    "|   |   `-- road.py\n",
    "|   |-- random_cam.py\n",
    "|   |-- score_cam.py\n",
    "|   |-- shapley_cam.py\n",
    "|   |-- sobel_cam.py\n",
    "|   |-- utils\n",
    "|   |   |-- __init__.py\n",
    "|   |   |-- find_layers.py\n",
    "|   |   |-- image.py\n",
    "|   |   |-- model_targets.py\n",
    "|   |   |-- reshape_transforms.py\n",
    "|   |   `-- svd_on_activations.py\n",
    "|   `-- xgrad_cam.py\n",
    "|-- requirements.txt\n",
    "|-- setup.cfg\n",
    "|-- setup.py\n",
    "|-- tests\n",
    "|   |-- test_context_release.py\n",
    "|   |-- test_one_channel.py\n",
    "|   `-- test_run_all_models.py\n",
    "|-- tutorials\n",
    "|   |-- CAM Metrics And Tuning Tutorial.ipynb\n",
    "|   |-- Class Activation Maps for Object Detection With Faster RCNN.ipynb\n",
    "|   |-- Class Activation Maps for Semantic Segmentation.ipynb\n",
    "|   |-- Deep Feature Factorizations.ipynb\n",
    "|   |-- EigenCAM for YOLO5.ipynb\n",
    "|   |-- HuggingFace.ipynb\n",
    "|   |-- Pixel Attribution for embeddings.ipynb\n",
    "|   |-- bbox.png\n",
    "|   |-- huggingface_dff.png\n",
    "|   |-- multimage.png\n",
    "|   |-- puppies.jpg\n",
    "|   `-- vision_transformers.md\n",
    "`-- usage_examples\n",
    "    |-- clip_example.py\n",
    "    |-- swinT_example.py\n",
    "    `-- vit_example.py\n",
    "\n",
    "8 directories, 117 files\n",
    "\n",
    "$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402ce9d5-e1bf-420a-baa0-7b9d73d0e5f6",
   "metadata": {},
   "source": [
    "## Starting on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a1119-217e-4e6f-9d1d-c0560d139574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only-for-start#!date +'%s_%Y-%m-%dT%H:%M:%S:%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d86fa4-13d7-4353-b120-b736bde4423b",
   "metadata": {},
   "source": [
    "Original output when first starting SageMaker\n",
    "\n",
    "`1750120224_2025-06-17T00:30:24:+0000`<br/>\n",
    "Local: `2025-06-16T18:30:24-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7808b-9e64-4d63-87ed-e0c9d10efba4",
   "metadata": {},
   "source": [
    "### Imports, noting deprecated imports that would have been used in previous versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24563893-0192-4e47-ab23-41239d7fb170",
   "metadata": {},
   "source": [
    "`Pandas` is complaining on 2025-06-16, so we'll have an\n",
    "\n",
    "### Install for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579e4685-f8ac-43e9-a325-76cb08ee878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -q -y numexpr\n",
    "\n",
    "#!echo \"yes\" | pip install -q --exists-action i \"numexpr==2.8.4\"\n",
    "#or possibly\n",
    "!pip install -q --no-input \"numexpr==2.8.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce8bca9-756d-4689-bf13-8421bf68102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 00:32:07.972119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-17 00:32:07.997754: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-17 00:32:07.997795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-17 00:32:08.013964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-17 00:32:09.018743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numexpr\n",
    "import pandas as pd\n",
    "\n",
    "####  In keras.io/examples/vision/grad_cam/ \n",
    "####+ (2017 book references, created 2020, modified 2021)\n",
    "##import os\n",
    "##os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "##from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "  #  for: Sequential\n",
    "# from [tensorflow.]keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "  #  for: Dense, Activation, Conv2D, MaxPooling2D,\n",
    "  #+      Dropout, Flatten, Input\n",
    "\n",
    "#  from [tensorflow.]keras.layers import \\\n",
    "#+               Dense, Dropout, Activation,\n",
    "#+               Flatten, Conv2D, MaxPooling2D\n",
    "# from [tensorflow.]keras.layers import Activation\n",
    "# from [tensorflow.]keras.layers.core import Dense, Flatten\n",
    "# from [tensorflow.]keras.layers.core import Dropout\n",
    "# from [tensorflow.]keras.layers.convolutional import Conv2D\n",
    "# from [tensorflow.]keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "  # for: Adam\n",
    "#from [tensorflow.]keras.optimizers import Adam\n",
    "from tensorflow.keras import metrics\n",
    "  # for: categorical_crossentropy\n",
    "#from [tensorflow.]keras.metrics import categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras import losses\n",
    "  # for: sparse_categorical_crossentropy\n",
    "#from [tensorflow.]keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "# Callbacks (logging and seeing the model train)\n",
    "from tensorflow.keras import callbacks\n",
    "  #  for: CSVLogger, Callback  ## custom callback\n",
    "  #+      (maybe EarlyStopping, ModelCheckpoint)\n",
    "#from [tensorflow.]keras.callbacks import CSVLogger\n",
    "\n",
    "# for Grad-CAM in TensorFlow 2.x\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "# Dataset\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "##from tensorflow.keras.applications.resnet50 import (\n",
    "##  ResNet50,\n",
    "##  preprocess_input,\n",
    "##  decode_predictions,\n",
    "##)\n",
    "\n",
    "import tensorboard\n",
    "\n",
    "##########################################################\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "##----------------\n",
    "#  Image stuff\n",
    "##----------------\n",
    "import cv2\n",
    "from PIL import ImageFont\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "  # Remember that Colab doesn't like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e03c2b-ed8f-4afd-ad1c-8bfd5df634e6",
   "metadata": {},
   "source": [
    "Output (warnings, errors, info) came up as\n",
    "\n",
    "```\n",
    "2025-06-16 19:52:06.186069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "2025-06-16 19:52:14.321964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "2025-06-16 19:52:14.396558: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "2025-06-16 19:52:23.964297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "2025-06-16 19:52:36.233734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
    "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
    "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "Matplotlib is building the font cache; this may take a moment.\n",
    "UsageError: unrecognized arguments: # Remember that Colab doesn't like this.\n",
    "```\n",
    "\n",
    "It seems that no installs are needed for SageMaker, though a newer `numexpr` might be nice.\n",
    "\n",
    "With the `numexpr` install included, I didn't get output. I hope that means everything's okay.\n",
    "\n",
    "Okay, restarted kernel and cleared output, then I got the following output (later on 2025-06-16 MDT)\n",
    "\n",
    "```\n",
    "2025-06-17 00:32:07.972119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "2025-06-17 00:32:07.997754: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "2025-06-17 00:32:07.997795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "2025-06-17 00:32:08.013964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "2025-06-17 00:32:09.018743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7dc61-732a-465c-8241-ca5b6064e12a",
   "metadata": {},
   "source": [
    "<strong>Imports for more specialized</strong>\n",
    "\n",
    "(e.g. looking at documentation, timing things, seeing the date and time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e760e671-5051-4061-9191-39f2579a85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "  ## documentation\n",
    "import timeit\n",
    "  ## timing (code profiling)\n",
    "from datetime import datetime\n",
    "  ## get the date and time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949567c-51c8-4061-8095-0f3ea685bebb",
   "metadata": {},
   "source": [
    "<strong>See if we need installation of stuff here on SageMaker</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c6b5c1-40d2-4856-a69f-2c5a526ef311",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-input humanfriendly\n",
    "!pip install -q --no-input visualkeras\n",
    "!pip install -q --no-input netron\n",
    "#!pip install quiver_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57162240-fe51-4b32-9147-b1643ebb60ff",
   "metadata": {},
   "source": [
    "Got an error trying `quiver_engine`, 2025-06-16T18:38:00-0600\n",
    "\n",
    "```\n",
    "Collecting quiver_engine\n",
    "  Using cached quiver_engine-0.1.4.1.4.tar.gz (398 kB)\n",
    "  Preparing metadata (setup.py) ... error\n",
    "  error: subprocess-exited-with-error\n",
    "  \n",
    "  Ã— python setup.py egg_info did not run successfully.\n",
    "  â”‚ exit code: 1\n",
    "  â•°â”€> [1 lines of output]\n",
    "      error in quiver_engine setup command: \"values of 'package_data' dict\" must be of type <tuple[str, ...] | list[str]> (got 'quiverboard/dist/*')\n",
    "      [end of output]\n",
    "  \n",
    "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
    "error: metadata-generation-failed\n",
    "\n",
    "Ã— Encountered error while generating package metadata.\n",
    "â•°â”€> See above for output.\n",
    "\n",
    "note: This is an issue with the package mentioned above, not pip.\n",
    "hint: See above for details.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfd70b4-8946-453a-a212-9961d33096e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import humanfriendly\n",
    "import visualkeras\n",
    "import netron\n",
    "#from quiver_engine import server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493b1a2-cfa4-45cc-9844-e58e07f480d0",
   "metadata": {},
   "source": [
    "## Make sure you check out the environment specifics at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc30b0-9c56-4454-bc96-401351b2c065",
   "metadata": {},
   "source": [
    "## Models. Hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4ef9f-00d9-4973-bf88-f7b304af4392",
   "metadata": {},
   "source": [
    "### Baseline &ndash; Standard CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764bda72-52c7-408b-a64e-86ac29d6f52c",
   "metadata": {},
   "source": [
    "### Let's bring in the CIFAR10 Dataset &ndash; Quick and Reckless Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10841467-5a7b-4640-9519-6cabd2b0e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's try with the CIFAR10 Dataset. Why not?\n",
    "#+ I'm not sure how dataset imports work with tensorflow, so\n",
    "#+ I'm only importing `cifar10` and not\n",
    "#+ from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "## Moving the import up\n",
    "#from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c89bc0-2c28-4de3-89f8-310cfac11ba9",
   "metadata": {},
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d69e27-921b-4be1-90e1-f7845c571288",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89f6e0-436b-4331-a0df-b34cd2344a2e",
   "metadata": {},
   "source": [
    "### Now to start the inspection and the CNN code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcef0f2-3759-497d-b903-fc806fe51e64",
   "metadata": {},
   "source": [
    "Pick an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920e211-81b4-458a-b3e0-7ad3677accdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_for_inspection = x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58d796-afae-42a1-8900-aef9ee8d14ef",
   "metadata": {},
   "source": [
    "Let's have a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f27d70-fd0c-45d3-9a65-66a8d2ec85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.imshow(image_for_inspection)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7a861-bb80-4ffd-b897-3f52170cf28d",
   "metadata": {},
   "source": [
    "Inspect the image a bit more, so we know how we need to normalize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8e4fd-0cfc-450b-bbaf-a824d3a5f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pixels_to_inspect = 5\n",
    "\n",
    "print(f\"Image shape: {image_for_inspection.shape}\")\n",
    "top_corner_array = \\\n",
    "    image_for_inspection[0:n_pixels_to_inspect, :]\n",
    "print(f\"Top left corner values:\\n{top_corner_array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a6de3-49cc-4cbd-af55-63befb5db01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( (\n",
    "    \"\\n\\nNotice that stacking them side-by-side would give us \\n\"\n",
    "    \"the RGB values as they'd be seen in the image, but \\n\"\n",
    "    \"I'm going for quick stuff. Assuming 255 would be okay, \\n\"\n",
    "    \"or perhaps I should say reasonable, but let's do a better\\n\"\n",
    "    \"check.\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print( (\n",
    "    f\"Minimum single RGB value: {min(image_for_inspection.flatten())}\\n\"\n",
    "    f\"Maximum single RGB value: {max(image_for_inspection.flatten())}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbc88c-45d6-4fa0-9945-e3e357f8e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd600193-bded-44d6-bcdd-58eee2d18027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cool visualization of some of our new data, but first\n",
    "#+ let's double check the image size\n",
    "\n",
    "print(f\"Another image's shape: {x_train[1].shape}\")\n",
    "\n",
    "n_pixels_per_side = 32\n",
    "n_channels = 3\n",
    "\n",
    "#  Making sure it's what we expect\n",
    "assert x_train[1].shape == (n_pixels_per_side,\n",
    "                            n_pixels_per_side,\n",
    "                            n_channels\n",
    "                           )\n",
    "\n",
    "print(\"If you've gotten here, the shape is correct, \", end=\"\")\n",
    "print(f\"({n_pixels_per_side}, {n_pixels_per_side}, {n_channels})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc60feb-842b-4222-b30b-46cc097284dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The class names with indexes as defined by CIFAR\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "#  Let's show some images\n",
    "good_figsize_val = 8\n",
    "plt.figure(figsize=(good_figsize_val, good_figsize_val))\n",
    "\n",
    "n_rows = 5\n",
    "n_cols = n_rows  # Let's make our output of images square\n",
    "\n",
    "n_images_to_show = n_rows * n_cols\n",
    "for i in range(n_images_to_show):\n",
    "  plt.subplot(n_rows, n_cols, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(x_train[i])\n",
    "  #  CIFAR labels are arrays, which is\n",
    "  #+ why we need the extra index.\n",
    "  plt.xlabel(class_names[y_train[i][0]])\n",
    "##endof:  for i in range(n_images_to_show)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be9fdf-080a-405d-a4d0-717e7f9bf273",
   "metadata": {},
   "source": [
    "#### I want to see exactly what the `filters` parameter is. This can be minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e87e0a-54ee-4fd0-a8e3-75c2ee81901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(inspect.getdoc(layers.Conv2D).splitlines()[:54]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed30c70-62e4-4609-9b7e-496cdf9beabd",
   "metadata": {},
   "source": [
    "#### I wanted to see exactly what the `filters` parameter was:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c687f0b-ddc8-4583-9b54-98dd9e285353",
   "metadata": {},
   "source": [
    "```\n",
    "filters: int, the dimension of the output space (the number of filters\n",
    "         in the convolution).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729fe17-24d4-48bc-be9a-2bcad51d922a",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571aeed-23b1-4d0a-89a8-8100b47924e2",
   "metadata": {},
   "source": [
    "### Shell/Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6955d-b6dc-4e5e-8cd1-6d070af0bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = models.Sequential()  # the shell of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c18a7-53e2-4c91-97fb-b24c5f854082",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c2d78-ce0e-4ad3-88a5-8049a44a2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Input layer\n",
    "\n",
    "n_pix = n_pixels_per_side\n",
    "\n",
    "in_0 = layers.Input(shape(n_pix, n_pix, n_channels))\n",
    "model_0.add(in_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13a773-1f84-4371-b5a6-9ecf6d935252",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04093952-0beb-4172-bf54-192c14437993",
   "metadata": {},
   "source": [
    "### First Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82db3a-7571-48d6-be32-94a29cb55145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  First convolutional layer\n",
    "\n",
    "n_filters_1 = 32\n",
    "\n",
    "conv_1 = layers.Conv2D(filters=n_filters_1,\n",
    "                       kernel_size=(3, 3),\n",
    "                       padding='same'\n",
    "                      )\n",
    "\n",
    "model_0.add(conv_1)\n",
    "model_0.add(layers.Activation('relu'))\n",
    "\n",
    "##  Same as\n",
    "#conv_1_2 = layers.Conv2D(filters=32,\n",
    "#                         kernel_size=(3, 3),\n",
    "#                         activation='relu',\n",
    "#                         padding='same'\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fe4a7-f97d-431a-8c0c-589bd41a79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6a90c-d1e3-47c6-9636-39a38a4316eb",
   "metadata": {},
   "source": [
    "### First Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b2b96-d114-4d64-aca8-abc846ef2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  First pooling layer\n",
    "\n",
    "pool_1 = layers.MaxPooling2D(pool_size=(2, 2),\n",
    "                             strides=2,\n",
    "                             padding='valid'\n",
    "                            )\n",
    "  #  Note that `strides=2` is the default for `pool_size=(2,2)`,\n",
    "  #+ but I want to make sure I see details. `padding='valid'`\n",
    "  #+ is also default.\n",
    "\n",
    "model_0.add(pool_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6513b27-32a5-4a76-9ef9-280991243df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa539b-d847-4afb-9988-5c6a60103524",
   "metadata": {},
   "source": [
    "### Second Convolutional Layer and Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a401fb2-0dd5-4edf-88e7-3b34f5477c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters_2 = 64\n",
    "\n",
    "conv_2 = layers.Conv2D(filters=n_filters_2,\n",
    "                       kernel_size=(3, 3),\n",
    "                       padding='same'\n",
    "                      )\n",
    "\n",
    "model_0.add(conv_2)\n",
    "model_0.add(layers.Activation('relu'))\n",
    "\n",
    "pool_2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "model_0.add(pool_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde017e-dc6f-478f-a029-9819ff106fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93137a45-1073-4aea-9c43-1063dda07335",
   "metadata": {},
   "source": [
    "### Third, and for this one, last Convolutional Layer and Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad2ded-5fbc-4b37-a941-248c4fa9695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters_3 = 64\n",
    "\n",
    "conv_3 = layers.Conv2D(filters=n_filters_3,\n",
    "                       kernel_size=(3, 3),\n",
    "                       padding='same'\n",
    "                      )\n",
    "\n",
    "model_0.add(conv_3)\n",
    "model_0.add(layers.Activation('relu'))\n",
    "\n",
    "pool_3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "model_0.add(pool_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5f17c-fdf3-4aad-84e2-0c7366268086",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b461488-4964-46fb-9073-93f83d06f083",
   "metadata": {},
   "source": [
    "### Flattening and the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4048f0-c80e-4ae5-8aac-a6848153e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The Fully Connected layer (after flattening)\n",
    "\n",
    "model_0.add(layers.Flatten())\n",
    "\n",
    "fc_layer = layers.Dense(units=64)\n",
    "\n",
    "model_0.add(fc_layer)\n",
    "model_0.add(layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75ea88-5ce3-4538-a170-78eb61d02255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a553b-5a2f-4ba2-9260-b35e55d9eb50",
   "metadata": {},
   "source": [
    "### The Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af93fbc-4865-4252-85e7-9eee06fdb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_head = layers.Dense(units=n_classes)\n",
    "model_0.add(decision_head)\n",
    "##no activation for output (not one-hot encoded)#\n",
    "##model_0.add(layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff5d75-98b7-4eba-945b-1497eb96d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d04ff-9c1d-48f0-9119-e976948a99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5ee4c-9a1b-4eb8-82b9-ea321a0bd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_str = str(model_0)\n",
    "print(f\"model_0_str: '{model_0_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b755e13-6b5b-42b2-85e7-a0ea960c55b2",
   "metadata": {},
   "source": [
    "The `print` didn't seem so nice &ndash; no super-nice summary or visualization, actually the summarization is incomplete &ndash; so let's look at a few options for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e1515-c116-4ddf-8e2d-019821ac9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e33b76-9f83-4408-8173-ce428e3896ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  What would have come was\n",
    "#font = ImageFont.truetype(\"arial.ttf\", 32)\n",
    "visualkeras.layered_view(model_0, legend=True) #, font=font)\n",
    "\n",
    "#  Easy fix, just leave out the font parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2015dd-3fe6-4814-a9b6-3d4dcdf7dff2",
   "metadata": {},
   "source": [
    "#### Compile Using Optimizer and Loss, Specify Metrics to be Reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b08864-ff0e-4e28-be50-0a5d7b527258",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_tic = timeit.default_timer()\n",
    "model_0.compile(\n",
    "    optimizer=optimizers.Adam(),  # default learning rate?\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "compile_toc = timeit.default_timer()\n",
    "print(f\"Compile time: {compile_toc - compile_tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c1aba-4c2f-4ecb-aaa4-2f41208dcfa4",
   "metadata": {},
   "source": [
    "## Train of it, with both viewing and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d2908-33e8-4ad2-87e6-e93f0b2f03c9",
   "metadata": {},
   "source": [
    "cf. [tensorboard documentation and example](https://www.tensorflow.org/tensorboard/graphs), which doesn't seem to want to play with the Wayback Machine (archive.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a95f77-8d8d-448d-b24d-0d84d17719a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4ef1c-25e2-4715-8b43-57a8be0576e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!stat ./tblogs/\n",
    "# if necessary, also\n",
    "#!rm -rf ./tblogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870267d-5704-4b05-b060-b1644d8137a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  if necessary, (output is anything but\n",
    "#+                `stat: cannot statx './tblogs/':`\n",
    "#+                ` No such file or directory`),\n",
    "#+\n",
    "#+ Actually, I just have it telling you it can't if it can't\n",
    "!rm -rf ./tblogs/ && echo \"No problem came up with removal\" || \\\n",
    "echo -e \"Remvoing ./tblogs/ didn't work\\n\"\\\n",
    "\"likely because it didn't exist\\nLet me check ...\"\n",
    "!echo\n",
    "!echo \"Checking stat, whether there's a problem or not\"\n",
    "!stat ./tblogs/ || echo \"Not able to stat, dir likely nonexistent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fce412-e6a0-4653-a42c-7878bf0fdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"tblogs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = callback.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fafca7-18cb-4c7e-87dc-1b726d887106",
   "metadata": {},
   "source": [
    "cf. [StackOverflow Discussion](https://web.archive.org/web/20250423184118/https://stackoverflow.com/questions/67757496/tensorflow-keras-print-out-and-save-loss-and-gradients-during-model-fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a34dfe-0ea0-492a-9ebb-9f01c448768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableVariablesCallback(callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    print(model_0.trainable_variables)\n",
    "  ##endof:  on_epoch_end\n",
    "##endof:  TrainableVariablesCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc34a6-b1e5-4bb3-a1e4-7e1063a95fe9",
   "metadata": {},
   "source": [
    "@todo : Get compute use, memory use, etc. and log, e.g., the trainable variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608f255-dd66-4293-9e8f-998a5ec63e85",
   "metadata": {},
   "source": [
    "If on Colab\n",
    "\n",
    "<strong>REMEMBER TO CHANGE YOUR RUNTIME TO GPU</strong>\n",
    "\n",
    "(You might need to re-run old cells, but it's worth it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fe6c6-83be-4abc-91ea-f2c7bce2d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075d0ae-5f67-4ce6-b5ec-8997d5b650d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  This one gives way too much output. If desired, I can\n",
    "##+ figure out how to send it to a file.\n",
    "# my_trainable_vars_logger = TrainableVariablesCallback()\n",
    "\n",
    "!>training.log  # empty the file without deleting the name\n",
    "my_csv_logger = callbacks.CSVLogger('training.log')\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "train_tic = timeit.default_timer()\n",
    "\n",
    "history = model_0.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[my_csv_logger,\n",
    "               #, my_trainable_vars_logger,\n",
    "               tensorboard_callback\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_toc = timeit.default_timer()\n",
    "print(f\"Train time: {train_toc - train_tic:0.4f} seconds\")\n",
    "\n",
    "train_time_str = format_timespan(train_toc - train_tic)\n",
    "print(f\"which equates to: {train_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b2a4a-e344-4924-96ba-e7d1ceb58db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tblogs  #  This seems to be having less problems.\n",
    "                              #+ It seemed to give nice loss/accuracy\n",
    "                              #+ that showed we were overfitting by a lot.\n",
    "                              #+ That problem doesn't seem to be as bad,\n",
    "                              #+ as you'll probably notice in my plots.\n",
    "                              #+ The graphs to see the architectures were\n",
    "                              #+ not all there; and they're still not;\n",
    "                              #+ Only the \"Conceptual graph\" shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16b9c1-42ac-4b64-b9f3-5284b8a16977",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  If you need to do so, and can see the process number it gives you,\n",
    "##+ you can use that number to kill the TensorBoard server\n",
    "# !kill 11839"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465baef-0b4a-44f5-9ebb-74f9157a0829",
   "metadata": {},
   "source": [
    "### Standard View of the Accuracy and Loss During Training, maybe more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7b44a-7595-402f-919a-ac5f0b7a04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat training.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04698f3f-b4bf-444e-892a-29f428cc4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fe7d8-0fe1-45c1-9bb4-574cb8dea4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5572e-37ee-415b-b9fa-906f5f3a4890",
   "metadata": {},
   "source": [
    "That no longer looks like BAD overfitting to me. Maybe I \n",
    "had some remnants of previous model builds, or maybe I \n",
    "got a bad batch of random starting weights.\n",
    "(I wish I could see where the initial weights are coming from.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b19b3c-3beb-494c-a9d2-a4ee4b5e1035",
   "metadata": {},
   "source": [
    "## And now, run it on our test set ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65a437-fce8-46c8-aca0-8461d4ca46e3",
   "metadata": {},
   "source": [
    "... even though the evaluation set we saw above <em>is</em> the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bb97a-ce98-4856-a650-85bd874912ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = \\\n",
    "        model_0.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ddfaa-e0e7-4108-96bf-346a393cd76a",
   "metadata": {},
   "source": [
    "Next will come looking at misclassified instances, or at least a look at inference from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf6d6aa-7fef-40f2-9ed6-ea812a4c9cbc",
   "metadata": {},
   "source": [
    "## Some details with inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a1116-d53f-4018-a0aa-275e0264454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_one_image = x_test[137]\n",
    "\n",
    "# maybe see if this_one_image is already an array\n",
    "print(str(type(this_one_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154ccfc-ca42-409a-b0e1-05032234cbf8",
   "metadata": {},
   "source": [
    "Look at the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836ef86-b6e3-47cf-906b-597005647eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.imshow(this_one_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e09d50-91bd-4b0e-aad4-188de6a62983",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = tf.keras.utils.img_to_array(this_one_image)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ddc59c-70d5-4aa2-ade5-fa58e5357ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(img_array)\n",
    "\n",
    "print(f\" Predictions for each of the classes:\\n{predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab80e99-684b-41f9-bc47-eb5bfa991b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(f\" Score for each of the classes:\\n{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18143fc-0070-48a9-a169-446efb17cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_class = class_names[np.argmax(score)]\n",
    "\n",
    "print(f\"winning_class: {winning_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45187370-f15d-4274-930a-37fdb70c489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"This image most likely belongs to {} with a {:2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d1f69-5cf6-4c07-83f7-ddf3536ffc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = 500\n",
    "n_random_ints = 10\n",
    "dashes = \"--------------------------------------------------------------\"\n",
    "for idx in np.random.randint(1, max_n+1, size=n_random_ints):\n",
    "  print()\n",
    "  print()\n",
    "  print(dashes)\n",
    "  print()\n",
    "  this_img = x_test[idx]\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(this_img)\n",
    "  plt.show()\n",
    "  print()\n",
    "  print(f\"Ground truth label: {y_test[idx][0]}\")\n",
    "  print()\n",
    "  img_array = tf.keras.utils.img_to_array(this_image)\n",
    "  img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "  predictions = model.predict(img_array)\n",
    "  print(f\" Predictions for each of the classes:\\n{predictions}\")\n",
    "  print()\n",
    "  score = tf.nn.softmax(predictions[0])\n",
    "  print(f\" Score for each of the classes:\\n{score}\")\n",
    "  print()\n",
    "  winning_class = class_names[np.argmax(score)]\n",
    "  print(f\"winning_class: {winning_class}\")\n",
    "  print()\n",
    "  print(\"{:2f} percent confidence.\".format(100 * np.max(score)))\n",
    "  print()\n",
    "  print(dashes)\n",
    "  print()\n",
    "##endof:  for idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75168b-b9da-422e-b51d-864b23cdc83c",
   "metadata": {},
   "source": [
    "#### I'm going to have to look more at how to figure out how to ID and look at misclassified instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fb071-45b5-4362-9772-f64607c63269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07dc66-e12a-4a27-8d9a-e37cdb50fdd2",
   "metadata": {},
   "source": [
    "## Now, let's do the same with Grad-CAM and all other visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88658e8a-fbc8-4d0c-958f-77c01319b2e1",
   "metadata": {},
   "source": [
    "Looks like we need to name layers and do some other stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927289dc-c997-485c-8719-113a1944f907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56f632-e1f7-48d4-bc97-8e26b122139f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc7a45-323f-4b0d-8cff-1d2e9d52b92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023907c6-a415-4a6f-a80b-5afde0ff61d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3c788-b3f1-4f04-8ea1-6f0a94653eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd5485-ed50-4508-a6ca-93b8941e5066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b110b5-ffec-479c-a195-5257e4ba01c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e6b9f0-384f-45ef-8268-59f12ef8a1b0",
   "metadata": {},
   "source": [
    "## Here are some nice pictures that will be goals for visualizations <br/>and explanations of Grad-CAM that should help explain things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1664f8-9426-4518-96fc-056062e18429",
   "metadata": {},
   "source": [
    "Lots of these come from a nice site, learnopencv.com\n",
    "\n",
    "I feel the creators do a really good job with visualization. The other sources are below all the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87540622-ed6d-477d-8a9b-55d79038c46a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_analyticsvidhya.png\"\n",
    "       alt=\"CNN\"\n",
    "       width=\"750px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a61a6-dfac-4fb0-ac91-869824d3af06",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_04.png\"\n",
    "       alt=\"CNN\"\n",
    "       width=\"750px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e6946-fad2-47f9-940f-1efbf06d5c99",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_05.png\"\n",
    "       alt=\"CNN\"\n",
    "       width=\"750px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60291c82-03f5-4c9f-be79-d9e5c2a3d5ac",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_06.png\"\n",
    "       alt=\"CNN\"\n",
    "       width=\"1000px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0c77d-1ad8-4482-9151-a3322932c042",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_01.png\"\n",
    "       alt=\"CNN Code and Architecture Mix\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6e974-fb26-4e1b-98b6-cb0d2d09963b",
   "metadata": {},
   "source": [
    "That shows a bit how we get to a point where they might trust us. Now observe the Grad-CAM wonder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84323d5-014c-4072-adaf-c9ffe05a7baf",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/orig_image_grad_camm_fig_2-1.png\"\n",
    "       alt=\"Grad-CAM Cat and Dog, Original\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46d70c-b75a-4f50-a93e-578efcc8f9e2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/combination_dog_guidedbackprop_and_gradcam_then_ResNet.png\"\n",
    "       alt=\"Grad-Cam for dog, combined\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600b41-a897-49d2-897d-3d6bd74eb82d",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/combination_dog_guidedbackprop_and_gradcam_then_ResNet.png\"\n",
    "       alt=\"Grad-Cam Dog details, combined\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10368b0d-9c62-4baf-9a2b-3fac0e08e65c",
   "metadata": {},
   "source": [
    "Pretty convincing for me. I would definitely call this explainable AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed43f7-3d67-4cb0-b825-a09403982de1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/interesting_semantic_segmentation_gradcam.png\"\n",
    "       alt=\"Grad-Cam\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c1c4d2-1b6d-4f6b-9335-896140ae31da",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/decisions_for_right_reasons_gender_issue_gradcam.png\"\n",
    "       alt=\"Grad-Cam important gender issue\"\n",
    "       width=\"500px\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cba92-1f48-46b5-b285-17fab99ba675",
   "metadata": {},
   "source": [
    "`https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d342f06-acc0-49fe-9ed1-c58c6dbaf5eb",
   "metadata": {},
   "source": [
    "-yGHUa5PL4A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26363280-18d7-40c9-bccc-e38bd439a3f8",
   "metadata": {},
   "source": [
    "Fine-Tuning ResNet50 A Practical Guide by It's Amit Jan 2025<br/>\n",
    "https://drive.google.com/file/d/1WHs_NcXro37ByDvsYkocXF6rVfE7DyeQ/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ded26-6a2c-45d0-af44-0e04c0ae75b5",
   "metadata": {},
   "source": [
    "Training and Hosting resnet-50 model using Amazon Sagemaker and Flask by Ravi Shukla Nov 2022<br/>\n",
    "https://drive.google.com/file/d/1RQ7SPUk8ZhJYzDJGpQVASgwCrNC4JqFD/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913c4eb-7006-42bb-adb3-c1c185ddaa35",
   "metadata": {},
   "source": [
    "https://adamharley.com/nn_vis/cnn/3d.html\n",
    "\n",
    "https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network\n",
    "\n",
    "https://datascience.stackexchange.com/questions/12851/how-do-you-visualize-neural-network-architectures\n",
    "\n",
    "https://datascience.stackexchange.com/questions/2670/visualizing-deep-neural-network-training\n",
    "\n",
    "https://learnopencv.com/understanding-convolutional-neural-networks-cnn/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/what-is-the-convolutional-neural-network-architecture/\n",
    "\n",
    "https://www.researchgate.net/figure/The-overall-LeNet-architecture-The-numbers-at-the-convolution-and-pooling-layers_fig2_318972455\n",
    "\n",
    "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n",
    "\n",
    "https://github.com/jacobgil/pytorch-grad-cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8e35f-b088-4f25-8474-1179c63a0569",
   "metadata": {},
   "source": [
    "## Specifics for Colab Environment &ndash; I would<br/>minimize it unless you need to match versions<br/>because things won't work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b000da2-5d29-482b-94ae-fdb126bf1092",
   "metadata": {},
   "source": [
    "For this commit / pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935570b-2a88-42f4-9fea-0b4392826126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!date +'%s_%Y-%m-%dT%H:%M:%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dbb08-5fd2-417c-990f-6b35733afadb",
   "metadata": {},
   "source": [
    "Output with this commit:\n",
    "\n",
    "`1750118934_2025-06-17T00:08:54+0000`\n",
    "\n",
    "Local: `2025-06-16T18:08:54-0600`\n",
    "\n",
    "(I'll try to keep this updated for each commit, or at least for each pull request.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d9522-4f9e-4b79-9a38-bbc29b43bba4",
   "metadata": {},
   "source": [
    "#### Uncomment and run before merging (or after merging) each pull request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb94c52c-a951-4899-a81b-ded40d5e2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046f3d5-1673-4d13-8206-fdc08bf4aaac",
   "metadata": {},
   "source": [
    "Output at `pip freeze` at approximately `1750118934_2025-06-17T00:08:54+0000`\n",
    "\n",
    "```\n",
    "absl-py==2.3.0\n",
    "aggdraw==1.3.19\n",
    "aiohappyeyeballs @ file:///home/conda/feedstock_root/build_artifacts/aiohappyeyeballs_1741775197943/work\n",
    "aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1748897545370/work\n",
    "aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1734342155601/work\n",
    "alabaster @ file:///home/conda/feedstock_root/build_artifacts/alabaster_1704848697227/work\n",
    "annotated-types==0.7.0\n",
    "antlr4-python3-runtime==4.9.3\n",
    "anyio @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_anyio_1742243108/work\n",
    "argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1749017159514/work\n",
    "argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1725356557095/work\n",
    "arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1733584251875/work\n",
    "astroid @ file:///home/conda/feedstock_root/build_artifacts/astroid_1746997007492/work\n",
    "astropy @ file:///home/conda/feedstock_root/build_artifacts/astropy_1732730164730/work\n",
    "astropy-iers-data @ file:///home/conda/feedstock_root/build_artifacts/astropy-iers-data_1748866287586/work\n",
    "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1733250440834/work\n",
    "astunparse==1.6.3\n",
    "async-lru @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_async-lru_1742153708/work\n",
    "async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1733235340728/work\n",
    "asyncssh @ file:///home/conda/feedstock_root/build_artifacts/asyncssh_1746470644867/work\n",
    "atomicwrites @ file:///home/conda/feedstock_root/build_artifacts/atomicwrites_1734603084217/work\n",
    "attrs==25.3.0\n",
    "Authlib==1.6.0\n",
    "autopep8 @ file:///home/conda/feedstock_root/build_artifacts/autopep8_1693061251004/work\n",
    "autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1736983369505/work\n",
    "awscli==1.40.34\n",
    "awscrt @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_awscrt_1746842415/work\n",
    "babel @ file:///home/conda/feedstock_root/build_artifacts/babel_1738490167835/work\n",
    "backports-datetime-fromisoformat==2.0.3\n",
    "backports.tarfile @ file:///home/conda/feedstock_root/build_artifacts/backports.tarfile_1733325779670/work\n",
    "bcrypt @ file:///home/conda/feedstock_root/build_artifacts/bcrypt_1740762453223/work\n",
    "beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1744783198182/work\n",
    "binaryornot @ file:///home/conda/feedstock_root/build_artifacts/binaryornot_1734071693792/work\n",
    "bitarray @ file:///home/conda/feedstock_root/build_artifacts/bitarray_1748068027752/work\n",
    "black @ file:///home/conda/feedstock_root/build_artifacts/black-recipe_1742502760723/work\n",
    "bleach @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_bleach_1737382993/work\n",
    "blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1731096409132/work\n",
    "bokeh @ file:///home/conda/feedstock_root/build_artifacts/bokeh_1747091499565/work\n",
    "boto3==1.38.35\n",
    "botocore==1.38.35\n",
    "Bottleneck @ file:///home/conda/feedstock_root/build_artifacts/bottleneck_1747241408193/work\n",
    "Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1725267488082/work\n",
    "brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1725393311144/work\n",
    "cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work\n",
    "certifi @ file:///home/conda/feedstock_root/build_artifacts/certifi_1746569525376/work/certifi\n",
    "cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1725560520483/work\n",
    "chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1741797914774/work\n",
    "charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1746214863626/work\n",
    "click==8.1.8\n",
    "cloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1736947526808/work\n",
    "colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1733218098505/work\n",
    "comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1733502965406/work\n",
    "contourpy @ file:///home/conda/feedstock_root/build_artifacts/contourpy_1744743067588/work\n",
    "cookiecutter @ file:///home/conda/feedstock_root/build_artifacts/cookiecutter_1734069695401/work\n",
    "coverage @ file:///home/conda/feedstock_root/build_artifacts/coverage_1748048692470/work\n",
    "cryptography==45.0.3\n",
    "cycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1733332471406/work\n",
    "Cython @ file:///home/conda/feedstock_root/build_artifacts/cython_1747670283756/work\n",
    "cytoolz @ file:///home/conda/feedstock_root/build_artifacts/cytoolz_1734107196509/work\n",
    "dask @ file:///home/conda/feedstock_root/build_artifacts/dask-core_1747771612930/work\n",
    "debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1744321252239/work\n",
    "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1740384970518/work\n",
    "defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work\n",
    "Deprecated @ file:///home/conda/feedstock_root/build_artifacts/deprecated_1737986966356/work\n",
    "diff-match-patch @ file:///home/conda/feedstock_root/build_artifacts/diff-match-patch_1734637534167/work\n",
    "dill @ file:///home/conda/feedstock_root/build_artifacts/dill_1744798524990/work\n",
    "distributed @ file:///home/conda/feedstock_root/build_artifacts/distributed_1747775339483/work\n",
    "distro @ file:///home/conda/feedstock_root/build_artifacts/distro_1675116244235/work\n",
    "docker==7.1.0\n",
    "docstring-to-markdown @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_docstring-to-markdown_1746284388/work\n",
    "docutils @ file:///home/conda/feedstock_root/build_artifacts/docutils_1666754874910/work\n",
    "dparse==0.6.4\n",
    "entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1733327148154/work\n",
    "et_xmlfile @ file:///home/conda/feedstock_root/build_artifacts/et_xmlfile_1733749653422/work\n",
    "exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1746947292760/work\n",
    "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1745502089858/work\n",
    "fastapi==0.115.12\n",
    "fastcache @ file:///home/conda/feedstock_root/build_artifacts/fastcache_1725746788488/work\n",
    "fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1733235979760/work/dist\n",
    "filelock==3.16.1\n",
    "flake8 @ file:///home/conda/feedstock_root/build_artifacts/flake8_1739898391164/work\n",
    "Flask @ file:///home/conda/feedstock_root/build_artifacts/flask_1747203985407/work\n",
    "flask-cors @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_flask-cors_1747515361/work\n",
    "flatbuffers==25.2.10\n",
    "fonttools @ file:///home/conda/feedstock_root/build_artifacts/fonttools_1748465473694/work\n",
    "fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1733327382592/work/dist\n",
    "frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1746635328256/work\n",
    "fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1748101524683/work\n",
    "future @ file:///home/conda/feedstock_root/build_artifacts/future_1738926421307/work\n",
    "gast==0.6.0\n",
    "gevent @ file:///home/conda/feedstock_root/build_artifacts/gevent_1747078508282/work\n",
    "gmpy2 @ file:///home/conda/feedstock_root/build_artifacts/gmpy2_1745509363766/work\n",
    "google-pasta==0.2.0\n",
    "graphene==3.4.3\n",
    "graphql-core==3.2.6\n",
    "graphql-relay==3.2.0\n",
    "greenlet @ file:///home/conda/feedstock_root/build_artifacts/greenlet_1746824031437/work\n",
    "grpcio==1.72.1\n",
    "gssapi @ file:///home/conda/feedstock_root/build_artifacts/python-gssapi_1733827675249/work\n",
    "gym==0.26.2\n",
    "gym-notices==0.0.8\n",
    "h11==0.16.0\n",
    "h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1738578511449/work\n",
    "h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1739952247544/work\n",
    "hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1736983344385/work\n",
    "hpack @ file:///home/conda/feedstock_root/build_artifacts/hpack_1737618293087/work\n",
    "humanfriendly==10.0\n",
    "hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1737618333194/work\n",
    "idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1733211830134/work\n",
    "imagecodecs-lite @ file:///home/conda/feedstock_root/build_artifacts/imagecodecs-lite_1716011160511/work\n",
    "imageio @ file:///home/conda/feedstock_root/build_artifacts/imageio_1738273805233/work\n",
    "imagesize @ file:///home/conda/feedstock_root/build_artifacts/imagesize_1656939531508/work\n",
    "immutables @ file:///home/conda/feedstock_root/build_artifacts/immutables_1747742326998/work\n",
    "importlib-metadata==6.11.0\n",
    "importlib_resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1736252299705/work\n",
    "inflection @ file:///home/conda/feedstock_root/build_artifacts/inflection_1734732140003/work\n",
    "iniconfig @ file:///home/conda/feedstock_root/build_artifacts/iniconfig_1733223141826/work\n",
    "intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work\n",
    "ipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1719845459717/work\n",
    "ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1698846603011/work\n",
    "ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1746454582739/work\n",
    "isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1733493628631/work/dist\n",
    "isort @ file:///home/conda/feedstock_root/build_artifacts/isort_1746190248328/work\n",
    "itsdangerous @ file:///home/conda/feedstock_root/build_artifacts/itsdangerous_1733308265247/work\n",
    "jaraco.classes @ file:///home/conda/feedstock_root/build_artifacts/jaraco.classes_1733325873251/work\n",
    "jaraco.context @ file:///home/conda/feedstock_root/build_artifacts/jaraco.context_1733382590553/work\n",
    "jaraco.functools @ file:///home/conda/feedstock_root/build_artifacts/jaraco.functools_1733746366381/work\n",
    "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1733300866624/work\n",
    "jeepney @ file:///home/conda/feedstock_root/build_artifacts/jeepney_1740828240267/work\n",
    "jellyfish @ file:///home/conda/feedstock_root/build_artifacts/jellyfish_1734444752947/work\n",
    "Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1741263328855/work\n",
    "jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1733229141657/work\n",
    "joblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1748019130050/work\n",
    "json5 @ file:///home/conda/feedstock_root/build_artifacts/json5_1743722064131/work\n",
    "jsonpointer @ file:///home/conda/feedstock_root/build_artifacts/jsonpointer_1725302897999/work\n",
    "jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1748294245630/work\n",
    "jsonschema-specifications @ file:///tmp/tmpuvkyqc9y/src\n",
    "jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1733818543322/work\n",
    "jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1733817997778/work\n",
    "jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_jupyter_events_1738765986/work\n",
    "jupyter-lsp @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_jupyter-lsp_1748550130/work/jupyter-lsp\n",
    "jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1733440914442/work\n",
    "jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1748333051527/work\n",
    "jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_jupyter_server_1747083217/work\n",
    "jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1733427956852/work\n",
    "jupyterlab @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_1748272853841/work\n",
    "jupyterlab_pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1733328101776/work\n",
    "jupyterlab_server @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_server_1733599573484/work\n",
    "jupyterlab_widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1746450771080/work\n",
    "keras==3.10.0\n",
    "keyring @ file:///home/conda/feedstock_root/build_artifacts/keyring_1735210185992/work\n",
    "kiwisolver @ file:///home/conda/feedstock_root/build_artifacts/kiwisolver_1725459213453/work\n",
    "krb5 @ file:///home/conda/feedstock_root/build_artifacts/pykrb5_1741302027048/work\n",
    "lazy-object-proxy @ file:///home/conda/feedstock_root/build_artifacts/lazy-object-proxy_1744895524715/work\n",
    "lazy_loader @ file:///home/conda/feedstock_root/build_artifacts/lazy-loader_1733636780672/work\n",
    "libclang==18.1.1\n",
    "lief @ file:///home/conda/feedstock_root/build_artifacts/lief_1743551769507/work/api/python\n",
    "llvmlite==0.44.0\n",
    "locket @ file:///home/conda/feedstock_root/build_artifacts/locket_1650660393415/work\n",
    "lz4 @ file:///home/conda/feedstock_root/build_artifacts/lz4_1746561905891/work\n",
    "Markdown==3.8\n",
    "markdown-it-py @ file:///home/conda/feedstock_root/build_artifacts/markdown-it-py_1733250460757/work\n",
    "MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1733219680183/work\n",
    "marshmallow==4.0.0\n",
    "matplotlib==3.10.3\n",
    "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1733416936468/work\n",
    "mccabe @ file:///home/conda/feedstock_root/build_artifacts/mccabe_1733216466933/work\n",
    "mdurl @ file:///home/conda/feedstock_root/build_artifacts/mdurl_1733255585584/work\n",
    "mistune @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_mistune_1742402716/work\n",
    "mkl-service==2.4.2\n",
    "mkl_fft==1.3.14\n",
    "ml-dtypes==0.3.2\n",
    "mock==4.0.3\n",
    "more-itertools @ file:///home/conda/feedstock_root/build_artifacts/more-itertools_1745349462661/work\n",
    "mpi4py==4.0.3\n",
    "mpmath @ file:///home/conda/feedstock_root/build_artifacts/mpmath_1733302684489/work\n",
    "msgpack @ file:///home/conda/feedstock_root/build_artifacts/msgpack-python_1725974993022/work\n",
    "multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1747722347633/work\n",
    "multiprocess==0.70.18\n",
    "munkres==1.1.4\n",
    "mypy_extensions @ file:///home/conda/feedstock_root/build_artifacts/mypy_extensions_1745776566517/work\n",
    "namex==0.1.0\n",
    "narwhals @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_narwhals_1748271935/work\n",
    "nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1734628800805/work\n",
    "nbconvert @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_nbconvert-core_1738067871/work\n",
    "nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1733402752141/work\n",
    "nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1733325553580/work\n",
    "netron==8.3.9\n",
    "networkx @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_networkx_1731521053/work\n",
    "nltk @ file:///home/conda/feedstock_root/build_artifacts/nltk_1734309924703/work\n",
    "notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1748278664848/work\n",
    "notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1733408315203/work\n",
    "numba @ file:///home/conda/feedstock_root/build_artifacts/numba_1744232039266/work\n",
    "numexpr==2.8.4\n",
    "numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1707225380409/work/dist/numpy-1.26.4-cp310-cp310-linux_x86_64.whl#sha256=51131fd8fc130cd168aecaf1bc0ea85f92e8ffebf211772ceb16ac2e7f10d7ca\n",
    "numpydoc @ file:///home/conda/feedstock_root/build_artifacts/numpydoc_1733650859674/work\n",
    "nvidia-ml-py @ file:///home/conda/feedstock_root/build_artifacts/nvidia-ml-py_1746576379096/work\n",
    "omegaconf==2.3.0\n",
    "opencv-python==4.11.0.86\n",
    "openpyxl @ file:///home/conda/feedstock_root/build_artifacts/openpyxl_1725460826776/work\n",
    "opt_einsum==3.4.0\n",
    "optree==0.16.0\n",
    "overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1734587627321/work\n",
    "packaging==24.2\n",
    "pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1726878398774/work\n",
    "pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work\n",
    "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1733271261340/work\n",
    "partd @ file:///home/conda/feedstock_root/build_artifacts/partd_1715026491486/work\n",
    "path @ file:///home/conda/feedstock_root/build_artifacts/path_1737244316269/work\n",
    "pathlib2 @ file:///home/conda/feedstock_root/build_artifacts/pathlib2_1725350144616/work\n",
    "pathos==0.3.4\n",
    "pathspec @ file:///home/conda/feedstock_root/build_artifacts/pathspec_1733233363808/work\n",
    "patsy @ file:///home/conda/feedstock_root/build_artifacts/patsy_1733792384640/work\n",
    "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1733301927746/work\n",
    "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1733327343728/work\n",
    "pillow @ file:///home/conda/feedstock_root/build_artifacts/pillow_1746646209821/work\n",
    "pkginfo @ file:///home/conda/feedstock_root/build_artifacts/pkginfo_1739984581450/work\n",
    "pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1733344503739/work\n",
    "platformdirs @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_platformdirs_1746710438/work\n",
    "plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1748388440098/work\n",
    "pluggy @ file:///home/conda/feedstock_root/build_artifacts/pluggy_1747339660894/work\n",
    "ply @ file:///home/conda/feedstock_root/build_artifacts/ply_1733239724146/work\n",
    "pox==0.3.6\n",
    "ppft==1.7.7\n",
    "prometheus_client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1748896729786/work\n",
    "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1677600924538/work\n",
    "propcache @ file:///home/conda/feedstock_root/build_artifacts/propcache_1744524934684/work\n",
    "protobuf==4.25.8\n",
    "psutil==6.1.1\n",
    "psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1727892677567/work\n",
    "psycopg2-binary @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1701737547976/work/psycopg2-binary\n",
    "ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1733302279685/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl#sha256=92c32ff62b5fd8cf325bec5ab90d7be3d2a8ca8c8a3813ff487a8d2002630d1f\n",
    "pure_eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1733569405015/work\n",
    "py-cpuinfo @ file:///home/conda/feedstock_root/build_artifacts/py-cpuinfo_1733236359728/work\n",
    "py4j==0.10.9.5\n",
    "pyarrow==18.1.0\n",
    "pyasn1==0.6.1\n",
    "pybind11 @ file:///D:/bld/pybind11-split_1747934313674/work\n",
    "pybind11_global @ file:///home/conda/feedstock_root/build_artifacts/pybind11-split_1747934270866/work\n",
    "pycodestyle @ file:///home/conda/feedstock_root/build_artifacts/pycodestyle_1733216196861/work\n",
    "pyconify @ file:///home/conda/feedstock_root/build_artifacts/pyconify_1738865203119/work\n",
    "pycosat @ file:///home/conda/feedstock_root/build_artifacts/pycosat_1732588391741/work\n",
    "pycparser @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_pycparser_1733195786/work\n",
    "pycryptodome @ file:///home/conda/feedstock_root/build_artifacts/pycryptodome_1747561177604/work\n",
    "pycurl @ file:///home/conda/feedstock_root/build_artifacts/pycurl_1741333902605/work\n",
    "pydantic==2.9.2\n",
    "pydantic_core==2.23.4\n",
    "pydocstyle @ file:///home/conda/feedstock_root/build_artifacts/pydocstyle_1733261631732/work\n",
    "pyerfa @ file:///home/conda/feedstock_root/build_artifacts/pyerfa_1731377671994/work\n",
    "pyflakes @ file:///home/conda/feedstock_root/build_artifacts/pyflakes_1733216066937/work\n",
    "pyfunctional==1.5.0\n",
    "pygame==2.6.1\n",
    "PyGithub @ file:///home/conda/feedstock_root/build_artifacts/pygithub_1740204758187/work\n",
    "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1736243443484/work\n",
    "PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1732782409051/work\n",
    "pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1725289118206/work\n",
    "pylint @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_pylint_1746387959/work\n",
    "pylint-venv @ file:///home/conda/feedstock_root/build_artifacts/pylint-venv_1734789278875/work\n",
    "pyls-spyder @ file:///home/conda/feedstock_root/build_artifacts/pyls-spyder_1736029828480/work\n",
    "PyNaCl @ file:///home/conda/feedstock_root/build_artifacts/pynacl_1725739248276/work\n",
    "pynvml @ file:///home/conda/feedstock_root/build_artifacts/pynvml_1734538935786/work\n",
    "pyodbc @ file:///home/conda/feedstock_root/build_artifacts/pyodbc_1729084271546/work\n",
    "pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1747560772891/work\n",
    "pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1743089729650/work\n",
    "PyQt5==5.15.11\n",
    "PyQt5_sip==12.17.0\n",
    "PyQtWebEngine==5.15.7\n",
    "pyrsistent @ file:///home/conda/feedstock_root/build_artifacts/pyrsistent_1725353509161/work\n",
    "PySide6==6.9.1\n",
    "PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1733217236728/work\n",
    "pyspark==3.3.0\n",
    "pyspnego @ file:///home/conda/feedstock_root/build_artifacts/pyspnego_1731411726251/work\n",
    "pytest @ file:///home/conda/feedstock_root/build_artifacts/pytest_1748907508048/work\n",
    "python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work\n",
    "python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work\n",
    "python-lsp-black @ file:///home/conda/feedstock_root/build_artifacts/python-lsp-black_1736029913871/work\n",
    "python-lsp-jsonrpc @ file:///home/conda/feedstock_root/build_artifacts/python-lsp-jsonrpc_1736013464386/work\n",
    "python-lsp-server @ file:///home/conda/feedstock_root/build_artifacts/python-lsp-server-meta_1739028314576/work\n",
    "python-slugify @ file:///home/conda/feedstock_root/build_artifacts/python-slugify_1733756245724/work\n",
    "pytoolconfig @ file:///home/conda/feedstock_root/build_artifacts/pytoolconfig_1733360721313/work\n",
    "pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1706886791323/work\n",
    "pyuca @ file:///home/conda/feedstock_root/build_artifacts/pyuca_1734685489013/work\n",
    "PyWavelets==1.8.0\n",
    "pyxdg @ file:///home/conda/feedstock_root/build_artifacts/pyxdg_1654536799286/work\n",
    "PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1737454647378/work\n",
    "pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1743831255788/work\n",
    "QDarkStyle @ file:///home/conda/feedstock_root/build_artifacts/qdarkstyle_1736088649807/work\n",
    "qstylizer @ file:///home/conda/feedstock_root/build_artifacts/qstylizer_1732204919793/work/dist/qstylizer-0.2.4-py2.py3-none-any.whl#sha256=020f9e24a101c9478a6b32575838b93eeefacc8887520a67c172ba33ad22aa2c\n",
    "QtAwesome @ file:///home/conda/feedstock_root/build_artifacts/qtawesome_1741041188600/work\n",
    "qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1733730417233/work\n",
    "QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1739289837248/work\n",
    "referencing @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_referencing_1737836872/work\n",
    "regex @ file:///home/conda/feedstock_root/build_artifacts/regex_1730952192772/work\n",
    "requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1733217035951/work\n",
    "requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1733915992449/work\n",
    "rfc3339_validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1733599910982/work\n",
    "rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work\n",
    "rich @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_rich_1743371105/work/dist\n",
    "rope @ file:///home/conda/feedstock_root/build_artifacts/rope_1733342323938/work\n",
    "rpds-py @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_rpds-py_1747837825/work\n",
    "rsa==4.7.2\n",
    "rtree @ file:///home/conda/feedstock_root/build_artifacts/rtree_1741378561624/work\n",
    "ruamel-yaml-conda @ file:///home/conda/feedstock_root/build_artifacts/ruamel_yaml_1695546328261/work\n",
    "ruamel.yaml @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml_1678272977710/work\n",
    "ruamel.yaml.clib @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml.clib_1728724455198/work\n",
    "s3fs==0.4.2\n",
    "s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1747960786701/work\n",
    "safety-schemas==0.0.14\n",
    "sagemaker==2.246.0\n",
    "sagemaker-core==1.0.36\n",
    "sagemaker_pyspark==1.4.5\n",
    "schema==0.7.7\n",
    "scikit-image @ file:///home/conda/feedstock_root/build_artifacts/scikit-image_1681313146947/work/dist/scikit_image-0.20.0-cp310-cp310-linux_x86_64.whl\n",
    "scikit-learn @ file:///home/conda/feedstock_root/build_artifacts/scikit-learn_1736496756180/work/dist/scikit_learn-1.6.1-cp310-cp310-linux_x86_64.whl#sha256=8b3481924bda36bf9a85c5f500f48e43e1178ead014b2d2ecf10f7b9b49935b4\n",
    "scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy-split_1739790642651/work/dist/scipy-1.15.2-cp310-cp310-linux_x86_64.whl#sha256=9e52bad6c3294d1a5b04a13632118ca2157130603c6c018c2d710162b223b27e\n",
    "seaborn @ file:///home/conda/feedstock_root/build_artifacts/seaborn-split_1733730015268/work\n",
    "SecretStorage @ file:///home/conda/feedstock_root/build_artifacts/secretstorage_1725915608929/work\n",
    "Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1733322040660/work\n",
    "shap @ file:///home/conda/feedstock_root/build_artifacts/shap_1744972817899/work\n",
    "shellingham==1.5.4\n",
    "shiboken6==6.9.1\n",
    "sip @ file:///home/conda/feedstock_root/build_artifacts/sip_1745411202924/work\n",
    "six @ file:///home/conda/feedstock_root/build_artifacts/six_1733380938961/work\n",
    "slicer @ file:///home/conda/feedstock_root/build_artifacts/slicer_1710029110974/work\n",
    "smclarify==0.5\n",
    "smdebug-rulesconfig==1.0.1\n",
    "sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1733244044561/work\n",
    "snowballstemmer @ file:///home/conda/feedstock_root/build_artifacts/snowballstemmer_1747749415787/work\n",
    "sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1738440353519/work\n",
    "soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1746563585861/work\n",
    "sparkmagic==0.22.0\n",
    "Sphinx @ file:///home/conda/feedstock_root/build_artifacts/sphinx_1713554891974/work\n",
    "sphinxcontrib-applehelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-applehelp_1733754101641/work\n",
    "sphinxcontrib-devhelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-devhelp_1733754113405/work\n",
    "sphinxcontrib-htmlhelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-htmlhelp_1733754280555/work\n",
    "sphinxcontrib-jsmath @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-jsmath_1733753744933/work\n",
    "sphinxcontrib-qthelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-qthelp_1733753408017/work\n",
    "sphinxcontrib-serializinghtml @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-serializinghtml_1733750479108/work\n",
    "sphinxcontrib-websupport @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-websupport_1705125899521/work\n",
    "spyder @ file:///home/conda/feedstock_root/build_artifacts/spyder-base_1747952905431/work\n",
    "spyder-kernels @ file:///home/conda/feedstock_root/build_artifacts/spyder-kernels_1747849464559/work\n",
    "SQLAlchemy @ file:///home/conda/feedstock_root/build_artifacts/sqlalchemy_1747298815541/work\n",
    "stack_data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1733569443808/work\n",
    "starlette==0.46.2\n",
    "statsmodels @ file:///home/conda/feedstock_root/build_artifacts/statsmodels_1727986707121/work\n",
    "superqt @ file:///home/conda/feedstock_root/build_artifacts/superqt_1743212840100/work\n",
    "sympy @ file:///home/conda/feedstock_root/build_artifacts/sympy_1745946051654/work\n",
    "tables @ file:///home/conda/feedstock_root/build_artifacts/pytables_1733265846456/work\n",
    "tabulate @ file:///home/conda/feedstock_root/build_artifacts/tabulate_1733589744265/work\n",
    "tblib @ file:///home/conda/feedstock_root/build_artifacts/tblib_1743515515538/work\n",
    "tenacity==9.1.2\n",
    "tensorboard==2.16.2\n",
    "tensorboard-data-server==0.7.2\n",
    "tensorflow==2.16.2\n",
    "tensorflow-io-gcs-filesystem==0.37.1\n",
    "termcolor==3.1.0\n",
    "terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1710262609923/work\n",
    "testpath @ file:///home/conda/feedstock_root/build_artifacts/testpath_1733327295736/work\n",
    "text-unidecode @ file:///home/conda/feedstock_root/build_artifacts/text-unidecode_1733749896243/work\n",
    "textdistance @ file:///home/conda/feedstock_root/build_artifacts/textdistance_1736082828021/work\n",
    "tf_keras==2.16.0\n",
    "threadpoolctl @ file:///home/conda/feedstock_root/build_artifacts/threadpoolctl_1741878222898/work\n",
    "three-merge @ file:///home/conda/feedstock_root/build_artifacts/three-merge_1736031697739/work\n",
    "tifffile @ file:///home/conda/feedstock_root/build_artifacts/tifffile_1591280222285/work\n",
    "tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1729802851396/work\n",
    "toml @ file:///home/conda/feedstock_root/build_artifacts/toml_1734091811753/work\n",
    "tomli @ file:///home/conda/feedstock_root/build_artifacts/tomli_1733256695513/work\n",
    "tomlkit @ file:///home/conda/feedstock_root/build_artifacts/tomlkit_1733230743009/work\n",
    "toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1733736030883/work\n",
    "tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1748003301700/work\n",
    "tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1735661334605/work\n",
    "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1733367359838/work\n",
    "typed-ast @ file:///home/conda/feedstock_root/build_artifacts/typed-ast_1695409894288/work\n",
    "typer==0.16.0\n",
    "types-python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/types-python-dateutil_1747417193651/work\n",
    "typing-inspection==0.4.1\n",
    "typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/bld/rattler-build_typing_extensions_1748959427/work\n",
    "typing_utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1733331286120/work\n",
    "tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1742745135198/work\n",
    "ujson @ file:///home/conda/feedstock_root/build_artifacts/ujson_1724954402136/work\n",
    "unicodedata2 @ file:///home/conda/feedstock_root/build_artifacts/unicodedata2_1736692496989/work\n",
    "uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1733323593477/work/dist\n",
    "urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1718728347128/work\n",
    "uvicorn==0.34.3\n",
    "visualkeras==0.1.4\n",
    "watchdog @ file:///home/conda/feedstock_root/build_artifacts/watchdog_1730492868936/work\n",
    "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1733231326287/work\n",
    "webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1733359735138/work\n",
    "webencodings @ file:///home/conda/feedstock_root/build_artifacts/webencodings_1733236011802/work\n",
    "websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1733157342724/work\n",
    "Werkzeug @ file:///home/conda/feedstock_root/build_artifacts/werkzeug_1733160440960/work\n",
    "whatthepatch @ file:///home/conda/feedstock_root/build_artifacts/whatthepatch_1736040839611/work\n",
    "widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1744291053367/work\n",
    "wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1736869474897/work\n",
    "wurlitzer @ file:///home/conda/feedstock_root/build_artifacts/wurlitzer_1736089142811/work\n",
    "XlsxWriter @ file:///home/conda/feedstock_root/build_artifacts/xlsxwriter_1745134629697/work\n",
    "xyzservices @ file:///home/conda/feedstock_root/build_artifacts/xyzservices_1745598038694/work\n",
    "yapf @ file:///home/conda/feedstock_root/build_artifacts/yapf_1736192735449/work\n",
    "yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1744972437465/work\n",
    "zict @ file:///home/conda/feedstock_root/build_artifacts/zict_1733261551178/work\n",
    "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1748277385522/work\n",
    "zope.event @ file:///home/conda/feedstock_root/build_artifacts/zope.event_1732754803800/work\n",
    "zope.interface @ file:///home/conda/feedstock_root/build_artifacts/zope.interface_1732794946912/work\n",
    "zstandard==0.23.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
