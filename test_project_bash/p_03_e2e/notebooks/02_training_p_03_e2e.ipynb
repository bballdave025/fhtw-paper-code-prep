{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ee65f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 09:03:23.251227: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-07 09:03:23.316619: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-07 09:03:23.316669: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-07 09:03:23.316716: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-07 09:03:23.327590: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-07 09:03:24.927139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, tensorflow as tf\n",
    "os.environ[\"PYTHONHASHSEED\"]=\"137\";\n",
    "random.seed(137);\n",
    "np.random.seed(137);\n",
    "tf.random.set_seed(137)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683bc247-bbd2-4c21-9cbd-e045de30ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: elapsed: command not found\n"
     ]
    }
   ],
   "source": [
    "!elapsed --check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e55d98-e7cf-4851-b1c8-866ee6a25573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGDIR= /home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "if \"TAGDIR\" not in os.environ:\n",
    "  os.environ[\"TAGDIR\"] = str(Path.cwd().parent)\n",
    "  tagdir = Path(os.environ[\"TAGDIR\"]).resolve()\n",
    "  (outputs := tagdir/\"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "  (tagdir/\"outputs\"/\"csv_logs\").mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "  tagdir = Path(os.environ[\"TAGDIR\"]).resolve()\n",
    "for p in (tagdir, tagdir.parent):\n",
    "  sp = str(p)\n",
    "  if sp not in sys.path:\n",
    "    sys.path.insert(0, sp)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\",\"4\")\n",
    "os.environ.setdefault(\"TF_NUM_INTEROP_THREADS\",\"2\")\n",
    "os.environ.setdefault(\"TF_NUM_INTRAOP_THREADS\",\"4\")\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\",\"\")\n",
    "print(\"TAGDIR=\", tagdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d8c9df-654d-4b04-9d79-e496c86fb692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TAGDIR\" not in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e1d42e-7b8a-4b8d-be47-2beeb680189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 09:14:04.008583: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 552960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  1/352 [..............................] - ETA: 16:37 - loss: 2.3117 - accuracy: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 09:14:17.442781: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 27998208 exceeds 10% of free system memory.\n",
      "2025-09-07 09:14:17.447657: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 27998208 exceeds 10% of free system memory.\n",
      "2025-09-07 09:14:17.947457: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 27998208 exceeds 10% of free system memory.\n",
      "2025-09-07 09:14:17.953139: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 27998208 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 179s 500ms/step - loss: 1.6201 - accuracy: 0.4102 - val_loss: 1.2937 - val_accuracy: 0.5422\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 174s 493ms/step - loss: 1.2498 - accuracy: 0.5528 - val_loss: 1.1626 - val_accuracy: 0.5910\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 187s 532ms/step - loss: 1.0759 - accuracy: 0.6209 - val_loss: 1.1021 - val_accuracy: 0.6134\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 178s 505ms/step - loss: 0.9589 - accuracy: 0.6628 - val_loss: 0.9323 - val_accuracy: 0.6800\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 171s 485ms/step - loss: 0.8537 - accuracy: 0.7013 - val_loss: 0.8888 - val_accuracy: 0.6896\n",
      "[DONE] test_acc=0.6780\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'log_test_summary' from 'scripts.py_utils_p_03_e2e' (/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_utils_p_03_e2e.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE] test_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy_utils_p_03_e2e\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_test_summary\n\u001b[1;32m     25\u001b[0m log_test_summary(acc, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(loss), seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m137\u001b[39m, tagdir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAGDIR\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'log_test_summary' from 'scripts.py_utils_p_03_e2e' (/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_utils_p_03_e2e.py)"
     ]
    }
   ],
   "source": [
    "# Train/evaluate/log (baseline)\n",
    "\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\")/255.0\n",
    "x_test = x_test.astype(\"float32\")/255.0\n",
    "from tensorflow.keras import layers, models\n",
    "m = models.Sequential([\n",
    "  layers.Input((32,32,3)),\n",
    "  layers.Conv2D(32,3,activation=\"relu\"),\n",
    "  layers.Conv2D(32,3,activation=\"relu\"),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64,3,activation=\"relu\"),\n",
    "  layers.Conv2D(64,3,activation=\"relu\"),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "m.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "m.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1, verbose=1)\n",
    "loss, acc = m.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"[DONE] test_acc={acc:.4f}\")\n",
    "from scripts.py_utils_p_03_e2e import log_test_summary\n",
    "log_test_summary(acc, loss=float(loss), seed=137, tagdir=os.environ[\"TAGDIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e140227c-d4d0-42a1-8d27-fb6886c43993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import; if the function isn't there yet, define a minimal fallback.\n",
    "try:\n",
    "    from scripts.py_utils_p_03_e2e import log_test_summary\n",
    "except (ImportError, AttributeError):\n",
    "    import json, csv, os, time\n",
    "    from pathlib import Path\n",
    "    def log_test_summary(acc: float, loss: float, seed: int, tagdir: str):\n",
    "        td = Path(tagdir).resolve()\n",
    "        out = td / \"outputs\"\n",
    "        out.mkdir(parents=True, exist_ok=True)\n",
    "        ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # JSON\n",
    "        js = {\"timestamp\": ts, \"seed\": seed, \"test_acc\": float(acc), \"loss\": float(loss)}\n",
    "        with open(out / f\"test_summary_seed{seed}_{ts}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(js, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # CSV\n",
    "        csv_path = out / f\"csv_logs/train_history_seed{seed}_{ts}.csv\"\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"timestamp\",\"seed\",\"metric\",\"value\"])\n",
    "            w.writerow([ts, seed, \"test_acc\", float(acc)])\n",
    "            w.writerow([ts, seed, \"loss\", float(loss)])\n",
    "\n",
    "        print(f\"[DONE] test_acc={acc:.4f}  | JSON & CSV written to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d950fc5-6450-4818-8d0c-19b26a47b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] test_acc=0.6780  | JSON & CSV written to /home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "log_test_summary(acc, loss=float(loss), seed=137, tagdir=os.environ[\"TAGDIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a712024-f795-4935-8d77-352375b73a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d50a41-a266-48d0-b5b9-68a448e4d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dad2576c-ea8c-4b89-9b6d-0a2a72510892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ad102ac-0112-4503-843c-24d1268f3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll just have to check stuff on the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002a59-a808-4612-bcf3-d7c954f7ad8b",
   "metadata": {},
   "source": [
    "```bash\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ cd test_project_bash/p_03_e2e/\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n",
    "$ ls -1 outputs/\n",
    "csv_logs\n",
    "gradcam_images\n",
    "test_summary_seed137_20250907_095057.json\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n",
    "$ ls -1 outputs/csv_logs\n",
    "train_history_seed137_20250907_095057.csv\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n",
    "$ wc -l < outputs/test_summary_seed137_20250907_095057.json \n",
    "5\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n",
    "$ cat outputs/test_summary_seed137_20250907_095057.json \n",
    "{\n",
    "  \"timestamp\": \"20250907_095057\",\n",
    "  \"seed\": 137,\n",
    "  \"test_acc\": 0.6779999732971191,\n",
    "  \"loss\": 0.9174857139587402\n",
    "}=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n",
    "$ wc -l < outputs/csv_logs/train_history_seed137_20250907_095057.csv \n",
    "3\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n",
    "$ cat outputs/csv_logs/train_history_seed137_20250907_095057.csv \n",
    "timestamp,seed,metric,value\n",
    "20250907_095057,137,test_acc,0.6779999732971191\n",
    "20250907_095057,137,loss,0.9174857139587402\n",
    "=====..........................................................retval=0.........\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ elapsed --check\n",
    "3882\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ echo \"$(elapsed --check) / 60\" | bc -l\n",
    "64.95000000000000000000\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "```\n",
    "\n",
    "### About an hour isn't too bad,\n",
    "\n",
    "especially considering the training time on a CPU. We had things\n",
    "training by 45 minutes. I'm going to `cat` a lot of the scripts\n",
    "here, just to have backup in another place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876079a7-103e-4a33-ad63-edd112bcf73e",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274d949-e511-4c6d-8080-a00d566142f5",
   "metadata": {},
   "source": [
    "### `bash` scripts\n",
    "\n",
    "```bash\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e\n",
    "$ cd ../../../\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    " <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb\n",
    "$ cd fhtw-paper-code-prep/\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ find . -type f -iname \"*.sh\" | wc -l\n",
    "17\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ find . -type f -iname \"*.py\" | wc -l\n",
    "18\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ find . -type f -iname \"*.sh\" | sort | wc -l\n",
    "17\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ find . -type f -iname \"*.sh\" | sort | head\n",
    "./bin/audit_backup.sh\n",
    "./bin/backup_everything.sh\n",
    "./bin/backup_notebook_bakstamp.sh\n",
    "./bin/backup_notebooks.sh\n",
    "./bin/elapsed.sh\n",
    "./bin/fix_firstline_glitches.sh\n",
    "./bin/start_cifar_lab.sh\n",
    "./bin/sys_capture.sh\n",
    "./bin/system_info_bash_fallbacks.sh\n",
    "./structure.sh\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ find . -type f -iname \"*.sh\" | sort | tr '\\n' '\\0' | xargs -I{} -0 bash -c 'orig={}; sixtydash=\"--------------------------------------------------\"; tendash=\"----------\"; echo \"${sixtydash}\"; echo \"Script at:\"; echo \"$(realpath {})\"; echo \"${tendash}\"; cat {}; echo \"${sixtydash}\"; echo;'\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/audit_backup.sh\n",
    "----------\n",
    "#!/usr/bin/env bash\n",
    "# Standalone audit for an existing backup tarball.\n",
    "# Usage: audit_backup.sh <archive.tar.gz> <tag_path>\n",
    "set -euo pipefail\n",
    "OUT=\"${1:?Usage: $(basename \"$0\") <archive.tar.gz> <tag>}\"\n",
    "TAG=\"${2:?tag path e.g. test_project_bash/p_01}\"\n",
    "\n",
    "LIST=\"$(tar -tzf \"$OUT\")\"\n",
    "has(){ echo \"$LIST\" | grep -Eq \"$1\"; }\n",
    "\n",
    "report(){\n",
    "  local label=\"$1\" pat=\"$2\" warn=\"$3\"\n",
    "  if has \"$pat\"; then printf \"  - %-32s OK\\n\" \"$label\"\n",
    "  else printf \"  - %-32s %s\\n\" \"$label\" \"$warn\"\n",
    "  fi\n",
    "}\n",
    "\n",
    "echo \"=== Quick audit for $(basename \"$OUT\") ===\"\n",
    "report \"README_<tag>.md present?\" \"(^|.*/)${TAG}/README_.*\\\\.md$\" \"WARN: not found\"\n",
    "report \"notebooks/*.ipynb present?\" \"(^|.*/)${TAG}/notebooks/.*\\\\.ipynb$\" \"WARN: none found\"\n",
    "report \"scripts/*.py present?\" \"(^|.*/)${TAG}/scripts/.*\\\\.py$\" \"WARN: none found\"\n",
    "report \"__init__.py at tag root?\" \"(^|.*/)${TAG}/__init__\\\\.py$\" \"WARN: not found\"\n",
    "report \"scripts/__init__.py?\" \"(^|.*/)${TAG}/scripts/__init__\\\\.py$\" \"WARN: not found\"\n",
    "report \"env specs included?\" \"(^|.*/)environment_specifications(/|$)\" \"WARN: not included\"\n",
    "report \"bin/ included?\" \"(^|.*/)bin(/|$)\" \"WARN: not included\"\n",
    "report \"firstline audit (before)?\" \"(^|.*/)_staging_[^/]+/backup_meta/firstline_before\\\\.txt$\" \"WARN: not included\"\n",
    "report \"firstline audit (after)?\" \"(^|.*/)_staging_[^/]+/backup_meta/firstline_after\\\\.txt$\" \"INFO: not run\"\n",
    "report \"validate_env output?\" \"(^|.*/)_staging_[^/]+/backup_meta/validate_env_.*\\\\.txt$\" \"INFO: validate_env not run\"\n",
    "report \"sys_capture status?\" \"(^|.*/)_staging_[^/]+/backup_meta/sys_capture_status\\\\.txt$\" \"INFO: sys_capture not run\"\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/backup_everything.sh\n",
    "----------\n",
    "#!/usr/bin/env bash\n",
    "# @file : backup_everything.sh\n",
    "# Backup an experiment tag plus optional extras, with firstline audits.\n",
    "# Usage:\n",
    "#   backup_everything.sh <ROOT_DIR> <tag_path> [--expanded] [--skip-sys] [--skip-nbs] [--fix-firstline]\n",
    "# Notes:\n",
    "# - --expanded bundles: environment_specifications/, bin/, and validate_env.py output.\n",
    "# - We DO NOT call backup_notebook_bakstamp.sh (that’s for one-off notebooks).\n",
    "#\n",
    "# Typical COMPLETE Usage example\n",
    "#\n",
    "# ROOT=~/my_repos_dwb/fhtw-paper-code-prep\n",
    "#\n",
    "# #  Tight backup with “before” firstline report\n",
    "# \"$ROOT/bin/backup_everything.sh\" \"$ROOT\" test_project_bash/p01\n",
    "#\n",
    "# #  Expanded backup (adds env specs, bin/, validate_env.py output) + apply\n",
    "# #+ fixes and include “after” report\n",
    "# \"$ROOT/bin/backup_everything.sh\" \"$ROOT\" test_project_bash/p_01 --expanded --fix-firstline\n",
    "#\n",
    "set -euo pipefail\n",
    "\n",
    "if (( $# < 2 )); then\n",
    "  echo \"Usage: $(basename \"$0\") <ROOT_DIR> <tag_path> [--expanded] [--skip-sys] [--skip-nbs] [--fix-firstline]\" >&2\n",
    "  exit 2\n",
    "fi\n",
    "\n",
    "ROOT=\"$1\"; shift\n",
    "TAG=\"$1\";  shift\n",
    "\n",
    "EXPANDED=0\n",
    "RUN_SYS=1\n",
    "RUN_NBS=1\n",
    "DO_FIX_FIRSTLINE=0\n",
    "while (( $# )); do\n",
    "  case \"$1\" in\n",
    "    --expanded) EXPANDED=1 ;;\n",
    "    --skip-sys) RUN_SYS=0 ;;\n",
    "    --skip-nbs) RUN_NBS=0 ;;\n",
    "    --fix-firstline) DO_FIX_FIRSTLINE=1 ;;\n",
    "    *) echo \"Unknown option: $1\" >&2; exit 2 ;;\n",
    "  esac\n",
    "  shift\n",
    "done\n",
    "\n",
    "cd \"$ROOT\"\n",
    "\n",
    "TS=\"$(date +'%s_%Y-%m-%dT%H%M%S%z')\"\n",
    "BKP_DIR=\"$ROOT/backups\"\n",
    "mkdir -p \"$BKP_DIR\"\n",
    "\n",
    "# Staging area for meta artifacts to include in the tarball\n",
    "STAGE=\"$BKP_DIR/_staging_$TS\"\n",
    "mkdir -p \"$STAGE/backup_meta\"\n",
    "META=\"$STAGE/backup_meta\"\n",
    "\n",
    "# 1) Firstline audit (before) via fix_firstline_glitches.sh --dry-run\n",
    "FIXER=\"$ROOT/bin/fix_firstline_glitches.sh\"\n",
    "if [[ -x \"$FIXER\" ]]; then\n",
    "  echo \"[info] Running firstline audit (before) ...\"\n",
    "  if \"$FIXER\" --dry-run >\"$META/firstline_before.txt\" 2>\"$META/firstline_before.err\"; then\n",
    "    echo \"ok\" > \"$META/firstline_before.status\"\n",
    "  else\n",
    "    echo \"nonzero-exit\" > \"$META/firstline_before.status\"\n",
    "  fi\n",
    "else\n",
    "  echo \"[info] No fixer found at $FIXER (skipping firstline audit).\"\n",
    "fi\n",
    "\n",
    "# 2) Optional: perform fixes and capture after-report\n",
    "if [[ -x \"$FIXER\" && $DO_FIX_FIRSTLINE -eq 1 ]]; then\n",
    "  echo \"[info] Applying firstline fixes (live) ...\"\n",
    "  if \"$FIXER\" >\"$META/firstline_after.txt\" 2>\"$META/firstline_after.err\"; then\n",
    "    echo \"ok\" > \"$META/firstline_after.status\"\n",
    "  else\n",
    "    echo \"nonzero-exit\" > \"$META/firstline_after.status\"\n",
    "  fi\n",
    "fi\n",
    "\n",
    "# 3) Optional helpers\n",
    "if (( RUN_SYS )) && [[ -x \"$ROOT/bin/sys_capture.sh\" ]]; then\n",
    "  echo \"[info] Running sys_capture.sh ...\"\n",
    "  \"$ROOT/bin/sys_capture.sh\" >\"$META/sys_capture_stdout.txt\" 2>\"$META/sys_capture_stderr.txt\" || echo \"nonzero-exit\" > \"$META/sys_capture_status.txt\"\n",
    "  [[ -f \"$META/sys_capture_status.txt\" ]] || echo \"ok\" > \"$META/sys_capture_status.txt\"\n",
    "fi\n",
    "\n",
    "if (( RUN_NBS )) && [[ -x \"$ROOT/bin/backup_notebooks.sh\" ]]; then\n",
    "  echo \"[info] Running backup_notebooks.sh (all notebooks) ...\"\n",
    "  \"$ROOT/bin/backup_notebooks.sh\" >\"$META/backup_notebooks_stdout.txt\" 2>\"$META/backup_notebooks_stderr.txt\" || echo \"nonzero-exit\" > \"$META/backup_notebooks_status.txt\"\n",
    "  [[ -f \"$META/backup_notebooks_status.txt\" ]] || echo \"ok\" > \"$META/backup_notebooks_status.txt\"\n",
    "fi\n",
    "\n",
    "# 4) Validate env (always validate_env.py if present)\n",
    "if (( EXPANDED )) && [[ -f \"$ROOT/validate_env.py\" ]]; then\n",
    "  echo \"[info] Running validate_env.py ...\"\n",
    "  { python \"$ROOT/validate_env.py\"; } >\"$META/validate_env_stdout.txt\" 2>\"$META/validate_env_stderr.txt\" || true\n",
    "fi\n",
    "\n",
    "# 5) Build include list\n",
    "INCLUDE_PATHS=( \"$TAG\" \"$STAGE\" )\n",
    "if (( EXPANDED )); then\n",
    "  [[ -d environment_specifications ]] && INCLUDE_PATHS+=( environment_specifications )\n",
    "  [[ -d bin ]]                        && INCLUDE_PATHS+=( bin )\n",
    "fi\n",
    "\n",
    "OUT_BASE=\"$(echo \"$TAG\" | tr '/' '_')\"\n",
    "OUT_NAME=\"${OUT_BASE}_$TS\"\n",
    "[[ $EXPANDED -eq 1 ]] && OUT_NAME=\"${OUT_BASE}_plus_env_and_bin_$TS\"\n",
    "OUT=\"$BKP_DIR/${OUT_NAME}.tar.gz\"\n",
    "\n",
    "# 6) Create archive (aligns with your atreea-style exclusions)\n",
    "tar --create --gzip --file \"$OUT\" \\\n",
    "  --exclude='*/outputs/*' \\\n",
    "  --exclude='*/datasets/*' \\\n",
    "  --exclude='*/models/*' \\\n",
    "  --exclude='*/what_was_inside_notebook_directory_try1/*' \\\n",
    "  --exclude='*/.ipynb_checkpoints/*' \\\n",
    "  --exclude='*/__pycache__/*' \\\n",
    "  --exclude='*/bash_terminal_io_lab_notebooks/*' \\\n",
    "  --exclude='*/.DS_Store' \\\n",
    "  \"${INCLUDE_PATHS[@]}\"\n",
    "\n",
    "# 7) Quick verify + checksum\n",
    "COUNT=$(tar -tzf \"$OUT\" | wc -l | awk '{print $1}')\n",
    "echo \"[ok] Archive contains $COUNT entries.\"\n",
    "sha256sum \"$OUT\" | tee \"$OUT.sha256\" >/dev/null\n",
    "echo \"[ok] Backup written: $OUT\"\n",
    "echo \"[ok] SHA256 saved:   $OUT.sha256\"\n",
    "\n",
    "# 8) Inclusion audit (tolerant patterns)\n",
    "AUDIT_LOG=\"$BKP_DIR/${OUT_NAME}.audit.txt\"\n",
    "LIST=\"$(tar -tzf \"$OUT\")\"\n",
    "\n",
    "has() { echo \"$LIST\" | grep -Eq \"$1\"; }\n",
    "\n",
    "{\n",
    "  echo \"=== Inclusion audit for $OUT_NAME ===\"\n",
    "  echo \"Timestamp: $TS\"\n",
    "  echo\n",
    "  echo \"[Expect] Tag root: $TAG\"\n",
    "\n",
    "  # Allow optional ./ prefix and be resilient\n",
    "  has \"(^|.*/)${TAG}/README_.*\\\\.md$\" \\\n",
    "    && echo \"  - README_<tag>.md present?    OK\" \\\n",
    "    || echo \"  - README_<tag>.md present?    WARN: not found\"\n",
    "\n",
    "  has \"(^|.*/)${TAG}/notebooks/.*\\\\.ipynb$\" \\\n",
    "    && echo \"  - notebooks/*.ipynb present?  OK\" \\\n",
    "    || echo \"  - notebooks/*.ipynb present?  WARN: none found\"\n",
    "\n",
    "  has \"(^|.*/)${TAG}/scripts/.*\\\\.py$\" \\\n",
    "    && echo \"  - scripts/*.py present?       OK\" \\\n",
    "    || echo \"  - scripts/*.py present?       WARN: none found\"\n",
    "\n",
    "  has \"(^|.*/)${TAG}/__init__\\\\.py$\" \\\n",
    "    && echo \"  - __init__.py at tag root?    OK\" \\\n",
    "    || echo \"  - __init__.py at tag root?    WARN: not found\"\n",
    "\n",
    "  has \"(^|.*/)${TAG}/scripts/__init__\\\\.py$\" \\\n",
    "    && echo \"  - scripts/__init__.py?        OK\" \\\n",
    "    || echo \"  - scripts/__init__.py?        WARN: not found\"\n",
    "\n",
    "  if (( EXPANDED )); then\n",
    "    has \"(^|.*/)environment_specifications(/|$)\" \\\n",
    "      && echo \"  - environment_specifications/? OK\" \\\n",
    "      || echo \"  - environment_specifications/? WARN: not included\"\n",
    "\n",
    "    has \"(^|.*/)bin(/|$)\" \\\n",
    "      && echo \"  - bin/?                       OK\" \\\n",
    "      || echo \"  - bin/?                       WARN: not included\"\n",
    "\n",
    "    # Staging meta (handle absolute-to-relative path strip by tar)\n",
    "    has \"(^|.*/)_staging_[^/]+/backup_meta/validate_env_.*\\\\.txt$\" \\\n",
    "      && echo \"  - validate_env output in backup_meta/? OK\" \\\n",
    "      || echo \"  - validate_env output in backup_meta/? INFO: validate_env not run or produced no files\"\n",
    "  fi\n",
    "\n",
    "  has \"(^|.*/)_staging_[^/]+/backup_meta/firstline_before\\\\.txt$\" \\\n",
    "    && echo \"  - firstline audit (before)?   OK\" \\\n",
    "    || echo \"  - firstline audit (before)?   WARN: not included\"\n",
    "\n",
    "  has \"(^|.*/)_staging_[^/]+/backup_meta/firstline_after\\\\.txt$\" \\\n",
    "    && echo \"  - firstline audit (after)?    OK\" \\\n",
    "    || echo \"  - firstline audit (after)?    INFO: not run\"\n",
    "\n",
    "  has \"(^|.*/)_staging_[^/]+/backup_meta/sys_capture_status\\\\.txt$\" \\\n",
    "    && echo \"  - sys_capture status?         OK\" \\\n",
    "    || echo \"  - sys_capture status?         INFO: sys_capture not run\"\n",
    "\n",
    "  echo\n",
    "} > \"$AUDIT_LOG\"\n",
    "echo \"[ok] Wrote audit: $AUDIT_LOG\"\n",
    "\n",
    "# 9) Clean up staging (the tarball already contains it)\n",
    "rm -rf \"$STAGE\"\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/backup_notebook_bakstamp.sh\n",
    "----------\n",
    "#!/usr/bin/env bash\n",
    "# backup_notebook_bakstamp.sh\n",
    "# Create timestamped .bak copies of one or more notebooks in-place.\n",
    "# Usage: backup_notebook_bakstamp.sh NOTEBOOK.ipynb [MORE.ipynb ...]\n",
    "# Pattern: file.ipynb.$(date +'%s_%Y-%m-%dT%H%M%S%z').bak\n",
    "\n",
    "set -Eeuo pipefail\n",
    "\n",
    "if [[ $# -lt 1 ]]; then\n",
    "  echo \"Usage: $(basename \"$0\") NOTEBOOK.ipynb [MORE.ipynb ...]\" >&2\n",
    "  exit 2\n",
    "fi\n",
    "\n",
    "stamp=\"$(date +'%s_%Y-%m-%dT%H%M%S%z')\"\n",
    "\n",
    "for nb in \"$@\"; do\n",
    "  if [[ ! -f \"$nb\" ]]; then\n",
    "    echo \"Skipping non-existent: $nb\" >&2\n",
    "    continue\n",
    "  fi\n",
    "  dir=\"$(dirname \"$nb\")\"\n",
    "  base=\"$(basename \"$nb\")\"\n",
    "  bak=\"${dir}/${base}.${stamp}.bak\"\n",
    "  cp -v -- \"$nb\" \"$bak\"\n",
    "done\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/backup_notebooks.sh\n",
    "----------\n",
    "\\\n",
    "#!/usr/bin/env bash\n",
    "# backup_notebooks.sh\n",
    "# Make timestamped .bak copies of notebooks in ./notebooks/ (or a chosen dir).\n",
    "# Optionally prune older backups, keeping only the most recent N per notebook.\n",
    "#\n",
    "# Timestamp pattern: $(date +'%s_%Y-%m-%dT%H%M%S%z')\n",
    "\n",
    "set -Eeuo pipefail\n",
    "\n",
    "#PROJECT_DIR=\"${PROJECT_DIR:-$PWD}\"\n",
    "PROJECT_DIR=${PROJECT_DIR:-$HOME/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01}\n",
    "NB_DIR_DEFAULT=\"notebooks\"\n",
    "NB_DIR=\"\"\n",
    "PRUNE=0\n",
    "KEEP=\"${KEEP:-3}\"\n",
    "\n",
    "usage() {\n",
    "  cat <<USAGE\n",
    "Usage: $(basename \"$0\") [options] [NOTEBOOK.ipynb ...]\n",
    "\n",
    "Without arguments, operates on all *.ipynb under ./notebooks/.\n",
    "\n",
    "Options:\n",
    "  -p, --project DIR     Project directory (default: \\$PWD)\n",
    "  -n, --nbdir DIR       Notebooks subdirectory relative to project (default: notebooks)\n",
    "  --prune               After backup, prune old backups, keeping last N (see --keep)\n",
    "  -k, --keep N          Number of backups to keep per notebook when pruning (default: 3)\n",
    "  -h, --help            Show this help and exit\n",
    "\n",
    "Env overrides:\n",
    "  PROJECT_DIR, KEEP\n",
    "\n",
    "Examples:\n",
    "  $(basename \"$0\")\n",
    "  $(basename \"$0\") --prune --keep 5\n",
    "  $(basename \"$0\") -p \"$HOME/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01\" --prune\n",
    "  $(basename \"$0\") notebooks/01_model_build_p_01.ipynb\n",
    "USAGE\n",
    "}\n",
    "\n",
    "# Parse flags\n",
    "args=()\n",
    "while [[ $# -gt 0 ]]; do\n",
    "  case \"$1\" in\n",
    "    -p|--project) PROJECT_DIR=\"$2\"; shift 2;;\n",
    "    -n|--nbdir) NB_DIR=\"$2\"; shift 2;;\n",
    "    --prune) PRUNE=1; shift;;\n",
    "    -k|--keep) KEEP=\"$2\"; shift 2;;\n",
    "    -h|--help) usage; exit 0;;\n",
    "    --) shift; break;;\n",
    "    -*)\n",
    "      echo \"Unknown option: $1\" >&2\n",
    "      usage; exit 2\n",
    "      ;;\n",
    "    *)\n",
    "      args+=(\"$1\"); shift;;\n",
    "  esac\n",
    "done\n",
    "set -- \"${args[@]}\"\n",
    "\n",
    "# Resolve notebook directory\n",
    "if [[ -z \"${NB_DIR}\" ]]; then\n",
    "  NB_DIR=\"${PROJECT_DIR}/${NB_DIR_DEFAULT}\"\n",
    "elif [[ \"${NB_DIR}\" != /* ]]; then\n",
    "  NB_DIR=\"${PROJECT_DIR}/${NB_DIR}\"\n",
    "fi\n",
    "\n",
    "if [[ ! -d \"$NB_DIR\" ]]; then\n",
    "  echo \"No notebooks directory found at: $NB_DIR\" >&2\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "stamp=\"$(date +'%s_%Y-%m-%dT%H%M%S%z')\"\n",
    "\n",
    "backup_one() {\n",
    "  local nb=\"$1\"\n",
    "  if [[ ! -f \"$nb\" ]]; then\n",
    "    echo \"Skipping non-existent: $nb\" >&2\n",
    "    return 0\n",
    "  fi\n",
    "  local bak=\"${nb}.${stamp}.bak\"\n",
    "  cp -v -- \"$nb\" \"$bak\"\n",
    "  if [[ $PRUNE -eq 1 ]]; then\n",
    "    prune_backups \"$nb\"\n",
    "  fi\n",
    "}\n",
    "\n",
    "prune_backups() {\n",
    "  local nb=\"$1\"\n",
    "  # List backups for this notebook, sorted by mtime descending, keep first $KEEP, remove rest\n",
    "  # Note: relies on filenames without newlines; standard in practice.\n",
    "  local dir base\n",
    "  dir=\"$(dirname \"$nb\")\"\n",
    "  base=\"$(basename \"$nb\")\"\n",
    "  # Gather list\n",
    "  mapfile -t all_baks < <(ls -1t -- \"$dir/${base}.\"*\".bak\" 2>/dev/null || true)\n",
    "  if (( ${#all_baks[@]} > KEEP )); then\n",
    "    # Determine deletions\n",
    "    local to_delete=( \"${all_baks[@]:KEEP}\" )\n",
    "    for old in \"${to_delete[@]}\"; do\n",
    "      echo \"Pruning old backup: $old\"\n",
    "      rm -f -- \"$old\"\n",
    "    done\n",
    "  fi\n",
    "}\n",
    "\n",
    "# Collect notebooks to operate on\n",
    "nb_list=()\n",
    "if [[ $# -gt 0 ]]; then\n",
    "  # Use provided paths\n",
    "  for p in \"$@\"; do\n",
    "    if [[ -d \"$p\" ]]; then\n",
    "      while IFS= read -r -d '' f; do nb_list+=(\"$f\"); done < <(find \"$p\" -maxdepth 1 -type f -name \"*.ipynb\" -print0)\n",
    "    else\n",
    "      nb_list+=(\"$p\")\n",
    "    fi\n",
    "  done\n",
    "else\n",
    "  while IFS= read -r -d '' f; do nb_list+=(\"$f\"); done < <(find \"$NB_DIR\" -maxdepth 1 -type f -name \"*.ipynb\" -print0)\n",
    "fi\n",
    "\n",
    "if [[ ${#nb_list[@]} -eq 0 ]]; then\n",
    "  echo \"No notebooks found to back up.\" >&2\n",
    "  exit 0\n",
    "fi\n",
    "\n",
    "for nb in \"${nb_list[@]}\"; do\n",
    "  backup_one \"$nb\"\n",
    "done\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/elapsed.sh\n",
    "----------\n",
    "elapsed() {\n",
    "  local state_file=\"/tmp/.elapsed_timer\"\n",
    "\n",
    "  case \"$1\" in\n",
    "    -s|--start)\n",
    "      date +%s > \"$state_file\"\n",
    "      echo \"Stopwatch started.\"\n",
    "      ;;\n",
    "    -p|--stop)\n",
    "      if [[ -f \"$state_file\" ]]; then\n",
    "        local start=$(<\"$state_file\")\n",
    "        local now=$(date +%s)\n",
    "        local diff=$((now - start))\n",
    "        echo \"$diff\"\n",
    "        rm -f \"$state_file\"\n",
    "      else\n",
    "        echo \"No stopwatch running. Use --start to begin.\"\n",
    "      fi\n",
    "      ;;\n",
    "    -c|--check)\n",
    "      if [[ -f \"$state_file\" ]]; then\n",
    "        local start=$(<\"$state_file\")\n",
    "        local now=$(date +%s)\n",
    "        local diff=$((now - start))\n",
    "        echo \"$diff\"\n",
    "      else\n",
    "        echo \"No stopwatch running. Use --start to begin.\"\n",
    "      fi\n",
    "      ;;\n",
    "    -h|--help|*)\n",
    "      cat <<EOF\n",
    "Usage: elapsed [OPTION]\n",
    "\n",
    "Options:\n",
    "  -s, --start   Start the stopwatch\n",
    "  -c, --check   Show elapsed time (in seconds, stopwatch keeps running)\n",
    "  -p, --stop    Stop the stopwatch and show elapsed time (in seconds)\n",
    "  -h, --help    Show this help message\n",
    "\n",
    "If run without an option, this help will be displayed.\n",
    "EOF\n",
    "      ;;\n",
    "  esac\n",
    "}\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/fix_firstline_glitches.sh\n",
    "----------\n",
    "#!/usr/bin/env bash\n",
    "# Fix common first-line glitches in *.sh and *.py:\n",
    "#  - Remove a standalone backslash \"\\\" on line 1\n",
    "#  - Remove a UTF-8 BOM on line 1\n",
    "# Optional: --add-shebang will add a shebang to files that lack one,\n",
    "#           choosing based on extension (.sh -> bash, .py -> python3).\n",
    "#\n",
    "# It first creates a repo backup tarball (no spaces in name), then\n",
    "# prints a per-file report of changes.\n",
    "#\n",
    "# Usage:\n",
    "#   fix_firstline_glitches.sh [--root <dir>] [--dry-run] [--add-shebang]\n",
    "#                              [--ext \".sh,.py\"] [--no-backup]\n",
    "#\n",
    "# Defaults:\n",
    "#   --root: git root if available, else CWD\n",
    "#   --ext:  \".sh,.py\"\n",
    "#   backup: ON (writes to <root>/backups)\n",
    "#\n",
    "#\n",
    "# More-complete Usage examples\n",
    "#\n",
    "# # Normal run at repo root,default extensions (.sh,.py),with backup and report\n",
    "# bin/fix_firstline_glitches.sh\n",
    "#\n",
    "# # Dry-run (no changes), just show what *would* be done\n",
    "# bin/fix_firstline_glitches.sh --dry-run\n",
    "#\n",
    "# # Opt-in: also add shebangs to files that lack one (based on extension)\n",
    "# bin/fix_firstline_glitches.sh --add-shebang\n",
    "# \n",
    "# # Run on a subdirectory only (e.g., your tag dir)\n",
    "# bin/fix_firstline_glitches.sh --root \"$ROOT/test_project_bash/p_01\"\n",
    "#\n",
    "# # Different extensions (add .ps1 if desired)\n",
    "# bin/fix_firstline_glitches.sh --ext \".sh,.py,.ps1\"\n",
    "#\n",
    "\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# --- args ---\n",
    "ROOT=\"\"\n",
    "DRY_RUN=0\n",
    "ADD_SHEBANG=0\n",
    "DO_BACKUP=1\n",
    "EXT_LIST=\".sh,.py\"\n",
    "\n",
    "while (( $# )); do\n",
    "  case \"$1\" in\n",
    "    --root) ROOT=\"${2:?}\"; shift 2;;\n",
    "    --dry-run) DRY_RUN=1; shift;;\n",
    "    --add-shebang) ADD_SHEBANG=1; shift;;\n",
    "    --ext) EXT_LIST=\"${2:?}\"; shift 2;;\n",
    "    --no-backup) DO_BACKUP=0; shift;;\n",
    "    -h|--help)\n",
    "      cat <<EOH\n",
    "Usage: $(basename \"$0\") [--root <dir>] [--dry-run] [--add-shebang] [--ext \".sh,.py\"] [--no-backup]\n",
    "EOH\n",
    "      exit 0;;\n",
    "    *)\n",
    "      echo \"Unknown option: $1\" >&2; exit 2;;\n",
    "  esac\n",
    "done\n",
    "\n",
    "# --- resolve root ---\n",
    "if [[ -z \"$ROOT\" ]]; then\n",
    "  if git rev-parse --show-toplevel >/dev/null 2>&1; then\n",
    "    ROOT=$(git rev-parse --show-toplevel)\n",
    "  else\n",
    "    ROOT=$(pwd -P)\n",
    "  fi\n",
    "fi\n",
    "cd \"$ROOT\"\n",
    "\n",
    "# --- sed in-place portability ---\n",
    "if sed --version >/dev/null 2>&1; then\n",
    "  SED_INPLACE=(sed -i)\n",
    "else\n",
    "  SED_INPLACE=(sed -i '')\n",
    "fi\n",
    "\n",
    "timestamp() { date +'%s_%Y-%m-%dT%H%M%S%z'; }\n",
    "\n",
    "# --- backup ---\n",
    "BACKUP_PATH=\"\"\n",
    "if (( DO_BACKUP )); then\n",
    "  TS=\"$(timestamp)\"\n",
    "  mkdir -p \"$ROOT/backups\"\n",
    "  BACKUP_BASENAME=\"fix_firstline_glitches_${TS}.tar.gz\"\n",
    "  BACKUP_PATH=\"$ROOT/backups/$BACKUP_BASENAME\"\n",
    "  echo \"Backing up to $BACKUP_PATH\"\n",
    "  # Exclude the backups dir (and this file), and suppress 'file changed as we read it' exits\n",
    "  tar --create --gzip --file \"$BACKUP_PATH\" \\\n",
    "      --exclude='./backups/*' \\\n",
    "      --exclude=\"./backups/$BACKUP_BASENAME\" \\\n",
    "      --exclude='*/outputs/*' \\\n",
    "      --exclude='*/datasets/*' \\\n",
    "      --exclude='*/models/*' \\\n",
    "      --exclude='*/what_was_inside_notebook_directory_try1/*' \\\n",
    "      --exclude='*/.ipynb_checkpoints/*' \\\n",
    "      --exclude='*/__pycache__/*' \\\n",
    "      --exclude='*/bash_terminal_io_lab_notebooks/*' \\\n",
    "      --exclude='*/.DS_Store' \\\n",
    "      --warning=no-file-changed \\\n",
    "      --ignore-failed-read \\\n",
    "      . || true\n",
    "  sha256sum \"$BACKUP_PATH\" | tee \"$BACKUP_PATH.sha256\" >/dev/null || true\n",
    "  echo \"Done backing up\"\n",
    "  echo\n",
    "fi\n",
    "\n",
    "# --- collect target files ---\n",
    "# Turn \".sh,.py\" into a find predicate\n",
    "IFS=',' read -r -a exts <<< \"$EXT_LIST\"\n",
    "pred=()\n",
    "for e in \"${exts[@]}\"; do\n",
    "  e=\"${e#\".\"}\"           # strip leading dot if present\n",
    "  pred+=( -iname \"*.${e}\" -o )\n",
    "done\n",
    "# drop trailing -o\n",
    "unset 'pred[${#pred[@]}-1]'\n",
    "\n",
    "mapfile -d '' FILES < <(find . -type f \\( \"${pred[@]}\" \\) -print0)\n",
    "\n",
    "# --- helpers ---\n",
    "strip_firstline_backslash() {\n",
    "  # remove a line-1 that is exactly \"\\\" (standalone backslash)\n",
    "  \"${SED_INPLACE[@]}\" '1{/^\\\\$/d;}' \"$1\"\n",
    "}\n",
    "\n",
    "strip_bom() {\n",
    "  # remove UTF-8 BOM at start of file\n",
    "  if sed --version >/dev/null 2>&1; then\n",
    "    \"${SED_INPLACE[@]}\" '1s/^\\xEF\\xBB\\xBF//g' \"$1\"\n",
    "  else\n",
    "    \"${SED_INPLACE[@]}\" $'1s/^\\xEF\\xBB\\xBF//' \"$1\"\n",
    "  fi\n",
    "}\n",
    "\n",
    "file_is_text() {\n",
    "  LC_ALL=C grep -Iq . -- \"$1\"\n",
    "}\n",
    "\n",
    "first2_bytes_hex() {\n",
    "  head -c 2 -- \"$1\" | xxd -p -l 2 2>/dev/null || true\n",
    "}\n",
    "\n",
    "has_shebang() {\n",
    "  [[ \"$(head -c 2 -- \"$1\" 2>/dev/null || true)\" == '#!' ]]\n",
    "}\n",
    "\n",
    "add_shebang_if_requested() {\n",
    "  local f=\"$1\"\n",
    "  (( ADD_SHEBANG )) || return 0\n",
    "  has_shebang \"$f\" && return 0\n",
    "\n",
    "  # choose interpreter by extension\n",
    "  case \"${f##*.}\" in\n",
    "    sh)  line='#!/usr/bin/env bash';;\n",
    "    py)  line='#!/usr/bin/env python3';;\n",
    "    *)   return 0;;\n",
    "  esac\n",
    "\n",
    "  # Prepend shebang safely (preserving existing content)\n",
    "  if (( DRY_RUN )); then\n",
    "    return 0\n",
    "  fi\n",
    "  # If file starts with BOM, remove it first so shebang is byte 0\n",
    "  strip_bom \"$f\"\n",
    "  tmp=\"$(mktemp)\"\n",
    "  printf '%s\\n' \"$line\" > \"$tmp\"\n",
    "  cat -- \"$f\" >> \"$tmp\"\n",
    "  mv -- \"$tmp\" \"$f\"\n",
    "  chmod +x \"$f\" || true\n",
    "  return 0\n",
    "}\n",
    "\n",
    "# --- processing ---\n",
    "echo \"Checking and changing files\"\n",
    "CHANGES=0\n",
    "for f in \"${FILES[@]}\"; do\n",
    "  # skip binaries just in case\n",
    "  file_is_text \"$f\" || { echo \"$f : skipped (binary)\"; continue; }\n",
    "\n",
    "  pre_hex=\"$(first2_bytes_hex \"$f\")\"\n",
    "  pre_hassb=0; has_shebang \"$f\" && pre_hassb=1\n",
    "\n",
    "  action_msgs=()\n",
    "  changed=0\n",
    "\n",
    "  # simulate or apply\n",
    "  if (( DRY_RUN )); then\n",
    "    # dry-run: check whether we'd change anything\n",
    "    # Check BOM:\n",
    "    if [[ \"$pre_hex\" =~ ^ef ]]; then\n",
    "      action_msgs+=( \"UTF-8 BOM stripped (would)\" )\n",
    "    fi\n",
    "    # Check standalone backslash on L1\n",
    "    if head -n1 -- \"$f\" | grep -qE '^[\\\\]$'; then\n",
    "      action_msgs+=( \"first-line backslash stripped (would)\" )\n",
    "    fi\n",
    "    # Shebang add?\n",
    "    if (( ! pre_hassb )); then\n",
    "      if (( ADD_SHEBANG )); then\n",
    "        case \"${f##*.}\" in\n",
    "          sh|py) action_msgs+=( \"shebang added (would)\" );;\n",
    "        esac\n",
    "      else\n",
    "        action_msgs+=( \"doesn't start with shebang\" )\n",
    "      fi\n",
    "    fi\n",
    "  else\n",
    "    # live edits\n",
    "    # strip BOM if present\n",
    "    if [[ \"$pre_hex\" =~ ^ef ]]; then\n",
    "      strip_bom \"$f\"; action_msgs+=( \"UTF-8 BOM stripped\" ); changed=1\n",
    "    fi\n",
    "    # strip standalone backslash on line 1\n",
    "    if head -n1 -- \"$f\" | grep -qE '^[\\\\]$'; then\n",
    "      strip_firstline_backslash \"$f\"; action_msgs+=( \"first-line backslash stripped\" ); changed=1\n",
    "    fi\n",
    "    # optionally add shebang\n",
    "    if (( ! pre_hassb )); then\n",
    "      if add_shebang_if_requested \"$f\"; then\n",
    "        if (( ADD_SHEBANG )); then\n",
    "          action_msgs+=( \"shebang added\" ); changed=1\n",
    "        else\n",
    "          action_msgs+=( \"doesn't start with shebang\" )\n",
    "        fi\n",
    "      fi\n",
    "    fi\n",
    "  fi\n",
    "\n",
    "  if (( changed )); then\n",
    "    (( CHANGES++ ))\n",
    "    echo \"$f : ${action_msgs[*]}\"\n",
    "  else\n",
    "    # print consistent status line\n",
    "    if ((${#action_msgs[@]})); then\n",
    "      echo \"$f : ${action_msgs[*]}\"\n",
    "    else\n",
    "      echo \"$f : no change\"\n",
    "    fi\n",
    "  fi\n",
    "done\n",
    "\n",
    "echo\n",
    "if (( DRY_RUN )); then\n",
    "  echo \"Dry-run complete. Files that say '(would)' would be modified.\"\n",
    "else\n",
    "  echo \"Edits complete. $CHANGES file(s) modified.\"\n",
    "fi\n",
    "\n",
    "# Exit nonzero in dry-run if we WOULD change anything (useful in CI)\n",
    "if (( DRY_RUN && CHANGES > 0 )); then\n",
    "  exit 3\n",
    "fi\n",
    "\n",
    "exit 0\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/start_cifar_lab.sh\n",
    "----------\n",
    "#!/usr/bin/env bash\n",
    "# start_cifar_lab.sh\n",
    "# Helper to cd into your CIFAR-10 project tag directory, activate env, and launch JupyterLab.\n",
    "\n",
    "set -Eeuo pipefail\n",
    "\n",
    "PROJECT_DIR=\"${PROJECT_DIR:-$HOME/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01}\"\n",
    "ENV_NAME=\"${ENV_NAME:-vanillacnn}\"\n",
    "NO_BROWSER=\"${NO_BROWSER:-0}\"\n",
    "PORT=\"${PORT:-8888}\"\n",
    "ENSURE_KERNEL=\"${ENSURE_KERNEL:-0}\"\n",
    "KERNEL_NAME=\"${KERNEL_NAME:-vanillacnn}\"\n",
    "KERNEL_DISPLAY=\"${KERNEL_DISPLAY:-Python (vanillacnn)}\"\n",
    "\n",
    "usage() {\n",
    "  cat <<USAGE\n",
    "Usage: $(basename \"$0\") [options]\n",
    "\n",
    "Options:\n",
    "  -p, --project DIR      Project tag directory (default: \\$HOME/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01)\n",
    "  -e, --env NAME         Conda environment name (default: vanillacnn)\n",
    "  -k, --ensure-kernel    Ensure ipykernel is registered for this env (default: off)\n",
    "  -b, --no-browser       Launch JupyterLab without opening a browser (default: off)\n",
    "  -P, --port PORT        Jupyter port (default: 8888)\n",
    "  -h, --help             Show this help and exit\n",
    "USAGE\n",
    "}\n",
    "\n",
    "while [[ $# -gt 0 ]]; do\n",
    "  case \"$1\" in\n",
    "    -p|--project) PROJECT_DIR=\"$2\"; shift 2;;\n",
    "    -e|--env) ENV_NAME=\"$2\"; shift 2;;\n",
    "    -k|--ensure-kernel) ENSURE_KERNEL=\"1\"; shift;;\n",
    "    -b|--no-browser) NO_BROWSER=\"1\"; shift;;\n",
    "    -P|--port) PORT=\"$2\"; shift 2;;\n",
    "    -h|--help) usage; exit 0;;\n",
    "    *) echo \"Unknown option: $1\" >&2; usage; exit 2;;\n",
    "  esac\n",
    "done\n",
    "\n",
    "# Init conda or micromamba\n",
    "if command -v conda >/dev/null 2>&1; then\n",
    "  eval \"$(conda shell.bash hook)\"\n",
    "  if ! conda activate \"$ENV_NAME\" 2>/dev/null; then\n",
    "    echo \"Conda env '$ENV_NAME' not found.\" >&2\n",
    "    echo \"Create it, e.g.\" >&2\n",
    "    echo \"  conda create -n $ENV_NAME --file $HOME/my_repos_dwb/fhtw-paper-code-prep/environment_specifications/conda-linux-64.lock\" >&2\n",
    "    echo \"OR possibly\" >&2\n",
    "    echo \"  conda create -n $ENV_NAME --file conda-linux-64.lock\" >&2\n",
    "    exit 1\n",
    "  fi\n",
    "elif command -v micromamba >/dev/null 2>&1; then\n",
    "  eval \"$(micromamba shell hook -s bash)\"\n",
    "  if ! micromamba activate \"$ENV_NAME\" 2>/dev/null; then\n",
    "    echo \"Micromamba env '$ENV_NAME' not found.\" >&2\n",
    "    echo \"Create it, e.g.\" >&2\n",
    "    echo \"   micromamba create -n $ENV_NAME --file $HOME/my_repos_dwb/fhtw-paper-code-prep/environment_specifications/conda-linux-64.lock\" >&2\n",
    "    echo \"OR possibly\" >&2\n",
    "    echo \"   micromamba create -n $ENV_NAME --file conda-linux-64.lock\" >&2\n",
    "    exit 1\n",
    "  fi\n",
    "else\n",
    "  echo \"Error: Neither conda nor micromamba found on PATH.\" >&2\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "# Optional: ensure kernel\n",
    "if [[ \"${ENSURE_KERNEL}\" == \"1\" ]]; then\n",
    "  python - <<PY\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import jupyter_client  # noqa\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ipykernel\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"ipykernel\", \"install\", \"--user\",\n",
    "                       \"--name\", \"${KERNEL_NAME}\", \"--display-name\", \"${KERNEL_DISPLAY}\"])\n",
    "print(\"Kernel ensured: ${KERNEL_DISPLAY}\")\n",
    "PY\n",
    "fi\n",
    "\n",
    "cd \"$PROJECT_DIR\"\n",
    "\n",
    "if command -v jupyter-lab >/dev/null 2>&1; then\n",
    "  if [[ \"$NO_BROWSER\" == \"1\" ]]; then\n",
    "    exec jupyter-lab --no-browser --port \"$PORT\"\n",
    "  else\n",
    "    exec jupyter-lab --port \"$PORT\"\n",
    "  fi\n",
    "else\n",
    "  if [[ \"$NO_BROWSER\" == \"1\" ]]; then\n",
    "    exec jupyter lab --no-browser --port \"$PORT\"\n",
    "  else\n",
    "    exec jupyter lab --port \"$PORT\"\n",
    "  fi\n",
    "fi\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/sys_capture.sh\n",
    "----------\n",
    "#!/usr/bin/env bash\n",
    "# sys_capture.sh\n",
    "# Capture env + system + git state for a run.\n",
    "#\n",
    "# Defaults target a project-level logs dir:\n",
    "#   $PROJECT_ROOT/logs/system_<stamp>/\n",
    "# You can override to a tag-specific location with --tag p_01 (then $PROJECT_ROOT/test_project_bash/p_01/logs/...)\n",
    "#\n",
    "# Usage:\n",
    "#   sys_capture.sh [--project ROOT] [--tag TAGDIR_NAME] [--out-root DIR] [--note TEXT]\n",
    "#                  [--no-git] [--no-conda-lock] [--no-pip-freeze] [--json-only]\n",
    "#\n",
    "# Examples:\n",
    "#   sys_capture.sh --project \"$HOME/my_repos_dwb/fhtw-paper-code-prep\"\n",
    "#   sys_capture.sh --project \"$PWD\" --tag p_01 --note \"seed=137 try1\"\n",
    "#   RUN_ID=\"p_01_try_$(date +'%Y-%m-%dT%H%M%S%z')\" sys_capture.sh --project \"$PWD\" --tag p_01\n",
    "#\n",
    "set -Eeuo pipefail\n",
    "\n",
    "PROJECT_ROOT=\"${PROJECT_ROOT:-}\"\n",
    "TAG_NAME=\"\"\n",
    "OUT_ROOT=\"\"\n",
    "NOTE=\"\"\n",
    "DO_GIT=1\n",
    "DO_CONDA_LOCK=1\n",
    "DO_PIP_FREEZE=1\n",
    "JSON_ONLY=0\n",
    "\n",
    "usage() {\n",
    "  sed -n '1,40p' \"$0\" | sed 's/^# \\{0,1\\}//'\n",
    "  exit 0\n",
    "}\n",
    "\n",
    "# --- parse args ---\n",
    "while [[ $# -gt 0 ]]; do\n",
    "  case \"$1\" in\n",
    "    -p|--project) PROJECT_ROOT=\"$2\"; shift 2;;\n",
    "    -t|--tag) TAG_NAME=\"$2\"; shift 2;;\n",
    "    -o|--out-root) OUT_ROOT=\"$2\"; shift 2;;\n",
    "    -n|--note) NOTE=\"$2\"; shift 2;;\n",
    "    --no-git) DO_GIT=0; shift;;\n",
    "    --no-conda-lock) DO_CONDA_LOCK=0; shift;;\n",
    "    --no-pip-freeze) DO_PIP_FREEZE=0; shift;;\n",
    "    --json-only) JSON_ONLY=1; shift;;\n",
    "    -h|--help) usage;;\n",
    "    *) echo \"Unknown option: $1\" >&2; usage;;\n",
    "  esac\n",
    "done\n",
    "\n",
    "# --- resolve project root ---\n",
    "if [[ -z \"${PROJECT_ROOT}\" ]]; then\n",
    "  # auto-detect by looking for repo markers\n",
    "  if [[ -f \"./README.md\" && -d \"./test_project_bash\" ]]; then\n",
    "    PROJECT_ROOT=\"$(pwd)\"\n",
    "  elif [[ -d \"../test_project_bash\" ]]; then\n",
    "    PROJECT_ROOT=\"$(cd .. && pwd)\"\n",
    "  else\n",
    "    PROJECT_ROOT=\"$(pwd)\"  # fallback\n",
    "  fi\n",
    "fi\n",
    "\n",
    "# --- compute base output dir ---\n",
    "STAMP=\"${RUN_ID:-$(date +'%s_%Y-%m-%dT%H%M%S%z')}\"\n",
    "if [[ -n \"${OUT_ROOT}\" ]]; then\n",
    "  BASE_OUT=\"${OUT_ROOT}\"\n",
    "elif [[ -n \"${TAG_NAME}\" ]]; then\n",
    "  BASE_OUT=\"${PROJECT_ROOT}/test_project_bash/${TAG_NAME}/logs\"\n",
    "else\n",
    "  BASE_OUT=\"${PROJECT_ROOT}/logs\"\n",
    "fi\n",
    "OUT=\"${BASE_OUT}/system_${STAMP}\"\n",
    "mkdir -p -- \"$OUT\"\n",
    "\n",
    "echo \"→ Capturing to: $OUT\"\n",
    "\n",
    "# --- 1) Git snapshot ---\n",
    "if (( DO_GIT )); then\n",
    "  if command -v git >/dev/null 2>&1; then\n",
    "    git -C \"$PROJECT_ROOT\" rev-parse HEAD > \"$OUT/git_commit.txt\" 2>/dev/null || true\n",
    "    git -C \"$PROJECT_ROOT\" status --porcelain=v1 > \"$OUT/git_status.txt\" 2>/dev/null || true\n",
    "    git -C \"$PROJECT_ROOT\" diff > \"$OUT/git_diff.patch\" 2>/dev/null || true\n",
    "  fi\n",
    "fi\n",
    "\n",
    "# --- 2) Conda/Python/env ---\n",
    "if command -v conda >/dev/null 2>&1; then\n",
    "  conda info > \"$OUT/conda_info.txt\" 2>/dev/null || true\n",
    "  conda env export --from-history > \"$OUT/environment.yml\" 2>/dev/null || true\n",
    "  if (( DO_CONDA_LOCK )); then\n",
    "    conda list --explicit > \"$OUT/conda-linux-64.lock\" 2>/dev/null || true\n",
    "  fi\n",
    "fi\n",
    "python -V > \"$OUT/python_version.txt\" 2>/dev/null || true\n",
    "if (( DO_PIP_FREEZE )); then\n",
    "  python -m pip freeze > \"$OUT/requirements.txt\" 2>/dev/null || true\n",
    "fi\n",
    "jupyter kernelspec list > \"$OUT/kernelspecs.txt\" 2>/dev/null || true\n",
    "\n",
    "# --- 3) System snapshot ---\n",
    "uname -a > \"$OUT/uname.txt\" 2>/dev/null || true\n",
    "( command -v lsb_release >/dev/null && lsb_release -a ) > \"$OUT/lsb_release.txt\" 2>/dev/null || true\n",
    "( command -v lscpu >/dev/null && lscpu ) > \"$OUT/lscpu.txt\" 2>/dev/null || true\n",
    "( command -v free >/dev/null && free -h ) > \"$OUT/mem.txt\" 2>/dev/null || true\n",
    "( command -v df  >/dev/null && df -hP . ) > \"$OUT/disk.txt\" 2>/dev/null || true\n",
    "\n",
    "# --- 4) GPU snapshot (non-fatal) ---\n",
    "( command -v nvidia-smi >/dev/null && nvidia-smi -q ) > \"$OUT/nvidia_smi.txt\" 2>/dev/null || true\n",
    "\n",
    "# --- 5) Structured JSON summary (Python) ---\n",
    "python - <<'PY' > \"$OUT/system_info.json\" 2>/dev/null || true\n",
    "import json, sys, platform, os, subprocess\n",
    "def cmd(args):\n",
    "    try: return subprocess.check_output(args, text=True).strip()\n",
    "    except Exception: return None\n",
    "info = {\n",
    "  \"python\": cmd([sys.executable, \"--version\"]),\n",
    "  \"platform\": platform.platform(),\n",
    "  \"uname\": dict(zip((\"system\",\"node\",\"release\",\"version\",\"machine\",\"processor\"),\n",
    "                    platform.uname())),\n",
    "  \"conda_prefix\": os.environ.get(\"CONDA_PREFIX\"),\n",
    "  \"git_commit\": cmd([\"git\",\"rev-parse\",\"HEAD\"]),\n",
    "}\n",
    "try:\n",
    "    import tensorflow as tf; info[\"tensorflow\"] = tf.__version__\n",
    "except Exception: pass\n",
    "try:\n",
    "    import numpy as np; info[\"numpy\"] = np.__version__\n",
    "except Exception: pass\n",
    "print(json.dumps(info, indent=2))\n",
    "PY\n",
    "\n",
    "# --- 6) Optional human note ---\n",
    "if [[ -n \"${NOTE}\" ]]; then\n",
    "  printf \"%s\\n\" \"$NOTE\" > \"$OUT/note.txt\"\n",
    "fi\n",
    "\n",
    "# --- 7) Minimal mode: keep only JSON if requested ---\n",
    "if (( JSON_ONLY )); then\n",
    "  find \"$OUT\" -type f ! -name 'system_info.json' -delete\n",
    "fi\n",
    "\n",
    "echo \"✓ Capture complete: $OUT\"\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/bin/system_info_bash_fallbacks.sh\n",
    "----------\n",
    "# Env / package managers\n",
    "conda info > conda_info.txt\n",
    "conda list > conda_list.txt\n",
    "conda list --explicit > conda-linux-64.lock  # exact builds\n",
    "command -v pip >/dev/null && pip freeze > requirements.txt\n",
    "\n",
    "# OS + kernel\n",
    "uname -a > uname.txt\n",
    "( command -v lsb_release >/dev/null && lsb_release -a ) > lsb_release.txt 2>/dev/null || \\\n",
    "  cat /etc/os-release > os_release.txt\n",
    "\n",
    "# CPU / memory / disk\n",
    "lscpu > lscpu.txt\n",
    "nproc > nproc.txt\n",
    "free -h > free_h.txt\n",
    "df -h . > df_h_here.txt\n",
    "\n",
    "# Python / Jupyter\n",
    "which python > which_python.txt\n",
    "python -V > python_version.txt\n",
    "command -v jupyter >/dev/null && jupyter --version > jupyter_version.txt\n",
    "command -v jupyter >/dev/null && jupyter kernelspec list > kernelspecs.txt\n",
    "\n",
    "# Locale (helps with encoding quirks)\n",
    "locale > locale.txt\n",
    "\n",
    "# TF / Keras relevant env knobs (if any are set)\n",
    "( printenv | grep -E '^(KERAS_HOME|TF_CPP_MIN_LOG_LEVEL|OMP_NUM_THREADS|TF_NUM_(INTRA|INTER)OP_THREADS)=' ) \\\n",
    "  > tf_env_knobs.txt\n",
    "\n",
    "# GPU (only if present)\n",
    "command -v nvidia-smi >/dev/null && nvidia-smi -L > nvidia_gpus.txt || true\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/structure.sh\n",
    "----------\n",
    "#!/usr/bin/env bash\n",
    "# Usage:                 ./structure.sh <ROOT_DIR> <tag1> [tag2 ...]\n",
    "#   OR\n",
    "#        WITH_NB_STUBS=1 ./structure.sh <ROOT_DIR> <tag1> [tag2 ...]\n",
    "#\n",
    "# Creates the structure shown in your example, including:\n",
    "# - README_<tag>.md\n",
    "# - notebooks/*_<tag>.ipynb\n",
    "#   - (Note that these are created idempotently, i.e. the are created\n",
    "#      only if they do not already exist. If  WITH_NB_STUBS=1  is\n",
    "#      included, minimal IPYNB files, i.e. those with enough JSON\n",
    "#      content to be recognized as IPYNB files, will be created,\n",
    "#      once again idempotently)\n",
    "# - scripts/{py_build_model,py_train_model,py_inference,py_utils}_<tag>.py\n",
    "# - scripts/{build_model,train_model,inference}_<tag>.cmd\n",
    "# - scripts/py_touch.py        # untagged (per tag directory)\n",
    "# - datasets/, models/, logs/, visualizations/, \n",
    "#   outputs/{csv_logs,gradcam_images}\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# As far as the backup subcommand, some examples follow\n",
    "#\n",
    "# # Back up a single tag (tight)\n",
    "# ./structure.sh backup \"$HOME/my_repos_dwb/fhtw-paper-code-prep\" test_project_bash/p_01\n",
    "#\n",
    "# # Expanded + apply firstline fixes (after audit)\n",
    "# ./structure.sh backup \"$HOME/my_repos_dwb/fhtw-paper-code-prep\" test_project_bash/p_01 --expanded --fix-firstline\n",
    "#\n",
    "# # Multiple tags, expanded, no system snapshot\n",
    "# ./structure.sh backup \"$HOME/my_repos_dwb/fhtw-paper-code-prep\" test_project_bash/p_01 test_project_bash/p_03_e2e --expanded --skip-sys\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# ---- usage/help (robust, multi-line) ----\n",
    "usage() {\n",
    "  cat <<EOF\n",
    "Usage:\n",
    "  $0 <ROOT_DIR> <tag1> [tag2 ...]\n",
    "\n",
    "Optionally:\n",
    "  WITH_NB_STUBS=1 $0 <ROOT_DIR> <tag1> [tag2 ...]\n",
    "\n",
    "Creates the scaffold; if WITH_NB_STUBS=1, also creates minimal .ipynb stubs.\n",
    "Idempotent (won’t clobber existing files).\n",
    "\n",
    "Subcommands:\n",
    "  $0 backup <ROOT_DIR> <tag1> [tag2 ...] [--expanded] [--fix-firstline] [--skip-sys] [--skip-nbs]\n",
    "    Runs bin/backup_everything.sh for each tag. Leaves scaffolding untouched.\n",
    "EOF\n",
    "}\n",
    "\n",
    "# ---- backup subcommand passthrough ----\n",
    "do_backup_subcommand() {\n",
    "  # Usage: structure.sh backup <ROOT_DIR> <tag1> [tag2 ...] [flags...]\n",
    "  # Flags are forwarded to bin/backup_everything.sh\n",
    "  local _args=(\"$@\")\n",
    "  if (( ${#_args[@]} < 2 )); then\n",
    "    echo \"Usage: $0 backup <ROOT_DIR> <tag1> [tag2 ...] [--expanded] [--fix-firstline] [--skip-sys] [--skip-nbs]\" >&2\n",
    "    exit 2\n",
    "  fi\n",
    "\n",
    "  local _root=\"${_args[0]}\"; shift || true\n",
    "  # collect tags until a flag (starts with --)\n",
    "  local _tags=()\n",
    "  while (( $# )) && [[ \"${1}\" != --* ]]; do\n",
    "    _tags+=(\"$1\"); shift\n",
    "  done\n",
    "  local _flags=(\"$@\")\n",
    "\n",
    "  # Ensure backup script exists\n",
    "  local _bkpsh=\"$_root/bin/backup_everything.sh\"\n",
    "  if [[ ! -x \"$_bkpsh\" ]]; then\n",
    "    echo \"[ERR] Not found or not executable: $_bkpsh\" >&2\n",
    "    echo \"      Please place backup_everything.sh in \\$_ROOT/bin and chmod +x.\" >&2\n",
    "    exit 1\n",
    "  fi\n",
    "\n",
    "  # Run for each tag\n",
    "  for t in \"${_tags[@]}\"; do\n",
    "    echo \"==> Backing up tag: $t\"\n",
    "    \"$_bkpsh\" \"$_root\" \"$t\" \"${_flags[@]}\"\n",
    "    echo\n",
    "  done\n",
    "  exit 0\n",
    "}\n",
    "\n",
    "#old#ROOT_DIR=\"${1:?Usage:\"\\\n",
    "#old#\"                                  $0 <ROOT_DIR> <tag1> [tag2 ...]\\n\"\\\n",
    "#old#\" (optionally) run WITH_NB_STUBS=1 $0 <ROOT_DIR> <tag1> [tag2 ...]\\n\"\\\n",
    "#old#\" The second creates minimal IPYNB files. Both create IPYNB idempotently i.e.\\n\"\\\n",
    "#old#\" only if they do not exist.}\"\n",
    "\n",
    "if (( $# < 2 )); then usage >&2; exit 1; fi\n",
    "\n",
    "# Subcommand: backup\n",
    "if [[ \"${1:-}\" == \"backup\" ]]; then\n",
    "  shift\n",
    "  do_backup_subcommand \"$@\"\n",
    "fi\n",
    "\n",
    "# ---- args/derived ----\n",
    "ROOT_DIR=$1; shift\n",
    "if [ $# -lt 1 ]; then\n",
    "  echo \"Need at least one tag (e.g., p_01 p_02)\" >&2\n",
    "  exit 2\n",
    "fi\n",
    "TAGS=(\"$@\")\n",
    "\n",
    "#  Untagged/common base (defaults to ROOT_DIR, can be overridden by env\n",
    ": \"${UNTAGGED_COMMON:=\"$ROOT_DIR\"}\"\n",
    "\n",
    "# Common per-tag untagged helper\n",
    "UNTAGGED_COMMON_FILES=(\n",
    "  \"scripts/py_touch.py\"\n",
    "  \"scripts/normalize_eol.py\"\n",
    "  #\".gitattributes\"\n",
    ")\n",
    "\n",
    "#  Put EOL entries in for platform-specific files,\n",
    "#+  thus ensuring platform independence\n",
    "ensure_gitattributes_entries() {\n",
    "  local base=\"${1:-$ROOT_DIR}\"\n",
    "  local repo_root\n",
    "  repo_root=$(git -C \"$base\" rev-parse --show-toplevel 2>/dev/null || echo \"$base\")\n",
    "\n",
    "  local f=\"$repo_root/.gitattributes\"\n",
    "  local -a lines=(\n",
    "    \"*.sh              text eol=lf\"\n",
    "    \"*.ps1             text eol=crlf\"\n",
    "    \"*.cmd             text eol=crlf\"\n",
    "    \"*.py              text eol=lf\"\n",
    "    \"*.md              text eol=lf\"\n",
    "    \"*.ipynb           text eol=lf\"\n",
    "    \".gitattributes    text eol=lf\"\n",
    "  )\n",
    "\n",
    "  mkdir -p \"$repo_root\"\n",
    "  touch \"$f\"\n",
    "  chmod 0644 \"$f\" 2>/dev/null || true\n",
    "  # ensure file ends with newline\n",
    "  tail -c1 \"$f\" 2>/dev/null | grep -q $'\\n' || echo >> \"$f\"\n",
    "\n",
    "  for line in \"${lines[@]}\"; do\n",
    "    grep -Fqx -- \"$line\" \"$f\" || echo \"$line\" >> \"$f\"\n",
    "  done\n",
    "  echo \"[OK] ensured .gitattributes entries at $f\"\n",
    "}\n",
    "\n",
    "mkd() { mkdir -p -- \"$1\"; }\n",
    "touch_safe() {\n",
    "  local path=\"$1\"\n",
    "  mkd \"$(dirname -- \"$path\")\"\n",
    "  [ -f \"$path\" ] || : > \"$path\"\n",
    "}\n",
    "\n",
    "#@TODO : Use getopt to pass in the WITH_NB_STUBS option as well as other opts\n",
    "#\n",
    "#  Optional: export WITH_NB_STUBS=1 to write minimal *.ipynb files\n",
    "WITH_NB_STUBS=\"${WITH_NB_STUBS:-0}\"\n",
    "emit_ipynb() {\n",
    "  local path=\"$1\"\n",
    "  python - <<'PY' \"$path\"\n",
    "import json, sys, os\n",
    "nb = {\n",
    "  \"cells\":[{\"cell_type\":\"code\",\"execution_count\":None,\"metadata\":{},\"outputs\":[],\n",
    "    \"source\":[\n",
    "      \"import os, random, numpy as np, tensorflow as tf\\n\",\n",
    "      \"os.environ[\\\"PYTHONHASHSEED\\\"]=\\\"137\\\";\\n\",\n",
    "      \"random.seed(137);\\n\",\n",
    "      \"np.random.seed(137);\\n\",\n",
    "      \"tf.random.set_seed(137)\\n\"\n",
    "    ]}],\n",
    "  \"metadata\":{\n",
    "    \"kernelspec\":{\"display_name\":\"Python 3 (ipykernel)\",\"language\":\"python\",\"name\":\"python3\"},\n",
    "    \"language_info\":{\"name\":\"python\",\"version\":\"3.10\"}\n",
    "  },\n",
    "  \"nbformat\":4,\"nbformat_minor\":5\n",
    "}\n",
    "open(sys.argv[1], \"w\", encoding=\"utf-8\").write(json.dumps(nb, ensure_ascii=False, indent=1))\n",
    "PY\n",
    "}\n",
    "\n",
    "## Added 1756960018_2025-09-03T222658-0600, just barely\n",
    "if [ ! -d \"$ROOT_DIR\" ]; then\n",
    "  mkd \"$ROOT_DIR\"\n",
    "fi\n",
    "\n",
    "ensure_gitattributes_entries \"$ROOT_DIR\"\n",
    "\n",
    "echo \"  have this in here.\"\n",
    "echo \"  (Note that any files that should be excluded/ignored\"\n",
    "echo \"   are covered in the Git repo's root .gitignore file.)\"\n",
    "echo\n",
    "\n",
    "# Per-tag file stems (relative to the tag root)\n",
    "README_STEM=\"README.md\"\n",
    "\n",
    "#  Per-tag enabling import of tag-dir  and its script dir as packages\n",
    "#+ tag is NOT part of filename\n",
    "PACKAGE_FILES=(\n",
    "  \"__init__.py\"\n",
    "  \"scripts/__init__.py\"\n",
    ")\n",
    "\n",
    "# Notebooks (placed under notebooks/, with tag suffix)\n",
    "NB_FILES=(\n",
    "  \"notebooks/00_data_exploration.ipynb\"\n",
    "  \"notebooks/01_model_build.ipynb\"\n",
    "  \"notebooks/02_training.ipynb\"\n",
    "  \"notebooks/03_inference_quick_explore.ipynb\"\n",
    ")\n",
    "\n",
    "#  Scripts (tag-suffixed python placeholders)\n",
    "#+ These could perform the same thing as the\n",
    "#+ *.sh versions and the *.ps1 versions. \n",
    "#+ We'll probably only get to one of them\n",
    "#+ before the Fragmentology publication.\n",
    "PY_FILES=(\n",
    "  \"scripts/py_build_model.py\"\n",
    "  \"scripts/py_train_model.py\"\n",
    "  \"scripts/py_inference.py\"\n",
    "  \"scripts/py_utils.py\"\n",
    ")\n",
    "\n",
    "#  Scripts (tag-suffixed SH placeholders)\n",
    "#+ These could perform the same thing as the\n",
    "#+ py_*.py versions and the *.ps1 versions. \n",
    "#+ We'll probably only get to one of them\n",
    "#+ before the Fragmentology publication.\n",
    "THE_SH_FILES=(\n",
    "  \"scripts/build_model.sh\"\n",
    "  \"scripts/train_model.sh\"\n",
    "  \"scripts/inference.sh\"\n",
    "  \"scripts/py_utils.sh\"\n",
    ")\n",
    "\n",
    "#  Scripts (tag-suffixed SH placeholders)\n",
    "#+ These could perform the same thing as the\n",
    "#+ py_*.py versions and the *.ps1 versions. \n",
    "#+ We'll probably only get to one of them\n",
    "#+ before the Fragmentology publication.\n",
    "THE_PS_FILES=(\n",
    "  \"scripts/build_model.ps1\"\n",
    "  \"scripts/train_model.ps1\"\n",
    "  \"scripts/inference.ps1\"\n",
    ")\n",
    "\n",
    "#  Scripts (tag-suffixed CMD placeholders)\n",
    "#+ We probably won't ever end up touching\n",
    "#+ these, but they're here for completeness.\n",
    "CMD_FILES=(\n",
    "  \"scripts/build_model.cmd\"\n",
    "  \"scripts/train_model.cmd\"\n",
    "  \"scripts/inference.cmd\"\n",
    ")\n",
    "\n",
    "# Fixed directory set under each tag\n",
    "DIRS=(\n",
    "  \"notebooks\"\n",
    "  \"datasets\"\n",
    "  \"models\"\n",
    "  \"logs\"\n",
    "  \"visualizations\"\n",
    "  \"scripts\"\n",
    "  \"outputs/csv_logs\"\n",
    "  \"outputs/gradcam_images\"\n",
    ")\n",
    "\n",
    "for tag in \"$@\"; do\n",
    "  TAG_DIR=\"${ROOT_DIR%/}/$tag\"\n",
    "\n",
    "  # Directories\n",
    "  for d in \"${DIRS[@]}\"; do\n",
    "    mkd \"$TAG_DIR/$d\"\n",
    "  done\n",
    "\n",
    "  # README_<tag>.md at tag root\n",
    "  touch_safe \"$TAG_DIR/${README_STEM%.md}_${tag}.md\"\n",
    "  \n",
    "  #  __init__.py files needed for import (create proper packages out of dirs)\n",
    "  #+ tag is NOT part of filename\n",
    "  for f in \"${PACKAGE_FILES[@]}\"; do\n",
    "    touch_safe \"$TAG_DIR/$f\"\n",
    "    # Info on the __init__.py helper\n",
    "    echo \"  ---------------------------------------------------------\"\n",
    "    echo \"  First OS-agnostic Python helper, __init__.py\"\n",
    "    echo \"  provided for tag, ``tag' at\"\n",
    "    echo \"  '$TAG_DIR/$f'\"\n",
    "    echo \"  to allow the directory name to serve as a package name,\"\n",
    "    echo \"  simplifying imports.\"\n",
    "    echo\n",
    "  done\n",
    "\n",
    "  # Notebooks with _<tag>.ipynb inside notebooks/\n",
    "  for f in \"${NB_FILES[@]}\"; do\n",
    "    base=\"$(basename -- \"$f\")\"\n",
    "    stem=\"${base%.*}\"\n",
    "    ext=\"${base##*.}\"\n",
    "    dst=\"$TAG_DIR/notebooks/${stem}_${tag}.${ext}\"\n",
    "    if [ \"$WITH_NB_STUBS\" = \"1\" ]; then\n",
    "      [ -f \"$dst\" ] || emit_ipynb \"$dst\"\n",
    "    else\n",
    "      touch_safe \"$dst\"\n",
    "    fi\n",
    "  done\n",
    "\n",
    "  # Python scripts with _<tag>.py\n",
    "  for f in \"${PY_FILES[@]}\"; do\n",
    "    base=\"$(basename -- \"$f\")\"\n",
    "    stem=\"${base%.*}\"\n",
    "    ext=\"${base##*.}\"\n",
    "    touch_safe \"$TAG_DIR/scripts/${stem}_${tag}.${ext}\"\n",
    "  done\n",
    "\n",
    "  # CMD placeholders with _<tag>.cmd\n",
    "  for f in \"${CMD_FILES[@]}\"; do\n",
    "    base=\"$(basename -- \"$f\")\"\n",
    "    stem=\"${base%.*}\"\n",
    "    ext=\"${base##*.}\"\n",
    "    touch_safe \"$TAG_DIR/scripts/${stem}_${tag}.${ext}\"\n",
    "  done\n",
    "\n",
    "  # PowerShell placeholders with _<tag>.ps1\n",
    "  for f in \"${THE_PS_FILES[@]}\"; do\n",
    "    base=\"$(basename -- \"$f\")\"\n",
    "    stem=\"${base%.*}\"\n",
    "    ext=\"${base##*.}\"\n",
    "    touch_safe \"$TAG_DIR/scripts/${stem}_${tag}.${ext}\"\n",
    "  done\n",
    "  \n",
    "  # SH placeholders with _<tag>.cmd\n",
    "  for f in \"${THE_SH_FILES[@]}\"; do\n",
    "    base=\"$(basename -- \"$f\")\"\n",
    "    stem=\"${base%.*}\"\n",
    "    ext=\"${base##*.}\"\n",
    "    touch_safe \"$TAG_DIR/scripts/${stem}_${tag}.${ext}\"\n",
    "  done\n",
    "  \n",
    "  # Untagged common helpers\n",
    "  for f in \"${UNTAGGED_COMMON_FILES}\"; do\n",
    "    touch_safe \"$TAG_DIR/$f\"\n",
    "  done\n",
    "  \n",
    "  \n",
    "  ##-----------------------------------------------------------------\n",
    "  ##  I do include a couple of per-experiment helpers, written \n",
    "  ##+ in Python. The first is in case some kind of `touch' \n",
    "  ##+ functionality be desired that's consistent between Windows\n",
    "  ##+ and Linux (*NIX). The second is an End Of Line Normalizer,\n",
    "  ##+ a helper-function-version of programs like dos2unix / unix2dos\n",
    "  ##+ This allows cross-platform creation of an experimental\n",
    "  ##+ directory very quickly.\n",
    "  \n",
    "  ##  The idea of a `touch'-like function for PowerShell was\n",
    "  ##+ abandoned. I _do_ include a per-experiment helper, written\n",
    "  ##+ in Python, the \"first\" explained above.\n",
    "  \n",
    "  py_touch_path=\"$TAG_DIR/scripts/py_touch.py\"\n",
    "  if [ ! -f \"$py_touch_path\" ]; then\n",
    "    cat << 'EOF' > \"$py_touch_path\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "for f in sys.argv[1:]: Path(f).touch(exist_ok=True)\n",
    "EOF\n",
    "\n",
    "  fi\n",
    "  \n",
    "  # Info on the py_touch helper\n",
    "  echo \"  ---------------------------------------------------------\"\n",
    "  echo \"  OS-agnostic helper,\"\n",
    "  echo \"  $py_touch_path\"\n",
    "  echo \"  provided for tag, ``$tag', in case it be desired.\"\n",
    "  echo \"  This could prove immensely helpful for Windows users.\"\n",
    "  echo \"  On *NIX, prefer touch(1) when available; this is a fallback.\"\n",
    "  echo\n",
    "  \n",
    "  # Create (untagged-common) normalize_eol.py if missing\n",
    "  norm_eol_path=\"$TAG_DIR/scripts/normalize_eol.py\"\n",
    "  if [ ! -f \"$norm_eol_path\" ]; then\n",
    "    cat << 'EONormF' > \"$norm_eol_path\"\n",
    "\"\"\"\n",
    "Normalize line endings (EOL) for text files.\n",
    "\n",
    "Usage modes:\n",
    "  A) By extension map (recommended)\n",
    "     python normalize_eol.py --root <DIR> --map sh=lf,ps1=crlf,cmd=crlf,py=lf,ipynb=lf,md=lf\n",
    "\n",
    "  B) Explicit mode on listed files\n",
    "     python normalize_eol.py --to-lf  file1 file2 ...\n",
    "     python normalize_eol.py --to-crlf file1 file2 ...\n",
    "\n",
    "Notes:\n",
    "  - Skips binaries by a simple heuristic (NULL byte check).\n",
    "  - Only rewrites when a change is needed.\n",
    "\"\"\"\n",
    "\n",
    "import argparse, os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_binary(data: bytes) -> bool:\n",
    "  \"\"\"\n",
    "  Heuristic: treat as binary if there is a NUL byte.\n",
    "  \"\"\"\n",
    "  return b\"\\x00\" in data\n",
    "\n",
    "def normalize_bytes(data: bytes, mode: str) -> bytes:\n",
    "  \"\"\"\n",
    "  Convert EOLs:\n",
    "    mode=''lf''   -> \\n\n",
    "    mode=''crlf'' -> \\r\\n\n",
    "  \"\"\"\n",
    "  # First unify to LF\n",
    "  text = data.replace(b\"\\r\\n\", b\"\\n\").replace(b\"\\r\", b\"\\n\")\n",
    "  if mode == \"lf\":\n",
    "    return text\n",
    "  elif mode == \"crlf\":\n",
    "    return text.replace(b\"\\n\", b\"\\r\\n\")\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "def normalize_file(path: Path, mode: str) -> bool:\n",
    "  \"\"\"\n",
    "  Normalize a single file in-place. Returns True if modified.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    raw = path.read_bytes()\n",
    "  except Exception:\n",
    "    return False\n",
    "  if is_binary(raw):\n",
    "    return False\n",
    "  new = normalize_bytes(raw, mode)\n",
    "  if new != raw:\n",
    "    path.write_bytes(new)\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def parse_map(map_str: str) -> dict:\n",
    "  \"\"\"\n",
    "  Parse ''ext=mode,ext=mode'' into dict like {''.sh'':''lf'', ''.ps1'':''crlf''}\n",
    "  \"\"\"\n",
    "  out = {}\n",
    "  for part in map_str.split(\",\"):\n",
    "    part = part.strip()\n",
    "    if not part:\n",
    "      continue\n",
    "    k, v = part.split(\"=\")\n",
    "    ext = k.strip().lower()\n",
    "    if not ext.startswith(\".\"):\n",
    "      ext = \".\" + ext\n",
    "    out[ext] = v.strip().lower()\n",
    "  return out\n",
    "\n",
    "def normalize_by_map(root: Path, extmap: dict) -> int:\n",
    "  \"\"\"\n",
    "  Walk root and apply per-extension modes. Returns count of modified files.\n",
    "  \"\"\"\n",
    "  n = 0\n",
    "  for p in root.rglob(\"*\"):\n",
    "    if not p.is_file():\n",
    "      continue\n",
    "    mode = extmap.get(p.suffix.lower())\n",
    "    if not mode:\n",
    "      continue\n",
    "    if normalize_file(p, mode):\n",
    "      n += 1\n",
    "  return n\n",
    "\n",
    "def main(argv=None):\n",
    "  \"\"\"\n",
    "  CLI entry point.\n",
    "  \"\"\"\n",
    "  ap = argparse.ArgumentParser()\n",
    "  g = ap.add_mutually_exclusive_group()\n",
    "  g.add_argument(\"--to-lf\",   action=\"store_true\", help=\"Force LF on listed files\")\n",
    "  g.add_argument(\"--to-crlf\", action=\"store_true\", help=\"Force CRLF on listed files\")\n",
    "  ap.add_argument(\"--root\", type=Path, help=\"Directory to normalize recursively\")\n",
    "  ap.add_argument(\"--map\",  type=str, help=\"Extension map like ''sh=lf,ps1=crlf''\")\n",
    "  ap.add_argument(\"files\", nargs=\"*\", type=Path, help=\"Files to normalize (with --to-*)\")\n",
    "  args = ap.parse_args(argv)\n",
    "\n",
    "  # Mode B: explicit files\n",
    "  if args.to_lf or args.to_crlf:\n",
    "    mode = \"lf\" if args.to_lf else \"crlf\"\n",
    "    changed = 0\n",
    "    for f in args.files:\n",
    "      if normalize_file(f, mode):\n",
    "        changed += 1\n",
    "    print(f\"Changed {changed} files.\")\n",
    "    return 0\n",
    "\n",
    "  # Mode A: by-extension map under --root\n",
    "  if args.root and args.map:\n",
    "    extmap = parse_map(args.map)\n",
    "    changed = normalize_by_map(args.root, extmap)\n",
    "    print(f\"Changed {changed} files under {args.root}.\")\n",
    "    return 0\n",
    "\n",
    "  ap.error(\"Provide either (--to-lf|--to-crlf files...) or --root DIR --map ext=mode,...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \"\"\"\n",
    "  Gets called if the module is called from command prompt, via\n",
    "    e.g.\n",
    "      > python normalize_eof.py <argument>\n",
    "    OR\n",
    "      $ python normalize_eof.py <argument>\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  #  Note that this next call combines calling main, the exiting with\n",
    "  #+ its return value. I think it's even clearer than assigning the\n",
    "  #+ return value of main to, say, retval, and then returning retval.\n",
    "  \n",
    "  sys.exit(main())\n",
    "\n",
    "EONormF\n",
    "\n",
    "  fi\n",
    "  \n",
    "  # Info on the normalize_eof helper\n",
    "  echo \"  ---------------------------------------------------------\"\n",
    "  echo \"  OS-agnostic helper,\"\n",
    "  echo \"  $norm_eol_path\"\n",
    "  echo \"  provided for tag, '$tag'. IT WILL BE NECESSARY TO USE IT!\"\n",
    "  echo \"  DO NOT MISS reading the pre-running instructions, in the $tag\"\n",
    "  echo \"  README at Section: REQUIRED Pre-running Instructions, to allow\"\n",
    "  echo \"  allow the files to run from your repo, whether on your local\"\n",
    "  echo \"  machine or on any online/VPN/other machine.\"\n",
    "  echo \"  THIS IS NECESSARY WHETHER ON WINDOWS OR *NIX!\"\n",
    "  echo \"  DO NOT PROCEED FURTHER WITHOUT FOLLOWING THOSE INSTRUCTIONS!\"\n",
    "  echo\n",
    "  echo \"  From bash, usage should be\"\n",
    "  echo \"    \\$ python \\\"\\$TAG_DIR/scripts/normalize_eol.py \\\\\\\"\"\n",
    "  echo \"         --root \\\"\\$TAG_DIR\\\" \\\\\\\"\"\n",
    "  echo \"         --map 'sh=lf,ps1=crlf,cmd=crlf,py=lf,ipynb=lf,md=lf' \"\n",
    "  echo\n",
    "  echo \"  From PowerShell, usage should be\"\n",
    "  echo \"    PS> & python \\\"<path-to-tagdir>\\\\scripts\\\\normalize_eol.py\\\" --root <path-to-tagdir> --map \\\"sh=lf,ps1=crlf,cmd=crlf,py=lf,ipynb=lf,md=lf\\\"\"\n",
    "  echo \"  where <path-to-tagdir> is analogous to\"\n",
    "  echo \"  \\\"$TAG_DIR\\\" in the Windows platform setup.\"\n",
    "  echo \n",
    "  \n",
    "\n",
    "  ##  .gitattributes content now added with ensure_gitattributes_entries\n",
    "  ##+ function, where it is created/appended at the root of the git repo\n",
    "  ##+ (if there is a git repo)\n",
    "#b4#  # Create (untagged-common) .gitattributes (or append if already exists)\n",
    "#b4#  gitattr_path=\"$TAG_DIR/.gitattributes\"\n",
    "#b4#  if [ ! -f \"$gitattr_path\" ]; then\n",
    "#b4#    cat << 'EOGitAttrF' >> \"$gitattr_path\"\n",
    "#b4#\n",
    "#b4#    #  .gitattributes addition (or creation) for ease in using\n",
    "#b4#    #+ platform-specific files (making it platform-agnostic)\n",
    "#b4#*.sh              text eol=lf\n",
    "#b4#*.ps1             text eol=crlf\n",
    "#b4#*.cmd             text eol=crlf\n",
    "#b4#*.py              text eol=lf\n",
    "#b4#*.md              text eol=lf\n",
    "#b4#*.ipynb           text eol=lf\n",
    "#b4#.gitattributes    text eol=lf\n",
    "#b4#\n",
    "#b4#EOGitAttrF\n",
    "#b4#\n",
    "#b4#    # Info on the .gitattributes additions (creation)\n",
    "#b4#    echo \"  ---------------------------------------------------------\"\n",
    "#b4#    echo \"  Helper to ensure correct EOL on OS-specific files,\"\n",
    "#b4#    echo \"  $gitattr_path\"\n",
    "#b4#    echo \"  provided for tag, '$tag'. This will allow everying\"\n",
    "#b4#    echo \"  to be stress-free platform-agnostic. Only used\"\n",
    "#b4#    echo \"  when things are done in a project where source\"\n",
    "#b4#    echo \"  control is handled via Git, but it doesn't hurt to\"\n",
    "#b4#    echo \"  have this in here.\"\n",
    "#b4#    echo \"  (Note that any files that should be excluded/ignored\"\n",
    "#b4#    echo \"   are covered in the Git repo's root .gitignore file.)\"\n",
    "#b4#    echo\n",
    "#b4#  \n",
    "#b4#  fi\n",
    "\n",
    "done\n",
    "\n",
    "echo \"--------------------------------------------------------------------\"\n",
    "echo \"Project scaffolding with tags for files and tag-named subdirectories created at $ROOT_DIR\"\n",
    "echo \"--------------------------------------------------------------------\"\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/build_model_p_01.sh\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/inference_p_01.sh\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/train_model_p_01.sh\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/build_model_p_03_e2e.sh\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/inference_p_03_e2e.sh\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_utils_p_03_e2e.sh\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/train_model_p_03_e2e.sh\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15120f8a-bd6e-45b1-bdf2-2bd5dee91522",
   "metadata": {},
   "source": [
    "### Python scripts\n",
    "\n",
    "```python\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ find . -type f -iname \"*.py\" | sort | tr '\\n' '\\0' | xargs -I{} -0 bash -c 'orig={}; sixtydash=\"--------------------------------------------------\"; tendash=\"----------\"; echo \"${sixtydash}\"; echo \"Script at:\"; echo \"$(realpath {})\"; echo \"${tendash}\"; cat {}; echo \"${sixtydash}\"; echo;'\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/general_lab_notebooks_-_other_examples/notebook_cache_fallback_for_jupyter_cell.py\n",
    "----------\n",
    "\n",
    "# Notebook cache fallback (paste at very top)\n",
    "If your notebook couldn’t see the fallback earlier, here’s the snippet again. Pop this in **before** importing TensorFlow:\n",
    "\n",
    "```python\n",
    "# --- CIFAR-10 cache + quieter TF ---\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"TF_NUM_INTEROP_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"TF_NUM_INTRAOP_THREADS\", \"4\")\n",
    "\n",
    "tagdir = Path(os.environ.get(\"TAGDIR\", Path.cwd()))\n",
    "if tagdir.name == \"notebooks\":\n",
    "  tagdir = tagdir.parent\n",
    "os.environ.setdefault(\"KERAS_HOME\", str(tagdir))  # -> $TAGDIR/datasets\n",
    "proj_cache = Path(os.environ[\"KERAS_HOME\"]) / \"datasets\"\n",
    "proj_cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "home_cache = Path.home() / \".keras\" / \"datasets\"\n",
    "candidates = [(\"cifar-10-python.tar.gz\", True), (\"cifar-10-batches-py\", False)]\n",
    "if home_cache.exists() and not any((proj_cache / name).exists() for name, _ in candidates):\n",
    "  for name, is_file in candidates:\n",
    "      src = home_cache / name\n",
    "      if src.exists():\n",
    "          dst = proj_cache / src.name\n",
    "          (shutil.copy2 if is_file else shutil.copytree)(src, dst, dirs_exist_ok=True)\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/__init__.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/__init__.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/normalize_eol.py\n",
    "----------\n",
    "\"\"\"\n",
    "Normalize line endings (EOL) for text files.\n",
    "\n",
    "Usage modes:\n",
    "  A) By extension map (recommended)\n",
    "     python normalize_eol.py --root <DIR> --map sh=lf,ps1=crlf,cmd=crlf,py=lf,ipynb=lf,md=lf\n",
    "\n",
    "  B) Explicit mode on listed files\n",
    "     python normalize_eol.py --to-lf  file1 file2 ...\n",
    "     python normalize_eol.py --to-crlf file1 file2 ...\n",
    "\n",
    "Notes:\n",
    "  - Skips binaries by a simple heuristic (NULL byte check).\n",
    "  - Only rewrites when a change is needed.\n",
    "\"\"\"\n",
    "\n",
    "import argparse, os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_binary(data: bytes) -> bool:\n",
    "  \"\"\"\n",
    "  Heuristic: treat as binary if there is a NUL byte.\n",
    "  \"\"\"\n",
    "  return b\"\\x00\" in data\n",
    "\n",
    "def normalize_bytes(data: bytes, mode: str) -> bytes:\n",
    "  \"\"\"\n",
    "  Convert EOLs:\n",
    "    mode=''lf''   -> \\n\n",
    "    mode=''crlf'' -> \\r\\n\n",
    "  \"\"\"\n",
    "  # First unify to LF\n",
    "  text = data.replace(b\"\\r\\n\", b\"\\n\").replace(b\"\\r\", b\"\\n\")\n",
    "  if mode == \"lf\":\n",
    "    return text\n",
    "  elif mode == \"crlf\":\n",
    "    return text.replace(b\"\\n\", b\"\\r\\n\")\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "def normalize_file(path: Path, mode: str) -> bool:\n",
    "  \"\"\"\n",
    "  Normalize a single file in-place. Returns True if modified.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    raw = path.read_bytes()\n",
    "  except Exception:\n",
    "    return False\n",
    "  if is_binary(raw):\n",
    "    return False\n",
    "  new = normalize_bytes(raw, mode)\n",
    "  if new != raw:\n",
    "    path.write_bytes(new)\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def parse_map(map_str: str) -> dict:\n",
    "  \"\"\"\n",
    "  Parse ''ext=mode,ext=mode'' into dict like {''.sh'':''lf'', ''.ps1'':''crlf''}\n",
    "  \"\"\"\n",
    "  out = {}\n",
    "  for part in map_str.split(\",\"):\n",
    "    part = part.strip()\n",
    "    if not part:\n",
    "      continue\n",
    "    k, v = part.split(\"=\")\n",
    "    ext = k.strip().lower()\n",
    "    if not ext.startswith(\".\"):\n",
    "      ext = \".\" + ext\n",
    "    out[ext] = v.strip().lower()\n",
    "  return out\n",
    "\n",
    "def normalize_by_map(root: Path, extmap: dict) -> int:\n",
    "  \"\"\"\n",
    "  Walk root and apply per-extension modes. Returns count of modified files.\n",
    "  \"\"\"\n",
    "  n = 0\n",
    "  for p in root.rglob(\"*\"):\n",
    "    if not p.is_file():\n",
    "      continue\n",
    "    mode = extmap.get(p.suffix.lower())\n",
    "    if not mode:\n",
    "      continue\n",
    "    if normalize_file(p, mode):\n",
    "      n += 1\n",
    "  return n\n",
    "\n",
    "def main(argv=None):\n",
    "  \"\"\"\n",
    "  CLI entry point.\n",
    "  \"\"\"\n",
    "  ap = argparse.ArgumentParser()\n",
    "  g = ap.add_mutually_exclusive_group()\n",
    "  g.add_argument(\"--to-lf\",   action=\"store_true\", help=\"Force LF on listed files\")\n",
    "  g.add_argument(\"--to-crlf\", action=\"store_true\", help=\"Force CRLF on listed files\")\n",
    "  ap.add_argument(\"--root\", type=Path, help=\"Directory to normalize recursively\")\n",
    "  ap.add_argument(\"--map\",  type=str, help=\"Extension map like ''sh=lf,ps1=crlf''\")\n",
    "  ap.add_argument(\"files\", nargs=\"*\", type=Path, help=\"Files to normalize (with --to-*)\")\n",
    "  args = ap.parse_args(argv)\n",
    "\n",
    "  # Mode B: explicit files\n",
    "  if args.to_lf or args.to_crlf:\n",
    "    mode = \"lf\" if args.to_lf else \"crlf\"\n",
    "    changed = 0\n",
    "    for f in args.files:\n",
    "      if normalize_file(f, mode):\n",
    "        changed += 1\n",
    "    print(f\"Changed {changed} files.\")\n",
    "    return 0\n",
    "\n",
    "  # Mode A: by-extension map under --root\n",
    "  if args.root and args.map:\n",
    "    extmap = parse_map(args.map)\n",
    "    changed = normalize_by_map(args.root, extmap)\n",
    "    print(f\"Changed {changed} files under {args.root}.\")\n",
    "    return 0\n",
    "\n",
    "  ap.error(\"Provide either (--to-lf|--to-crlf files...) or --root DIR --map ext=mode,...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \"\"\"\n",
    "  Gets called if the module is called from command prompt, via\n",
    "    e.g.\n",
    "      > python normalize_eof.py <argument>\n",
    "    OR\n",
    "      $ python normalize_eof.py <argument>\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  #  Note that this next call combines calling main, the exiting with\n",
    "  #+ its return value. I think it's even clearer than assigning the\n",
    "  #+ return value of main to, say, retval, and then returning retval.\n",
    "  \n",
    "  sys.exit(main())\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/py_build_model_p_01.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/py_inference_p_01.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/py_touch.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/py_train_model_p_01.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/scripts/py_utils_p_01.py\n",
    "----------\n",
    "\"\"\"\n",
    "py_utils_p_01.py\n",
    "\n",
    "Utility functions for experiment logging (Q&R acceptance).\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Default seed (can be overridden via env or caller)\n",
    "SEED = 137\n",
    "\n",
    "def _ts():\n",
    "    \"\"\"Return timestamp for filenames.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "def make_paths(seed: int = SEED, tagdir: str | Path | None = None):\n",
    "    \"\"\"\n",
    "    Build file paths for CSV log and JSON test summary.\n",
    "\n",
    "    Args:\n",
    "        seed: random seed used for run\n",
    "        tagdir: root of project (defaults to CWD or $TAGDIR)\n",
    "\n",
    "    Returns:\n",
    "        (csv_path, json_path)\n",
    "    \"\"\"\n",
    "    tagdir = Path(tagdir or Path.cwd())\n",
    "    outdir = tagdir / \"outputs\"\n",
    "    csv_dir = outdir / \"csv_logs\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    stamp = _ts()\n",
    "    csv_path = csv_dir / f\"train_history_seed{seed}_{stamp}.csv\"\n",
    "    json_path = outdir / f\"test_summary_seed{seed}_{stamp}.json\"\n",
    "    return csv_path, json_path\n",
    "\n",
    "def log_test_summary(test_acc: float,\n",
    "                     loss: float | None = None,\n",
    "                     seed: int = SEED,\n",
    "                     tagdir: str | Path | None = None,\n",
    "                     extra: dict | None = None):\n",
    "    \"\"\"\n",
    "    Write test summary JSON and print the sentinel line.\n",
    "\n",
    "    Args:\n",
    "        test_acc: final test accuracy\n",
    "        loss: optional test loss\n",
    "        seed: random seed\n",
    "        tagdir: project tag root\n",
    "        extra: extra fields to include\n",
    "    \"\"\"\n",
    "    _, json_path = make_paths(seed, tagdir)\n",
    "    payload = {\"seed\": seed, \"test_acc\": float(test_acc)}\n",
    "    if loss is not None:\n",
    "        payload[\"test_loss\"] = float(loss)\n",
    "    if extra:\n",
    "        payload.update(extra)\n",
    "\n",
    "    json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "\n",
    "    print(f\"[DONE] test_acc={payload['test_acc']:.4f}  ->  wrote {json_path}\")\n",
    "    return json_path\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/__init__.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/__init__.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/normalize_eol.py\n",
    "----------\n",
    "\"\"\"\n",
    "Normalize line endings (EOL) for text files.\n",
    "\n",
    "Usage modes:\n",
    "  A) By extension map (recommended)\n",
    "     python normalize_eol.py --root <DIR> --map sh=lf,ps1=crlf,cmd=crlf,py=lf,ipynb=lf,md=lf\n",
    "\n",
    "  B) Explicit mode on listed files\n",
    "     python normalize_eol.py --to-lf  file1 file2 ...\n",
    "     python normalize_eol.py --to-crlf file1 file2 ...\n",
    "\n",
    "Notes:\n",
    "  - Skips binaries by a simple heuristic (NULL byte check).\n",
    "  - Only rewrites when a change is needed.\n",
    "\"\"\"\n",
    "\n",
    "import argparse, os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_binary(data: bytes) -> bool:\n",
    "  \"\"\"\n",
    "  Heuristic: treat as binary if there is a NUL byte.\n",
    "  \"\"\"\n",
    "  return b\"\\x00\" in data\n",
    "\n",
    "def normalize_bytes(data: bytes, mode: str) -> bytes:\n",
    "  \"\"\"\n",
    "  Convert EOLs:\n",
    "    mode=''lf''   -> \\n\n",
    "    mode=''crlf'' -> \\r\\n\n",
    "  \"\"\"\n",
    "  # First unify to LF\n",
    "  text = data.replace(b\"\\r\\n\", b\"\\n\").replace(b\"\\r\", b\"\\n\")\n",
    "  if mode == \"lf\":\n",
    "    return text\n",
    "  elif mode == \"crlf\":\n",
    "    return text.replace(b\"\\n\", b\"\\r\\n\")\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "def normalize_file(path: Path, mode: str) -> bool:\n",
    "  \"\"\"\n",
    "  Normalize a single file in-place. Returns True if modified.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    raw = path.read_bytes()\n",
    "  except Exception:\n",
    "    return False\n",
    "  if is_binary(raw):\n",
    "    return False\n",
    "  new = normalize_bytes(raw, mode)\n",
    "  if new != raw:\n",
    "    path.write_bytes(new)\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def parse_map(map_str: str) -> dict:\n",
    "  \"\"\"\n",
    "  Parse ''ext=mode,ext=mode'' into dict like {''.sh'':''lf'', ''.ps1'':''crlf''}\n",
    "  \"\"\"\n",
    "  out = {}\n",
    "  for part in map_str.split(\",\"):\n",
    "    part = part.strip()\n",
    "    if not part:\n",
    "      continue\n",
    "    k, v = part.split(\"=\")\n",
    "    ext = k.strip().lower()\n",
    "    if not ext.startswith(\".\"):\n",
    "      ext = \".\" + ext\n",
    "    out[ext] = v.strip().lower()\n",
    "  return out\n",
    "\n",
    "def normalize_by_map(root: Path, extmap: dict) -> int:\n",
    "  \"\"\"\n",
    "  Walk root and apply per-extension modes. Returns count of modified files.\n",
    "  \"\"\"\n",
    "  n = 0\n",
    "  for p in root.rglob(\"*\"):\n",
    "    if not p.is_file():\n",
    "      continue\n",
    "    mode = extmap.get(p.suffix.lower())\n",
    "    if not mode:\n",
    "      continue\n",
    "    if normalize_file(p, mode):\n",
    "      n += 1\n",
    "  return n\n",
    "\n",
    "def main(argv=None):\n",
    "  \"\"\"\n",
    "  CLI entry point.\n",
    "  \"\"\"\n",
    "  ap = argparse.ArgumentParser()\n",
    "  g = ap.add_mutually_exclusive_group()\n",
    "  g.add_argument(\"--to-lf\",   action=\"store_true\", help=\"Force LF on listed files\")\n",
    "  g.add_argument(\"--to-crlf\", action=\"store_true\", help=\"Force CRLF on listed files\")\n",
    "  ap.add_argument(\"--root\", type=Path, help=\"Directory to normalize recursively\")\n",
    "  ap.add_argument(\"--map\",  type=str, help=\"Extension map like ''sh=lf,ps1=crlf''\")\n",
    "  ap.add_argument(\"files\", nargs=\"*\", type=Path, help=\"Files to normalize (with --to-*)\")\n",
    "  args = ap.parse_args(argv)\n",
    "\n",
    "  # Mode B: explicit files\n",
    "  if args.to_lf or args.to_crlf:\n",
    "    mode = \"lf\" if args.to_lf else \"crlf\"\n",
    "    changed = 0\n",
    "    for f in args.files:\n",
    "      if normalize_file(f, mode):\n",
    "        changed += 1\n",
    "    print(f\"Changed {changed} files.\")\n",
    "    return 0\n",
    "\n",
    "  # Mode A: by-extension map under --root\n",
    "  if args.root and args.map:\n",
    "    extmap = parse_map(args.map)\n",
    "    changed = normalize_by_map(args.root, extmap)\n",
    "    print(f\"Changed {changed} files under {args.root}.\")\n",
    "    return 0\n",
    "\n",
    "  ap.error(\"Provide either (--to-lf|--to-crlf files...) or --root DIR --map ext=mode,...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \"\"\"\n",
    "  Gets called if the module is called from command prompt, via\n",
    "    e.g.\n",
    "      > python normalize_eof.py <argument>\n",
    "    OR\n",
    "      $ python normalize_eof.py <argument>\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  #  Note that this next call combines calling main, the exiting with\n",
    "  #+ its return value. I think it's even clearer than assigning the\n",
    "  #+ return value of main to, say, retval, and then returning retval.\n",
    "  \n",
    "  sys.exit(main())\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_build_model_p_03_e2e.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_inference_p_03_e2e.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_touch.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_train_model_p_03_e2e.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_03_e2e/scripts/py_utils_p_03_e2e.py\n",
    "----------\n",
    "--------------------------------------------------\n",
    "\n",
    "--------------------------------------------------\n",
    "Script at:\n",
    "/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/validate_env.py\n",
    "----------\n",
    "import sys\n",
    "import importlib\n",
    "import psutil\n",
    "from ptflops import get_model_complexity_info\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# -------------------------\n",
    "# Package Checks, part 1\n",
    "# -------------------------\n",
    "# List of packages to check\n",
    "packages = {\n",
    "  \"Python\": sys.version,\n",
    "  \"tensorflow\": \"tensorflow\",\n",
    "  \"torch\": \"torch\",\n",
    "  \"torchvision\": \"torchvision\",\n",
    "  \"torchaudio\": \"torchaudio\",\n",
    "  \"numpy\": \"numpy\",\n",
    "  \"pandas\": \"pandas\",\n",
    "  \"scikit-learn\": \"sklearn\",\n",
    "  \"opencv\": \"cv2\",\n",
    "  \"Pillow\": \"PIL\",\n",
    "  \"matplotlib\": \"matplotlib\",\n",
    "  \"tensorboard\": \"tensorboard\",\n",
    "  \"visualkeras\": \"visualkeras\",\n",
    "  \"netron\": \"netron\",\n",
    "  \"ptflops\": \"ptflops\",\n",
    "  \"psutil\": \"psutil\",\n",
    "  \"tqdm\": \"tqdm\",\n",
    "  \"humanfriendly\": \"humanfriendly\",\n",
    "  \"sagemaker\": \"sagemaker\",\n",
    "  \"boto3\": \"boto3\",\n",
    "  \"jupyterlab\": \"jupyterlab\",\n",
    "  \"jsonschema\": \"jsonschema\",\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Package Checks, part 2\n",
    "# -------------------------\n",
    "def check_package(pkg_name, module_name):\n",
    "  try:\n",
    "    module = importlib.import_module(module_name)\n",
    "    version = getattr(module, \"__version__\", \"Unknown\")\n",
    "    print(f\"[OK] {pkg_name} - version: {version}\")\n",
    "  except ImportError:\n",
    "    print(f\"[MISSING] {pkg_name} - not installed\")\n",
    "  finally:\n",
    "    print(f\"Finished check_package for {pkg_name} ({module_name})\")\n",
    "    print()\n",
    "  ##endof:  try/except/finally <import and get attributes>\n",
    "##endof:  def check_package\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# GPU Checks\n",
    "# ------------------------\n",
    "def check_gpu():\n",
    "  print(\"\\nGPU Check:\")\n",
    "  print(\"=\" * 20)\n",
    "    \n",
    "    # TensorFlow GPU\n",
    "  try:\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      print(f\"[TF GPU] TensorFlow sees {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}\")\n",
    "    else:\n",
    "      print(\"[TF GPU] No GPU detected for TensorFlow.\")\n",
    "  except ImportError:\n",
    "    print(\"[TF GPU] TensorFlow not installed, cannot check GPU.\")\n",
    "  finally:\n",
    "    print(\"Finished Tensorflow GPU part of check_gpu\")\n",
    "    print()\n",
    "  ##endof:  try/except/finally <tensorflow gpu stuff>\n",
    "  \n",
    "    # PyTorch GPU\n",
    "  try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "      print(f\"[Torch GPU] PyTorch sees {torch.cuda.device_count()} \" + \\\n",
    "            f\"GPU(s): {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "      print(\"[Torch GPU] No GPU detected for PyTorch.\")\n",
    "  except ImportError:\n",
    "    print(\"[Torch GPU] PyTorch not installed, cannot check GPU.\")\n",
    "  finally:\n",
    "    print(\"Finished PyTorch GPU part of check_gpu\")\n",
    "    print()\n",
    "  ##endof:  try/except/finally <pytorch gpu stuff>\n",
    "##endof:  def check_gpu\n",
    "\n",
    "# -------------------------\n",
    "# Memory and FLOPs Tests\n",
    "# -------------------------\n",
    "def check_memory_flops():\n",
    "  print(\"\\nMemory and FLOPs Test:\")\n",
    "  print(\"=\"*20)\n",
    "  mem = psutil.virtual_memory()\n",
    "  print(f\"Total RAM: {mem.total/1e9:.2f} GB, Available: {mem.available/1e9:.2f} GB\")\n",
    "  # Small FLOPs test using ResNet18\n",
    "  try:\n",
    "    macs, params = get_model_complexity_info(\n",
    "        resnet18(), \n",
    "        (3, 224, 224), \n",
    "        as_strings=True,\n",
    "        print_per_layer_stat=False, \n",
    "        verbose=False\n",
    "    )\n",
    "    print(f\"ResNet18 Params: {params}, FLOPs: {macs}\")\n",
    "  except Exception as e:\n",
    "    print(f\"ptflops test failed: {e}\")\n",
    "  finally:\n",
    "    print(\"Finished check_memory_flops\")\n",
    "    print()\n",
    "  ##endof:  try/except/finally <get_model_complexity_info>\n",
    "##endof:  check_memory_flops\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "  '''\n",
    "  Gets called when validate_env.py is called from the command line\n",
    "  '''\n",
    "  \n",
    "  print(\"Environment Validation Report:\")\n",
    "  print(\"=\" * 40)\n",
    "  for name, mod in packages.items():\n",
    "    if name == \"Python\":\n",
    "      print(f\"Python - version: {mod}\")\n",
    "    else:\n",
    "      check_package(name, mod)\n",
    "    ##endof:  if/else <python>\n",
    "    \n",
    "  check_gpu()\n",
    "  check_memory_flops()\n",
    "##endof:  if __name__ == \"__main__\"\n",
    "--------------------------------------------------\n",
    "\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    "(vanilla_cnn)  <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43522978-b3b2-4cc6-9763-d12e9d10140b",
   "metadata": {},
   "source": [
    "## After getting ready and going to some (i.e. a little bit of) church.\n",
    "\n",
    "```bash\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ type timestamp\n",
    "timestamp is a function\n",
    "timestamp () \n",
    "{ \n",
    "    date +\"%s_%Y-%m-%dT%H%M%S%z\"\n",
    "}\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ elapsed --check\n",
    "9050\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ echo \"$(elapsed --check)/60\" | bc -l\n",
    "151.10000000000000000000\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ type timestamp\n",
    "timestamp is a function\n",
    "timestamp () \n",
    "{ \n",
    "    date +\"%s_%Y-%m-%dT%H%M%S%z\"\n",
    "}\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ timestamp\n",
    "1757264623_2025-09-07T110343-0600\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ elapsed --help\n",
    "Usage: elapsed [OPTION]\n",
    "\n",
    "Options:\n",
    "  -s, --start   Start the stopwatch\n",
    "  -c, --check   Show elapsed time (in seconds, stopwatch keeps running)\n",
    "  -p, --stop    Stop the stopwatch and show elapsed time (in seconds)\n",
    "  -h, --help    Show this help message\n",
    "\n",
    "If run without an option, this help will be displayed.\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ elapsed -p\n",
    "9098\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ elapsed --check\n",
    "No stopwatch running. Use --start to begin.\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ timestamp\n",
    "1757264660_2025-09-07T110420-0600\n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$ exit\n",
    "exit\n",
    "Script done.\n",
    "Raw log:   /home/bballdave025/work_logs/Lab_Notebook_bballdave025_1757257066_2025-09-07T085746-0600.log\n",
    "Clean log: \n",
    "=====..........................................................retval=0.........\n",
    "\n",
    " <=> conda environment, blank means none activated\n",
    "[cifar10-vanilla-cnn] <=> git branch, blank means not in a git repo\n",
    "bballdave025@MY-MACHINE ~/my_repos_dwb/fhtw-paper-code-prep\n",
    "$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f58e58c8-943d-4b1f-b5e6-eb3fdc1c559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757264785_2025-09-07T110625-0600\n"
     ]
    }
   ],
   "source": [
    "!date +\"%s_%Y-%m-%dT%H%M%S%z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92792a2-944a-4b1e-9023-a9c9e9968d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
