{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88eb361f-cca6-4a24-a9bc-8c0ae99baf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Note, deleted the minimum-JSON-for-notebook-detection cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568c1d0-ac58-4d99-8546-0f7315826027",
   "metadata": {},
   "source": [
    "We ran two cells before that gave us a `model` error\n",
    "\n",
    "```python\n",
    "# === CIFAR-10 bootstrap (one cell) ============================================\n",
    "import os, json, shutil, random, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- runtime knobs (quiet + threads) ----------------------------------------\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"TF_NUM_INTEROP_THREADS\", \"2\")\n",
    "os.environ.setdefault(\"TF_NUM_INTRAOP_THREADS\", \"4\")\n",
    "\n",
    "# ---- resolve TAGDIR (project tag root) ---------------------------------------\n",
    "# If running from p_01/notebooks, TAGDIR is parent; else use CWD or $TAGDIR.\n",
    "_notebooks = Path.cwd().name.lower() == \"notebooks\"\n",
    "tagdir = Path(os.environ.get(\"TAGDIR\") or (Path.cwd().parent if _notebooks else Path.cwd()))\n",
    "os.environ[\"TAGDIR\"] = str(tagdir)\n",
    "\n",
    "# ---- project paths -----------------------------------------------------------\n",
    "outdir = tagdir / \"outputs\"\n",
    "csv_dir = outdir / \"csv_logs\"\n",
    "gradcam_dir = outdir / \"gradcam_images\"\n",
    "for d in (outdir, csv_dir, gradcam_dir):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Keras dataset cache scoped to project -----------------------------------\n",
    "# This keeps CIFAR-10 under: <TAGDIR>/datasets\n",
    "os.environ.setdefault(\"KERAS_HOME\", str(tagdir))\n",
    "proj_cache = Path(os.environ[\"KERAS_HOME\"]) / \"datasets\"\n",
    "proj_cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seed from ~/.keras/datasets if available and project cache is empty\n",
    "home_cache = Path.home() / \".keras\" / \"datasets\"\n",
    "candidates = [(\"cifar-10-python.tar.gz\", True), (\"cifar-10-batches-py\", False)]\n",
    "if home_cache.exists() and not any((proj_cache / n).exists() for n, _ in candidates):\n",
    "    for name, is_file in candidates:\n",
    "        src = home_cache / name\n",
    "        if src.exists():\n",
    "            dst = proj_cache / src.name\n",
    "            (shutil.copy2 if is_file else shutil.copytree)(src, dst, dirs_exist_ok=True)\n",
    "\n",
    "# ---- seeding -----------------------------------------------------------------\n",
    "SEED = int(os.environ.get(\"SEED\", \"137\"))\n",
    "random.seed(SEED)\n",
    "try:\n",
    "    import numpy as _np\n",
    "    _np.random.seed(SEED)\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    import tensorflow as _tf\n",
    "    _tf.random.set_seed(SEED)\n",
    "except Exception:\n",
    "    _tf = None  # Training may still run with torch; this cell is TF-friendly but not TF-required.\n",
    "\n",
    "# ---- helpers for file naming & logging ---------------------------------------\n",
    "def _ts():\n",
    "    return datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "def make_paths(seed: int = SEED):\n",
    "    stamp = _ts()\n",
    "    csv_path = csv_dir / f\"train_history_seed{seed}_{stamp}.csv\"\n",
    "    json_path = outdir / f\"test_summary_seed{seed}_{stamp}.json\"\n",
    "    return csv_path, json_path\n",
    "\n",
    "def log_test_summary(test_acc: float, loss: float | None = None, seed: int = SEED, extra: dict | None = None):\n",
    "    \"\"\"\n",
    "    Write the minimal JSON required by Q&R and print the sentinel line.\n",
    "    Usage at end of eval:\n",
    "        log_test_summary(test_acc, loss=float(test_loss))\n",
    "    \"\"\"\n",
    "    _, json_path = make_paths(seed)\n",
    "    payload = {\"seed\": seed, \"test_acc\": float(test_acc)}\n",
    "    if loss is not None:\n",
    "        payload[\"test_loss\"] = float(loss)\n",
    "    if extra:\n",
    "        payload.update(extra)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "    print(f\"[DONE] test_acc={payload['test_acc']:.4f}  ->  wrote {json_path}\")\n",
    "\n",
    "# ---- pretty banner -----------------------------------------------------------\n",
    "print(\n",
    "    \"Bootstrap ready\\n\"\n",
    "    f\"  TAGDIR:         {tagdir}\\n\"\n",
    "    f\"  KERAS_HOME:     {Path(os.environ['KERAS_HOME'])}\\n\"\n",
    "    f\"  Outputs:        {outdir}\\n\"\n",
    "    f\"  CSV logs:       {csv_dir}\\n\"\n",
    "    f\"  Grad-CAM imgs:  {gradcam_dir}\\n\"\n",
    "    f\"  SEED:           {SEED}\\n\"\n",
    "    f\"  TensorFlow:     {('OK ' + _tf.__version__) if _tf else 'not imported'}\"\n",
    ")\n",
    "# ============================================================================== \n",
    "```\n",
    "\n",
    "and the second cell\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "tagdir = Path(os.environ[\"TAGDIR\"])          # .../test_project_bash/p_01\n",
    "pkg_parent = tagdir.parent                   # .../test_project_bash\n",
    "\n",
    "# allow BOTH styles:\n",
    "# 1) from scripts.py_utils_p_01 import ...\n",
    "if str(tagdir) not in sys.path:\n",
    "    sys.path.insert(0, str(tagdir))\n",
    "\n",
    "# 2) from p_01.scripts.py_utils_p_01 import ...\n",
    "if str(pkg_parent) not in sys.path:\n",
    "    sys.path.insert(0, str(pkg_parent))\n",
    "\n",
    "from p_01.scripts.py_utils_p_01 import log_test_summary, make_paths\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5b5596-ea4b-4ca0-bf00-11d977a92921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "391/391 - 112s - loss: 1.6037 - accuracy: 0.4191 - 112s/epoch - 285ms/step\n",
      "Epoch 2/4\n",
      "391/391 - 114s - loss: 1.1891 - accuracy: 0.5791 - 114s/epoch - 292ms/step\n",
      "Epoch 3/4\n",
      "391/391 - 123s - loss: 1.0119 - accuracy: 0.6409 - 123s/epoch - 313ms/step\n",
      "Epoch 4/4\n",
      "391/391 - 121s - loss: 0.8989 - accuracy: 0.6861 - 121s/epoch - 310ms/step\n",
      "[DONE] test_acc=0.6686  ->  wrote /home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/notebooks/outputs/test_summary_seed137_20250906T125134.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/bballdave025/my_repos_dwb/fhtw-paper-code-prep/test_project_bash/p_01/notebooks/outputs/test_summary_seed137_20250906T125134.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Minimal CIFAR-10 train+test with build_model (Q&R ready) ==============\n",
    "import numpy as np, tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# --- your build_model from earlier ------------------------------------------\n",
    "def build_model(input_shape=(32, 32, 3), n_classes: int = 10) -> Model:\n",
    "    \"\"\"Tiny A0-style CNN with named layers; logits output (no softmax layer).\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", name=\"conv1\")(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv2\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv3\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(n_classes, name=\"logits\")(x)\n",
    "    return Model(inputs, outputs, name=\"A0_CNN\")\n",
    "\n",
    "# --- seed & data ------------------------------------------------------------\n",
    "SEED = 137\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train.astype(\"float32\")/255.0, x_test.astype(\"float32\")/255.0\n",
    "\n",
    "BATCH, EPOCHS = 128, 4\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000, seed=SEED).batch(BATCH)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH)\n",
    "\n",
    "# --- model build/compile -----------------------------------------------------\n",
    "model = build_model()\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# --- training ---------------------------------------------------------------\n",
    "history = model.fit(train_ds, epochs=EPOCHS, verbose=2)\n",
    "\n",
    "# --- evaluation + Q&R logging ----------------------------------------------\n",
    "from scripts.py_utils_p_01 import log_test_summary, make_paths\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "log_test_summary(test_acc, loss=float(test_loss),\n",
    "                 seed=SEED,\n",
    "                 extra={\"epochs\": EPOCHS, \"batch\": BATCH})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bc0e7-b35a-46c4-9874-4071cfe36c93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
